{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model for Stock Price Prediction\n",
    "\n",
    "This notebook trains an LSTM (Long Short-Term Memory) neural network to predict stock prices using historical OHLCV data from the trading system database.\n",
    "\n",
    "## Objectives\n",
    "- Load historical market data from PostgreSQL\n",
    "- Use pre-calculated technical indicators from database\n",
    "- Create time-aware train/validation/test splits\n",
    "- Train an LSTM model using PyTorch with optimized hyperparameters\n",
    "- **Predict log returns** (stationary target) instead of raw prices\n",
    "- Use **Huber Loss** (robust to outliers) instead of MSE\n",
    "- Monitor **directional accuracy** (key trading metric) during training\n",
    "- Evaluate model performance with financial metrics\n",
    "- Visualize predictions and residuals\n",
    "\n",
    "## Requirements\n",
    "- PyTorch (install with: `pip install torch`)\n",
    "- Optuna for hyperparameter optimization (install with: `pip install optuna`)\n",
    "- Plotly for optimization visualizations (optional, install with: `pip install plotly`)\n",
    "- All other dependencies from `requirements.txt`\n",
    "- **Important**: Ensure you have a `.env` file in the project root with database credentials:\n",
    "  ```env\n",
    "  POSTGRES_HOST=localhost\n",
    "  POSTGRES_PORT=5432\n",
    "  POSTGRES_USER=postgres\n",
    "  POSTGRES_PASSWORD=your_password_here\n",
    "  TRADING_DB_NAME=trading_system\n",
    "  ```\n",
    "  Copy from `deployment/env.example` if needed."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-20T18:11:24.057675Z",
     "start_time": "2025-12-20T18:11:24.043339Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "Setup and Imports\n",
    "\"\"\"\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from typing import Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path().resolve().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Load environment variables from .env file\n",
    "from dotenv import load_dotenv\n",
    "env_path = project_root / \".env\"\n",
    "if env_path.exists():\n",
    "    load_dotenv(env_path)\n",
    "    print(f\"Loaded environment variables from: {env_path}\")\n",
    "else:\n",
    "    print(f\"Warning: .env file not found at {env_path}\")\n",
    "    print(\"Please ensure your database credentials are set in environment variables or .env file\")\n",
    "\n",
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Hyperparameter Optimization\n",
    "try:\n",
    "    import optuna\n",
    "    OPTUNA_AVAILABLE = True\n",
    "except ImportError:\n",
    "    OPTUNA_AVAILABLE = False\n",
    "    print(\"Warning: Optuna not installed. Install with: pip install optuna\")\n",
    "\n",
    "# Database\n",
    "from sqlalchemy import select, desc\n",
    "from src.shared.database.base import db_readonly_session\n",
    "from src.shared.database.models.market_data import MarketData\n",
    "from src.shared.database.models.technical_indicators import TechnicalIndicators\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded environment variables from: D:\\PythonProjects\\Trading-System\\.env\n",
      "PyTorch version: 2.7.0+cu128\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "execution_count": 74
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading\n",
    "\n",
    "Load historical market data from the database for a specific symbol."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-20T18:11:24.164686Z",
     "start_time": "2025-12-20T18:11:24.086598Z"
    }
   },
   "source": [
    "def load_market_data_with_indicators(\n",
    "    symbol: str,\n",
    "    start_date: Optional[datetime] = None,\n",
    "    end_date: Optional[datetime] = None,\n",
    "    data_source: str = \"yahoo\",\n",
    "    min_records: int = 1000\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load market data and technical indicators from database for a specific symbol.\n",
    "    \n",
    "    This function loads OHLCV data from market_data table and joins it with\n",
    "    pre-calculated technical indicators from analytics.technical_indicators table.\n",
    "    \n",
    "    Args:\n",
    "        symbol: Stock symbol (e.g., 'AAPL')\n",
    "        start_date: Start date (default: 1 year ago)\n",
    "        end_date: End date (default: today)\n",
    "        data_source: Data source filter ('yahoo', 'polygon', 'alpaca')\n",
    "        min_records: Minimum number of records required\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with OHLCV data and technical indicators indexed by timestamp\n",
    "    \"\"\"\n",
    "    if start_date is None:\n",
    "        start_date = datetime.now(timezone.utc) - timedelta(days=365)\n",
    "    if end_date is None:\n",
    "        end_date = datetime.now(timezone.utc)\n",
    "    \n",
    "    symbol_upper = symbol.upper()\n",
    "    \n",
    "    # Load market data\n",
    "    with db_readonly_session() as session:\n",
    "        # Query market data\n",
    "        market_query = (\n",
    "            select(MarketData)\n",
    "            .where(MarketData.symbol == symbol_upper)\n",
    "            .where(MarketData.data_source == data_source.lower())\n",
    "            .where(MarketData.timestamp >= start_date)\n",
    "            .where(MarketData.timestamp <= end_date)\n",
    "            .order_by(MarketData.timestamp)\n",
    "        )\n",
    "        \n",
    "        market_result = session.execute(market_query)\n",
    "        market_records = market_result.scalars().all()\n",
    "    \n",
    "    if len(market_records) < min_records:\n",
    "        raise ValueError(\n",
    "            f\"Insufficient data: {len(market_records)} records found, \"\n",
    "            f\"minimum {min_records} required\"\n",
    "        )\n",
    "    \n",
    "    # Convert market data to DataFrame\n",
    "    market_data = []\n",
    "    for record in market_records:\n",
    "        if record.is_complete:  # Only include complete OHLCV records\n",
    "            # Convert timestamp to date for joining with technical indicators\n",
    "            market_data.append({\n",
    "                'date': record.timestamp.date(),\n",
    "                'timestamp': record.timestamp,\n",
    "                'open': float(record.open),\n",
    "                'high': float(record.high),\n",
    "                'low': float(record.low),\n",
    "                'close': float(record.close),\n",
    "                'volume': int(record.volume) if record.volume else 0,\n",
    "            })\n",
    "    \n",
    "    df_market = pd.DataFrame(market_data)\n",
    "    \n",
    "    # Load technical indicators\n",
    "    with db_readonly_session() as session:\n",
    "        # Query technical indicators\n",
    "        ti_query = (\n",
    "            select(TechnicalIndicators)\n",
    "            .where(TechnicalIndicators.symbol == symbol_upper)\n",
    "            .where(TechnicalIndicators.date >= start_date.date())\n",
    "            .where(TechnicalIndicators.date <= end_date.date())\n",
    "            .order_by(TechnicalIndicators.date)\n",
    "        )\n",
    "        \n",
    "        ti_result = session.execute(ti_query)\n",
    "        ti_records = ti_result.scalars().all()\n",
    "    \n",
    "    # Convert technical indicators to DataFrame\n",
    "    ti_data = []\n",
    "    for record in ti_records:\n",
    "        ti_data.append({\n",
    "            'date': record.date,\n",
    "            # Moving Averages\n",
    "            'sma_20': float(record.sma_20) if record.sma_20 else None,\n",
    "            'sma_50': float(record.sma_50) if record.sma_50 else None,\n",
    "            'sma_200': float(record.sma_200) if record.sma_200 else None,\n",
    "            'ema_12': float(record.ema_12) if record.ema_12 else None,\n",
    "            'ema_26': float(record.ema_26) if record.ema_26 else None,\n",
    "            'ema_50': float(record.ema_50) if record.ema_50 else None,\n",
    "            # Momentum\n",
    "            'rsi': float(record.rsi) if record.rsi else None,\n",
    "            'rsi_14': float(record.rsi_14) if record.rsi_14 else None,\n",
    "            # MACD\n",
    "            'macd_line': float(record.macd_line) if record.macd_line else None,\n",
    "            'macd_signal': float(record.macd_signal) if record.macd_signal else None,\n",
    "            'macd_histogram': float(record.macd_histogram) if record.macd_histogram else None,\n",
    "            # Bollinger Bands\n",
    "            'bb_upper': float(record.bb_upper) if record.bb_upper else None,\n",
    "            'bb_middle': float(record.bb_middle) if record.bb_middle else None,\n",
    "            'bb_lower': float(record.bb_lower) if record.bb_lower else None,\n",
    "            'bb_position': float(record.bb_position) if record.bb_position else None,\n",
    "            'bb_width': float(record.bb_width) if record.bb_width else None,\n",
    "            # Volatility & Price Changes\n",
    "            'volatility_20': float(record.volatility_20) if record.volatility_20 else None,\n",
    "            'price_change_1d': float(record.price_change_1d) if record.price_change_1d else None,\n",
    "            'price_change_5d': float(record.price_change_5d) if record.price_change_5d else None,\n",
    "            'price_change_30d': float(record.price_change_30d) if record.price_change_30d else None,\n",
    "            # Volume\n",
    "            'avg_volume_20': int(record.avg_volume_20) if record.avg_volume_20 else None,\n",
    "            'current_volume': int(record.current_volume) if record.current_volume else None,\n",
    "        })\n",
    "    \n",
    "    df_ti = pd.DataFrame(ti_data)\n",
    "    \n",
    "    # Merge market data with technical indicators on date\n",
    "    if not df_ti.empty:\n",
    "        df = df_market.merge(df_ti, on='date', how='left')\n",
    "        print(f\"Loaded {len(df_market)} market data records\")\n",
    "        print(f\"Loaded {len(df_ti)} technical indicator records\")\n",
    "        print(f\"Merged dataset: {len(df)} records\")\n",
    "    else:\n",
    "        print(f\"Warning: No technical indicators found for {symbol}\")\n",
    "        print(\"Using market data only. Consider running populate_technical_indicators.py\")\n",
    "        df = df_market.copy()\n",
    "    \n",
    "    # Set timestamp as index (use market data timestamp)\n",
    "    df.set_index('timestamp', inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "    \n",
    "    # Remove duplicates (keep last)\n",
    "    df = df[~df.index.duplicated(keep='last')]\n",
    "    \n",
    "    # Calculate basic returns if not available from indicators\n",
    "    if 'returns' not in df.columns:\n",
    "        df['returns'] = df['close'].pct_change()\n",
    "    if 'log_returns' not in df.columns:\n",
    "        df['log_returns'] = np.log(df['close'] / df['close'].shift(1))\n",
    "    \n",
    "    # Price ratios\n",
    "    df['high_low_ratio'] = df['high'] / df['low']\n",
    "    df['close_open_ratio'] = df['close'] / df['open']\n",
    "    \n",
    "    # Volume ratio (if we have avg_volume_20)\n",
    "    if 'avg_volume_20' in df.columns and df['avg_volume_20'].notna().any():\n",
    "        df['volume_ratio'] = df['volume'] / df['avg_volume_20']\n",
    "    else:\n",
    "        df['volume_ratio'] = None\n",
    "    \n",
    "    # Price position (where close is relative to high-low range)\n",
    "    df['price_position'] = (df['close'] - df['low']) / (df['high'] - df['low'])\n",
    "    \n",
    "    print(f\"\\nFinal dataset: {len(df)} records\")\n",
    "    print(f\"Date range: {df.index.min()} to {df.index.max()}\")\n",
    "    print(f\"Missing values: {df.isnull().sum().sum()}\")\n",
    "    print(f\"\\nAvailable features: {list(df.columns)}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load data for a symbol (change as needed)\n",
    "SYMBOL = \"MU\"\n",
    "df_features = load_market_data_with_indicators(SYMBOL, data_source=\"yahoo\")\n",
    "df_features.head()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2025-12-20 12:11:24.115\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.shared.database.base\u001B[0m:\u001B[36mdb_readonly_session\u001B[0m:\u001B[36m156\u001B[0m - \u001B[34m\u001B[1mRead-only session completed successfully\u001B[0m\n",
      "\u001B[32m2025-12-20 12:11:24.133\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.shared.database.base\u001B[0m:\u001B[36mdb_readonly_session\u001B[0m:\u001B[36m156\u001B[0m - \u001B[34m\u001B[1mRead-only session completed successfully\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1731 market data records\n",
      "Loaded 366 technical indicator records\n",
      "Merged dataset: 1731 records\n",
      "\n",
      "Final dataset: 1731 records\n",
      "Date range: 2024-12-23 03:30:00-06:00 to 2025-12-19 09:30:00-06:00\n",
      "Missing values: 2\n",
      "\n",
      "Available features: ['date', 'open', 'high', 'low', 'close', 'volume', 'sma_20', 'sma_50', 'sma_200', 'ema_12', 'ema_26', 'ema_50', 'rsi', 'rsi_14', 'macd_line', 'macd_signal', 'macd_histogram', 'bb_upper', 'bb_middle', 'bb_lower', 'bb_position', 'bb_width', 'volatility_20', 'price_change_1d', 'price_change_5d', 'price_change_30d', 'avg_volume_20', 'current_volume', 'returns', 'log_returns', 'high_low_ratio', 'close_open_ratio', 'volume_ratio', 'price_position']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                 date    open     high     low   close  \\\n",
       "timestamp                                                                \n",
       "2024-12-23 03:30:00-06:00  2024-12-23  90.145  91.1000  88.400  88.500   \n",
       "2024-12-23 04:30:00-06:00  2024-12-23  88.520  89.2000  88.400  89.060   \n",
       "2024-12-23 05:30:00-06:00  2024-12-23  89.057  89.4100  88.432  88.585   \n",
       "2024-12-23 06:30:00-06:00  2024-12-23  88.585  89.4213  88.585  89.160   \n",
       "2024-12-23 07:30:00-06:00  2024-12-23  89.140  90.0050  89.030  89.790   \n",
       "\n",
       "                            volume   sma_20    sma_50   sma_200   ema_12  ...  \\\n",
       "timestamp                                                                 ...   \n",
       "2024-12-23 03:30:00-06:00  6648844  99.8347  102.9651  110.7619  97.6045  ...   \n",
       "2024-12-23 04:30:00-06:00  2661600  99.8347  102.9651  110.7619  97.6045  ...   \n",
       "2024-12-23 05:30:00-06:00  2726427  99.8347  102.9651  110.7619  97.6045  ...   \n",
       "2024-12-23 06:30:00-06:00  2008035  99.8347  102.9651  110.7619  97.6045  ...   \n",
       "2024-12-23 07:30:00-06:00  1920008  99.8347  102.9651  110.7619  97.6045  ...   \n",
       "\n",
       "                           price_change_5d  price_change_30d  avg_volume_20  \\\n",
       "timestamp                                                                     \n",
       "2024-12-23 03:30:00-06:00           -16.97            -19.71       23231731   \n",
       "2024-12-23 04:30:00-06:00           -16.97            -19.71       23231731   \n",
       "2024-12-23 05:30:00-06:00           -16.97            -19.71       23231731   \n",
       "2024-12-23 06:30:00-06:00           -16.97            -19.71       23231731   \n",
       "2024-12-23 07:30:00-06:00           -16.97            -19.71       23231731   \n",
       "\n",
       "                           current_volume   returns  log_returns  \\\n",
       "timestamp                                                          \n",
       "2024-12-23 03:30:00-06:00        21724019       NaN          NaN   \n",
       "2024-12-23 04:30:00-06:00        21724019  0.006328     0.006308   \n",
       "2024-12-23 05:30:00-06:00        21724019 -0.005333    -0.005348   \n",
       "2024-12-23 06:30:00-06:00        21724019  0.006491     0.006470   \n",
       "2024-12-23 07:30:00-06:00        21724019  0.007066     0.007041   \n",
       "\n",
       "                           high_low_ratio  close_open_ratio  volume_ratio  \\\n",
       "timestamp                                                                   \n",
       "2024-12-23 03:30:00-06:00        1.030543          0.981752      0.286197   \n",
       "2024-12-23 04:30:00-06:00        1.009050          1.006100      0.114567   \n",
       "2024-12-23 05:30:00-06:00        1.011059          0.994700      0.117358   \n",
       "2024-12-23 06:30:00-06:00        1.009441          1.006491      0.086435   \n",
       "2024-12-23 07:30:00-06:00        1.010951          1.007292      0.082646   \n",
       "\n",
       "                           price_position  \n",
       "timestamp                                  \n",
       "2024-12-23 03:30:00-06:00        0.037037  \n",
       "2024-12-23 04:30:00-06:00        0.825000  \n",
       "2024-12-23 05:30:00-06:00        0.156442  \n",
       "2024-12-23 06:30:00-06:00        0.687552  \n",
       "2024-12-23 07:30:00-06:00        0.779487  \n",
       "\n",
       "[5 rows x 34 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>sma_20</th>\n",
       "      <th>sma_50</th>\n",
       "      <th>sma_200</th>\n",
       "      <th>ema_12</th>\n",
       "      <th>...</th>\n",
       "      <th>price_change_5d</th>\n",
       "      <th>price_change_30d</th>\n",
       "      <th>avg_volume_20</th>\n",
       "      <th>current_volume</th>\n",
       "      <th>returns</th>\n",
       "      <th>log_returns</th>\n",
       "      <th>high_low_ratio</th>\n",
       "      <th>close_open_ratio</th>\n",
       "      <th>volume_ratio</th>\n",
       "      <th>price_position</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-12-23 03:30:00-06:00</th>\n",
       "      <td>2024-12-23</td>\n",
       "      <td>90.145</td>\n",
       "      <td>91.1000</td>\n",
       "      <td>88.400</td>\n",
       "      <td>88.500</td>\n",
       "      <td>6648844</td>\n",
       "      <td>99.8347</td>\n",
       "      <td>102.9651</td>\n",
       "      <td>110.7619</td>\n",
       "      <td>97.6045</td>\n",
       "      <td>...</td>\n",
       "      <td>-16.97</td>\n",
       "      <td>-19.71</td>\n",
       "      <td>23231731</td>\n",
       "      <td>21724019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.030543</td>\n",
       "      <td>0.981752</td>\n",
       "      <td>0.286197</td>\n",
       "      <td>0.037037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-23 04:30:00-06:00</th>\n",
       "      <td>2024-12-23</td>\n",
       "      <td>88.520</td>\n",
       "      <td>89.2000</td>\n",
       "      <td>88.400</td>\n",
       "      <td>89.060</td>\n",
       "      <td>2661600</td>\n",
       "      <td>99.8347</td>\n",
       "      <td>102.9651</td>\n",
       "      <td>110.7619</td>\n",
       "      <td>97.6045</td>\n",
       "      <td>...</td>\n",
       "      <td>-16.97</td>\n",
       "      <td>-19.71</td>\n",
       "      <td>23231731</td>\n",
       "      <td>21724019</td>\n",
       "      <td>0.006328</td>\n",
       "      <td>0.006308</td>\n",
       "      <td>1.009050</td>\n",
       "      <td>1.006100</td>\n",
       "      <td>0.114567</td>\n",
       "      <td>0.825000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-23 05:30:00-06:00</th>\n",
       "      <td>2024-12-23</td>\n",
       "      <td>89.057</td>\n",
       "      <td>89.4100</td>\n",
       "      <td>88.432</td>\n",
       "      <td>88.585</td>\n",
       "      <td>2726427</td>\n",
       "      <td>99.8347</td>\n",
       "      <td>102.9651</td>\n",
       "      <td>110.7619</td>\n",
       "      <td>97.6045</td>\n",
       "      <td>...</td>\n",
       "      <td>-16.97</td>\n",
       "      <td>-19.71</td>\n",
       "      <td>23231731</td>\n",
       "      <td>21724019</td>\n",
       "      <td>-0.005333</td>\n",
       "      <td>-0.005348</td>\n",
       "      <td>1.011059</td>\n",
       "      <td>0.994700</td>\n",
       "      <td>0.117358</td>\n",
       "      <td>0.156442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-23 06:30:00-06:00</th>\n",
       "      <td>2024-12-23</td>\n",
       "      <td>88.585</td>\n",
       "      <td>89.4213</td>\n",
       "      <td>88.585</td>\n",
       "      <td>89.160</td>\n",
       "      <td>2008035</td>\n",
       "      <td>99.8347</td>\n",
       "      <td>102.9651</td>\n",
       "      <td>110.7619</td>\n",
       "      <td>97.6045</td>\n",
       "      <td>...</td>\n",
       "      <td>-16.97</td>\n",
       "      <td>-19.71</td>\n",
       "      <td>23231731</td>\n",
       "      <td>21724019</td>\n",
       "      <td>0.006491</td>\n",
       "      <td>0.006470</td>\n",
       "      <td>1.009441</td>\n",
       "      <td>1.006491</td>\n",
       "      <td>0.086435</td>\n",
       "      <td>0.687552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-23 07:30:00-06:00</th>\n",
       "      <td>2024-12-23</td>\n",
       "      <td>89.140</td>\n",
       "      <td>90.0050</td>\n",
       "      <td>89.030</td>\n",
       "      <td>89.790</td>\n",
       "      <td>1920008</td>\n",
       "      <td>99.8347</td>\n",
       "      <td>102.9651</td>\n",
       "      <td>110.7619</td>\n",
       "      <td>97.6045</td>\n",
       "      <td>...</td>\n",
       "      <td>-16.97</td>\n",
       "      <td>-19.71</td>\n",
       "      <td>23231731</td>\n",
       "      <td>21724019</td>\n",
       "      <td>0.007066</td>\n",
       "      <td>0.007041</td>\n",
       "      <td>1.010951</td>\n",
       "      <td>1.007292</td>\n",
       "      <td>0.082646</td>\n",
       "      <td>0.779487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 75
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Overview\n",
    "\n",
    "Review the loaded data with pre-calculated technical indicators from the database."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-20T18:11:24.234410Z",
     "start_time": "2025-12-20T18:11:24.214213Z"
    }
   },
   "source": [
    "# Display data summary\n",
    "print(f\"Dataset shape: {df_features.shape}\")\n",
    "print(f\"\\nColumn types:\")\n",
    "print(df_features.dtypes)\n",
    "print(f\"\\nMissing values per column:\")\n",
    "missing = df_features.isnull().sum()\n",
    "print(missing[missing > 0])\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df_features.head(10)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (1731, 34)\n",
      "\n",
      "Column types:\n",
      "date                 object\n",
      "open                float64\n",
      "high                float64\n",
      "low                 float64\n",
      "close               float64\n",
      "volume                int64\n",
      "sma_20              float64\n",
      "sma_50              float64\n",
      "sma_200             float64\n",
      "ema_12              float64\n",
      "ema_26              float64\n",
      "ema_50              float64\n",
      "rsi                 float64\n",
      "rsi_14              float64\n",
      "macd_line           float64\n",
      "macd_signal         float64\n",
      "macd_histogram      float64\n",
      "bb_upper            float64\n",
      "bb_middle           float64\n",
      "bb_lower            float64\n",
      "bb_position         float64\n",
      "bb_width            float64\n",
      "volatility_20       float64\n",
      "price_change_1d     float64\n",
      "price_change_5d     float64\n",
      "price_change_30d    float64\n",
      "avg_volume_20         int64\n",
      "current_volume        int64\n",
      "returns             float64\n",
      "log_returns         float64\n",
      "high_low_ratio      float64\n",
      "close_open_ratio    float64\n",
      "volume_ratio        float64\n",
      "price_position      float64\n",
      "dtype: object\n",
      "\n",
      "Missing values per column:\n",
      "returns        1\n",
      "log_returns    1\n",
      "dtype: int64\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                 date     open     high     low   close  \\\n",
       "timestamp                                                                 \n",
       "2024-12-23 03:30:00-06:00  2024-12-23  90.1450  91.1000  88.400  88.500   \n",
       "2024-12-23 04:30:00-06:00  2024-12-23  88.5200  89.2000  88.400  89.060   \n",
       "2024-12-23 05:30:00-06:00  2024-12-23  89.0570  89.4100  88.432  88.585   \n",
       "2024-12-23 06:30:00-06:00  2024-12-23  88.5850  89.4213  88.585  89.160   \n",
       "2024-12-23 07:30:00-06:00  2024-12-23  89.1400  90.0050  89.030  89.790   \n",
       "2024-12-23 08:30:00-06:00  2024-12-23  89.8081  90.0700  89.230  89.860   \n",
       "2024-12-23 09:30:00-06:00  2024-12-23  89.8700  90.0000  89.075  89.880   \n",
       "2024-12-24 03:30:00-06:00  2024-12-24  89.5800  89.7000  88.230  89.060   \n",
       "2024-12-24 04:30:00-06:00  2024-12-24  89.0400  89.4199  88.760  89.341   \n",
       "2024-12-24 05:30:00-06:00  2024-12-24  89.3450  89.5800  89.060  89.530   \n",
       "\n",
       "                            volume   sma_20    sma_50   sma_200   ema_12  ...  \\\n",
       "timestamp                                                                 ...   \n",
       "2024-12-23 03:30:00-06:00  6648844  99.8347  102.9651  110.7619  97.6045  ...   \n",
       "2024-12-23 04:30:00-06:00  2661600  99.8347  102.9651  110.7619  97.6045  ...   \n",
       "2024-12-23 05:30:00-06:00  2726427  99.8347  102.9651  110.7619  97.6045  ...   \n",
       "2024-12-23 06:30:00-06:00  2008035  99.8347  102.9651  110.7619  97.6045  ...   \n",
       "2024-12-23 07:30:00-06:00  1920008  99.8347  102.9651  110.7619  97.6045  ...   \n",
       "2024-12-23 08:30:00-06:00  1991141  99.8347  102.9651  110.7619  97.6045  ...   \n",
       "2024-12-23 09:30:00-06:00  3767964  99.8347  102.9651  110.7619  97.6045  ...   \n",
       "2024-12-24 03:30:00-06:00  4436926  99.0867  102.5889  110.7368  96.3622  ...   \n",
       "2024-12-24 04:30:00-06:00  2690646  99.0867  102.5889  110.7368  96.3622  ...   \n",
       "2024-12-24 05:30:00-06:00  2429763  99.0867  102.5889  110.7368  96.3622  ...   \n",
       "\n",
       "                           price_change_5d  price_change_30d  avg_volume_20  \\\n",
       "timestamp                                                                     \n",
       "2024-12-23 03:30:00-06:00           -16.97            -19.71       23231731   \n",
       "2024-12-23 04:30:00-06:00           -16.97            -19.71       23231731   \n",
       "2024-12-23 05:30:00-06:00           -16.97            -19.71       23231731   \n",
       "2024-12-23 06:30:00-06:00           -16.97            -19.71       23231731   \n",
       "2024-12-23 07:30:00-06:00           -16.97            -19.71       23231731   \n",
       "2024-12-23 08:30:00-06:00           -16.97            -19.71       23231731   \n",
       "2024-12-23 09:30:00-06:00           -16.97            -19.71       23231731   \n",
       "2024-12-24 03:30:00-06:00           -17.59            -17.60       22906078   \n",
       "2024-12-24 04:30:00-06:00           -17.59            -17.60       22906078   \n",
       "2024-12-24 05:30:00-06:00           -17.59            -17.60       22906078   \n",
       "\n",
       "                           current_volume   returns  log_returns  \\\n",
       "timestamp                                                          \n",
       "2024-12-23 03:30:00-06:00        21724019       NaN          NaN   \n",
       "2024-12-23 04:30:00-06:00        21724019  0.006328     0.006308   \n",
       "2024-12-23 05:30:00-06:00        21724019 -0.005333    -0.005348   \n",
       "2024-12-23 06:30:00-06:00        21724019  0.006491     0.006470   \n",
       "2024-12-23 07:30:00-06:00        21724019  0.007066     0.007041   \n",
       "2024-12-23 08:30:00-06:00        21724019  0.000780     0.000779   \n",
       "2024-12-23 09:30:00-06:00        21724019  0.000223     0.000223   \n",
       "2024-12-24 03:30:00-06:00         9557335 -0.009123    -0.009165   \n",
       "2024-12-24 04:30:00-06:00         9557335  0.003155     0.003150   \n",
       "2024-12-24 05:30:00-06:00         9557335  0.002115     0.002113   \n",
       "\n",
       "                           high_low_ratio  close_open_ratio  volume_ratio  \\\n",
       "timestamp                                                                   \n",
       "2024-12-23 03:30:00-06:00        1.030543          0.981752      0.286197   \n",
       "2024-12-23 04:30:00-06:00        1.009050          1.006100      0.114567   \n",
       "2024-12-23 05:30:00-06:00        1.011059          0.994700      0.117358   \n",
       "2024-12-23 06:30:00-06:00        1.009441          1.006491      0.086435   \n",
       "2024-12-23 07:30:00-06:00        1.010951          1.007292      0.082646   \n",
       "2024-12-23 08:30:00-06:00        1.009414          1.000578      0.085708   \n",
       "2024-12-23 09:30:00-06:00        1.010385          1.000111      0.162190   \n",
       "2024-12-24 03:30:00-06:00        1.016661          0.994195      0.193701   \n",
       "2024-12-24 04:30:00-06:00        1.007435          1.003381      0.117464   \n",
       "2024-12-24 05:30:00-06:00        1.005839          1.002071      0.106075   \n",
       "\n",
       "                           price_position  \n",
       "timestamp                                  \n",
       "2024-12-23 03:30:00-06:00        0.037037  \n",
       "2024-12-23 04:30:00-06:00        0.825000  \n",
       "2024-12-23 05:30:00-06:00        0.156442  \n",
       "2024-12-23 06:30:00-06:00        0.687552  \n",
       "2024-12-23 07:30:00-06:00        0.779487  \n",
       "2024-12-23 08:30:00-06:00        0.750000  \n",
       "2024-12-23 09:30:00-06:00        0.870270  \n",
       "2024-12-24 03:30:00-06:00        0.564626  \n",
       "2024-12-24 04:30:00-06:00        0.880436  \n",
       "2024-12-24 05:30:00-06:00        0.903846  \n",
       "\n",
       "[10 rows x 34 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>sma_20</th>\n",
       "      <th>sma_50</th>\n",
       "      <th>sma_200</th>\n",
       "      <th>ema_12</th>\n",
       "      <th>...</th>\n",
       "      <th>price_change_5d</th>\n",
       "      <th>price_change_30d</th>\n",
       "      <th>avg_volume_20</th>\n",
       "      <th>current_volume</th>\n",
       "      <th>returns</th>\n",
       "      <th>log_returns</th>\n",
       "      <th>high_low_ratio</th>\n",
       "      <th>close_open_ratio</th>\n",
       "      <th>volume_ratio</th>\n",
       "      <th>price_position</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-12-23 03:30:00-06:00</th>\n",
       "      <td>2024-12-23</td>\n",
       "      <td>90.1450</td>\n",
       "      <td>91.1000</td>\n",
       "      <td>88.400</td>\n",
       "      <td>88.500</td>\n",
       "      <td>6648844</td>\n",
       "      <td>99.8347</td>\n",
       "      <td>102.9651</td>\n",
       "      <td>110.7619</td>\n",
       "      <td>97.6045</td>\n",
       "      <td>...</td>\n",
       "      <td>-16.97</td>\n",
       "      <td>-19.71</td>\n",
       "      <td>23231731</td>\n",
       "      <td>21724019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.030543</td>\n",
       "      <td>0.981752</td>\n",
       "      <td>0.286197</td>\n",
       "      <td>0.037037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-23 04:30:00-06:00</th>\n",
       "      <td>2024-12-23</td>\n",
       "      <td>88.5200</td>\n",
       "      <td>89.2000</td>\n",
       "      <td>88.400</td>\n",
       "      <td>89.060</td>\n",
       "      <td>2661600</td>\n",
       "      <td>99.8347</td>\n",
       "      <td>102.9651</td>\n",
       "      <td>110.7619</td>\n",
       "      <td>97.6045</td>\n",
       "      <td>...</td>\n",
       "      <td>-16.97</td>\n",
       "      <td>-19.71</td>\n",
       "      <td>23231731</td>\n",
       "      <td>21724019</td>\n",
       "      <td>0.006328</td>\n",
       "      <td>0.006308</td>\n",
       "      <td>1.009050</td>\n",
       "      <td>1.006100</td>\n",
       "      <td>0.114567</td>\n",
       "      <td>0.825000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-23 05:30:00-06:00</th>\n",
       "      <td>2024-12-23</td>\n",
       "      <td>89.0570</td>\n",
       "      <td>89.4100</td>\n",
       "      <td>88.432</td>\n",
       "      <td>88.585</td>\n",
       "      <td>2726427</td>\n",
       "      <td>99.8347</td>\n",
       "      <td>102.9651</td>\n",
       "      <td>110.7619</td>\n",
       "      <td>97.6045</td>\n",
       "      <td>...</td>\n",
       "      <td>-16.97</td>\n",
       "      <td>-19.71</td>\n",
       "      <td>23231731</td>\n",
       "      <td>21724019</td>\n",
       "      <td>-0.005333</td>\n",
       "      <td>-0.005348</td>\n",
       "      <td>1.011059</td>\n",
       "      <td>0.994700</td>\n",
       "      <td>0.117358</td>\n",
       "      <td>0.156442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-23 06:30:00-06:00</th>\n",
       "      <td>2024-12-23</td>\n",
       "      <td>88.5850</td>\n",
       "      <td>89.4213</td>\n",
       "      <td>88.585</td>\n",
       "      <td>89.160</td>\n",
       "      <td>2008035</td>\n",
       "      <td>99.8347</td>\n",
       "      <td>102.9651</td>\n",
       "      <td>110.7619</td>\n",
       "      <td>97.6045</td>\n",
       "      <td>...</td>\n",
       "      <td>-16.97</td>\n",
       "      <td>-19.71</td>\n",
       "      <td>23231731</td>\n",
       "      <td>21724019</td>\n",
       "      <td>0.006491</td>\n",
       "      <td>0.006470</td>\n",
       "      <td>1.009441</td>\n",
       "      <td>1.006491</td>\n",
       "      <td>0.086435</td>\n",
       "      <td>0.687552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-23 07:30:00-06:00</th>\n",
       "      <td>2024-12-23</td>\n",
       "      <td>89.1400</td>\n",
       "      <td>90.0050</td>\n",
       "      <td>89.030</td>\n",
       "      <td>89.790</td>\n",
       "      <td>1920008</td>\n",
       "      <td>99.8347</td>\n",
       "      <td>102.9651</td>\n",
       "      <td>110.7619</td>\n",
       "      <td>97.6045</td>\n",
       "      <td>...</td>\n",
       "      <td>-16.97</td>\n",
       "      <td>-19.71</td>\n",
       "      <td>23231731</td>\n",
       "      <td>21724019</td>\n",
       "      <td>0.007066</td>\n",
       "      <td>0.007041</td>\n",
       "      <td>1.010951</td>\n",
       "      <td>1.007292</td>\n",
       "      <td>0.082646</td>\n",
       "      <td>0.779487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-23 08:30:00-06:00</th>\n",
       "      <td>2024-12-23</td>\n",
       "      <td>89.8081</td>\n",
       "      <td>90.0700</td>\n",
       "      <td>89.230</td>\n",
       "      <td>89.860</td>\n",
       "      <td>1991141</td>\n",
       "      <td>99.8347</td>\n",
       "      <td>102.9651</td>\n",
       "      <td>110.7619</td>\n",
       "      <td>97.6045</td>\n",
       "      <td>...</td>\n",
       "      <td>-16.97</td>\n",
       "      <td>-19.71</td>\n",
       "      <td>23231731</td>\n",
       "      <td>21724019</td>\n",
       "      <td>0.000780</td>\n",
       "      <td>0.000779</td>\n",
       "      <td>1.009414</td>\n",
       "      <td>1.000578</td>\n",
       "      <td>0.085708</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-23 09:30:00-06:00</th>\n",
       "      <td>2024-12-23</td>\n",
       "      <td>89.8700</td>\n",
       "      <td>90.0000</td>\n",
       "      <td>89.075</td>\n",
       "      <td>89.880</td>\n",
       "      <td>3767964</td>\n",
       "      <td>99.8347</td>\n",
       "      <td>102.9651</td>\n",
       "      <td>110.7619</td>\n",
       "      <td>97.6045</td>\n",
       "      <td>...</td>\n",
       "      <td>-16.97</td>\n",
       "      <td>-19.71</td>\n",
       "      <td>23231731</td>\n",
       "      <td>21724019</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>1.010385</td>\n",
       "      <td>1.000111</td>\n",
       "      <td>0.162190</td>\n",
       "      <td>0.870270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-24 03:30:00-06:00</th>\n",
       "      <td>2024-12-24</td>\n",
       "      <td>89.5800</td>\n",
       "      <td>89.7000</td>\n",
       "      <td>88.230</td>\n",
       "      <td>89.060</td>\n",
       "      <td>4436926</td>\n",
       "      <td>99.0867</td>\n",
       "      <td>102.5889</td>\n",
       "      <td>110.7368</td>\n",
       "      <td>96.3622</td>\n",
       "      <td>...</td>\n",
       "      <td>-17.59</td>\n",
       "      <td>-17.60</td>\n",
       "      <td>22906078</td>\n",
       "      <td>9557335</td>\n",
       "      <td>-0.009123</td>\n",
       "      <td>-0.009165</td>\n",
       "      <td>1.016661</td>\n",
       "      <td>0.994195</td>\n",
       "      <td>0.193701</td>\n",
       "      <td>0.564626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-24 04:30:00-06:00</th>\n",
       "      <td>2024-12-24</td>\n",
       "      <td>89.0400</td>\n",
       "      <td>89.4199</td>\n",
       "      <td>88.760</td>\n",
       "      <td>89.341</td>\n",
       "      <td>2690646</td>\n",
       "      <td>99.0867</td>\n",
       "      <td>102.5889</td>\n",
       "      <td>110.7368</td>\n",
       "      <td>96.3622</td>\n",
       "      <td>...</td>\n",
       "      <td>-17.59</td>\n",
       "      <td>-17.60</td>\n",
       "      <td>22906078</td>\n",
       "      <td>9557335</td>\n",
       "      <td>0.003155</td>\n",
       "      <td>0.003150</td>\n",
       "      <td>1.007435</td>\n",
       "      <td>1.003381</td>\n",
       "      <td>0.117464</td>\n",
       "      <td>0.880436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-24 05:30:00-06:00</th>\n",
       "      <td>2024-12-24</td>\n",
       "      <td>89.3450</td>\n",
       "      <td>89.5800</td>\n",
       "      <td>89.060</td>\n",
       "      <td>89.530</td>\n",
       "      <td>2429763</td>\n",
       "      <td>99.0867</td>\n",
       "      <td>102.5889</td>\n",
       "      <td>110.7368</td>\n",
       "      <td>96.3622</td>\n",
       "      <td>...</td>\n",
       "      <td>-17.59</td>\n",
       "      <td>-17.60</td>\n",
       "      <td>22906078</td>\n",
       "      <td>9557335</td>\n",
       "      <td>0.002115</td>\n",
       "      <td>0.002113</td>\n",
       "      <td>1.005839</td>\n",
       "      <td>1.002071</td>\n",
       "      <td>0.106075</td>\n",
       "      <td>0.903846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 34 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 76
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "Prepare data for LSTM: select features, normalize, and create sequences."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-20T18:11:24.514603Z",
     "start_time": "2025-12-20T18:11:24.495676Z"
    }
   },
   "source": [
    "# Configuration\n",
    "SEQUENCE_LENGTH = 60  # Number of time steps to look back\n",
    "PREDICTION_HORIZON = 1  # Predict next 1 period\n",
    "# Use log_returns as target (stationary, focuses on movement rather than price level)\n",
    "TARGET_COLUMN = 'log_returns'  # Changed from 'close' to 'log_returns' for better stationarity\n",
    "\n",
    "# Select features from available columns\n",
    "# Use pre-calculated technical indicators from database\n",
    "# IMPORTANT: Remove non-stationary raw price levels (open, high, low, close)\n",
    "# These cause the model to develop negative bias due to scale differences\n",
    "# Keep only stationary features that describe relative movement\n",
    "feature_columns = [\n",
    "    # Volume (can be kept if normalized, but ratios are better)\n",
    "    'volume',\n",
    "    # Returns (calculated) - stationary\n",
    "    'returns', 'log_returns',\n",
    "    # Technical Indicators (from database) - all stationary\n",
    "    'sma_20', 'sma_50', 'sma_200',\n",
    "    'ema_12', 'ema_26', 'ema_50',\n",
    "    'rsi', 'rsi_14',\n",
    "    'macd_line', 'macd_signal', 'macd_histogram',\n",
    "    'bb_upper', 'bb_middle', 'bb_lower', 'bb_position', 'bb_width',\n",
    "    'volatility_20',\n",
    "    'price_change_1d', 'price_change_5d', 'price_change_30d',\n",
    "    # Derived features - all stationary ratios\n",
    "    'high_low_ratio', 'close_open_ratio',\n",
    "    'volume_ratio', 'price_position',\n",
    "]\n",
    "\n",
    "# Filter to only include columns that exist and have data\n",
    "available_features = [\n",
    "    col for col in feature_columns \n",
    "    if col in df_features.columns and df_features[col].notna().sum() > 0\n",
    "]\n",
    "\n",
    "print(f\"Available features: {len(available_features)}\")\n",
    "print(f\"Features: {available_features}\")\n",
    "\n",
    "# Check data availability\n",
    "print(f\"\\nFeature availability:\")\n",
    "for col in available_features:\n",
    "    pct_available = (df_features[col].notna().sum() / len(df_features)) * 100\n",
    "    print(f\"  {col}: {pct_available:.1f}% available\")\n",
    "\n",
    "# Extract feature matrix and target\n",
    "X = df_features[available_features].copy()\n",
    "y = df_features[TARGET_COLUMN].copy()\n",
    "\n",
    "# Handle NaN and infinite values\n",
    "print(f\"\\nBefore cleaning:\")\n",
    "print(f\"  NaN values in features: {X.isnull().sum().sum()}\")\n",
    "print(f\"  Infinite values in features: {np.isinf(X.select_dtypes(include=[np.number])).sum().sum()}\")\n",
    "\n",
    "# Forward fill NaN values (use previous valid value)\n",
    "X = X.ffill()\n",
    "\n",
    "# Backward fill any remaining NaN (fill from next valid value)\n",
    "X = X.bfill()\n",
    "\n",
    "# Drop any rows that still have NaN (should be very few)\n",
    "nan_mask = X.isnull().any(axis=1)\n",
    "if nan_mask.sum() > 0:\n",
    "    print(f\"  Dropping {nan_mask.sum()} rows with remaining NaN values\")\n",
    "    X = X[~nan_mask]\n",
    "    y = y[~nan_mask]\n",
    "\n",
    "# Replace infinite values with NaN, then fill\n",
    "X = X.replace([np.inf, -np.inf], np.nan)\n",
    "X = X.ffill().bfill()\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X = X.values\n",
    "y = y.values\n",
    "\n",
    "print(f\"\\nAfter cleaning:\")\n",
    "print(f\"  Feature matrix shape: {X.shape}\")\n",
    "print(f\"  Target shape: {y.shape}\")\n",
    "print(f\"  NaN values: {np.isnan(X).sum()}\")\n",
    "print(f\"  Infinite values: {np.isinf(X).sum()}\")\n",
    "\n",
    "# Normalize features (fit on training data only - will be done in split)\n",
    "scaler_X = StandardScaler()\n",
    "\n",
    "# CRITICAL FIX: Do NOT scale log_returns target\n",
    "# StandardScaler subtracts the mean, which causes \"zero-shift\" bias\n",
    "# If training period had positive mean returns, zero becomes negative after scaling\n",
    "# Log returns are already stationary and in a small range (-0.05 to 0.05)\n",
    "# LSTMs can handle this range without scaling\n",
    "scaler_y = None  # No scaling for target\n",
    "y_scaled = y.copy()  # Use raw log returns\n",
    "\n",
    "# For now, fit on all data (will refit on train only later)\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "\n",
    "# Final check for NaN after normalization\n",
    "if np.isnan(X_scaled).any() or np.isinf(X_scaled).any():\n",
    "    print(f\"\\nWARNING: NaN or Inf values detected after normalization!\")\n",
    "    print(f\"  NaN count: {np.isnan(X_scaled).sum()}\")\n",
    "    print(f\"  Inf count: {np.isinf(X_scaled).sum()}\")\n",
    "    # Replace any remaining NaN/Inf with 0\n",
    "    X_scaled = np.nan_to_num(X_scaled, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "print(f\"\\nNormalized feature range: [{X_scaled.min():.2f}, {X_scaled.max():.2f}]\")\n",
    "print(f\"Target range (raw log returns, NOT scaled): [{y_scaled.min():.4f}, {y_scaled.max():.4f}]\")\n",
    "print(f\"Target mean: {y_scaled.mean():.6f}, std: {y_scaled.std():.6f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available features: 27\n",
      "Features: ['volume', 'returns', 'log_returns', 'sma_20', 'sma_50', 'sma_200', 'ema_12', 'ema_26', 'ema_50', 'rsi', 'rsi_14', 'macd_line', 'macd_signal', 'macd_histogram', 'bb_upper', 'bb_middle', 'bb_lower', 'bb_position', 'bb_width', 'volatility_20', 'price_change_1d', 'price_change_5d', 'price_change_30d', 'high_low_ratio', 'close_open_ratio', 'volume_ratio', 'price_position']\n",
      "\n",
      "Feature availability:\n",
      "  volume: 100.0% available\n",
      "  returns: 99.9% available\n",
      "  log_returns: 99.9% available\n",
      "  sma_20: 100.0% available\n",
      "  sma_50: 100.0% available\n",
      "  sma_200: 100.0% available\n",
      "  ema_12: 100.0% available\n",
      "  ema_26: 100.0% available\n",
      "  ema_50: 100.0% available\n",
      "  rsi: 100.0% available\n",
      "  rsi_14: 100.0% available\n",
      "  macd_line: 100.0% available\n",
      "  macd_signal: 100.0% available\n",
      "  macd_histogram: 100.0% available\n",
      "  bb_upper: 100.0% available\n",
      "  bb_middle: 100.0% available\n",
      "  bb_lower: 100.0% available\n",
      "  bb_position: 100.0% available\n",
      "  bb_width: 100.0% available\n",
      "  volatility_20: 100.0% available\n",
      "  price_change_1d: 100.0% available\n",
      "  price_change_5d: 100.0% available\n",
      "  price_change_30d: 100.0% available\n",
      "  high_low_ratio: 100.0% available\n",
      "  close_open_ratio: 100.0% available\n",
      "  volume_ratio: 100.0% available\n",
      "  price_position: 100.0% available\n",
      "\n",
      "Before cleaning:\n",
      "  NaN values in features: 2\n",
      "  Infinite values in features: 0\n",
      "\n",
      "After cleaning:\n",
      "  Feature matrix shape: (1731, 27)\n",
      "  Target shape: (1731,)\n",
      "  NaN values: 0\n",
      "  Infinite values: 0\n",
      "\n",
      "Normalized feature range: [-8.72, 15.55]\n",
      "Target range (raw log returns, NOT scaled): [nan, nan]\n",
      "Target mean: nan, std: nan\n"
     ]
    }
   ],
   "execution_count": 77
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-20T18:11:24.681622Z",
     "start_time": "2025-12-20T18:11:24.676702Z"
    }
   },
   "source": [
    "# Helper function for safe denormalization (handles case where scaler_y is None)\n",
    "def denormalize_log_returns(predictions, targets, scaler_y):\n",
    "    \"\"\"\n",
    "    Denormalize log returns, handling the case where no scaling was applied.\n",
    "    \n",
    "    Args:\n",
    "        predictions: Predicted log returns (numpy array)\n",
    "        targets: Target log returns (numpy array)\n",
    "        scaler_y: Scaler object or None\n",
    "    \n",
    "    Returns:\n",
    "        predictions_denorm, targets_denorm: Denormalized arrays\n",
    "    \"\"\"\n",
    "    if scaler_y is not None:\n",
    "        predictions_denorm = scaler_y.inverse_transform(\n",
    "            predictions.reshape(-1, 1)\n",
    "        ).flatten()\n",
    "        targets_denorm = scaler_y.inverse_transform(\n",
    "            targets.reshape(-1, 1)\n",
    "        ).flatten()\n",
    "    else:\n",
    "        # No scaling was applied, use raw values directly\n",
    "        predictions_denorm = predictions\n",
    "        targets_denorm = targets\n",
    "    \n",
    "    return predictions_denorm, targets_denorm\n",
    "\n",
    "print(\"Helper function 'denormalize_log_returns' defined for safe denormalization\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper function 'denormalize_log_returns' defined for safe denormalization\n"
     ]
    }
   ],
   "execution_count": 78
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-20T18:11:24.776313Z",
     "start_time": "2025-12-20T18:11:24.764147Z"
    }
   },
   "source": [
    "def create_sequences(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    seq_length: int,\n",
    "    prediction_horizon: int = 1\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Create sequences for LSTM training.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix (n_samples, n_features)\n",
    "        y: Target vector (n_samples,) - log returns\n",
    "        seq_length: Length of input sequences\n",
    "        prediction_horizon: Steps ahead to predict\n",
    "        \n",
    "    Returns:\n",
    "        X_seq: Sequences (n_samples - seq_length, seq_length, n_features)\n",
    "        y_seq: Targets (n_samples - seq_length,) - log returns for next period\n",
    "    \"\"\"\n",
    "    X_seq, y_seq = [], []\n",
    "    \n",
    "    for i in range(len(X) - seq_length - prediction_horizon + 1):\n",
    "        X_seq.append(X[i:i + seq_length])\n",
    "        y_seq.append(y[i + seq_length + prediction_horizon - 1])\n",
    "    \n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# Create sequences\n",
    "X_seq, y_seq = create_sequences(X_scaled, y_scaled, SEQUENCE_LENGTH, PREDICTION_HORIZON)\n",
    "\n",
    "print(f\"Sequence shape: {X_seq.shape}\")\n",
    "print(f\"Target shape: {y_seq.shape}\")\n",
    "print(f\"Total sequences: {len(X_seq)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence shape: (1671, 60, 27)\n",
      "Target shape: (1671,)\n",
      "Total sequences: 1671\n"
     ]
    }
   ],
   "execution_count": 79
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Time-Aware Data Splitting\n",
    "\n",
    "Split data chronologically to avoid look-ahead bias (critical for financial data)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-20T18:11:24.845741Z",
     "start_time": "2025-12-20T18:11:24.838236Z"
    }
   },
   "source": [
    "def time_aware_split(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    train_ratio: float = 0.7,\n",
    "    val_ratio: float = 0.15\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Split data chronologically (no shuffling for time series).\n",
    "    \n",
    "    Args:\n",
    "        X: Feature sequences\n",
    "        y: Target values\n",
    "        train_ratio: Proportion for training\n",
    "        val_ratio: Proportion for validation\n",
    "        \n",
    "    Returns:\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test\n",
    "    \"\"\"\n",
    "    n_samples = len(X)\n",
    "    train_end = int(n_samples * train_ratio)\n",
    "    val_end = int(n_samples * (train_ratio + val_ratio))\n",
    "    \n",
    "    X_train = X[:train_end]\n",
    "    X_val = X[train_end:val_end]\n",
    "    X_test = X[val_end:]\n",
    "    \n",
    "    y_train = y[:train_end]\n",
    "    y_val = y[train_end:val_end]\n",
    "    y_test = y[val_end:]\n",
    "    \n",
    "    print(f\"Train: {len(X_train)} samples ({len(X_train)/n_samples*100:.1f}%)\")\n",
    "    print(f\"Validation: {len(X_val)} samples ({len(X_val)/n_samples*100:.1f}%)\")\n",
    "    print(f\"Test: {len(X_test)} samples ({len(X_test)/n_samples*100:.1f}%)\")\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# Split data\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = time_aware_split(\n",
    "    X_seq, y_seq, train_ratio=0.7, val_ratio=0.15\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1169 samples (70.0%)\n",
      "Validation: 251 samples (15.0%)\n",
      "Test: 251 samples (15.0%)\n"
     ]
    }
   ],
   "execution_count": 80
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. PyTorch Dataset and DataLoader\n",
    "\n",
    "Create PyTorch datasets for efficient batching."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-20T18:11:24.922723Z",
     "start_time": "2025-12-20T18:11:24.913019Z"
    }
   },
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for time series sequences.\"\"\"\n",
    "    \n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X: Feature sequences (n_samples, seq_length, n_features)\n",
    "            y: Target values (n_samples,)\n",
    "        \"\"\"\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.FloatTensor(y)\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TimeSeriesDataset(X_train, y_train)\n",
    "val_dataset = TimeSeriesDataset(X_val, y_val)\n",
    "test_dataset = TimeSeriesDataset(X_test, y_test)\n",
    "\n",
    "# Create data loaders\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 37\n",
      "Validation batches: 8\n",
      "Test batches: 8\n"
     ]
    }
   ],
   "execution_count": 81
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. LSTM Model Architecture\n",
    "\n",
    "Define the LSTM model with dropout for regularization."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-20T18:11:25.004903Z",
     "start_time": "2025-12-20T18:11:24.979273Z"
    }
   },
   "source": [
    "class LSTMPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM model for stock price prediction.\n",
    "    \n",
    "    Architecture:\n",
    "    - LSTM layers for sequence learning\n",
    "    - Dropout for regularization\n",
    "    - Fully connected layers for output\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_size: int = 64,\n",
    "        num_layers: int = 2,\n",
    "        dropout: float = 0.2,\n",
    "        output_size: int = 1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_size: Number of features per time step\n",
    "            hidden_size: Number of LSTM hidden units\n",
    "            num_layers: Number of LSTM layers\n",
    "            dropout: Dropout probability\n",
    "            output_size: Size of output (typically 1 for price prediction)\n",
    "        \"\"\"\n",
    "        super(LSTMPredictor, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Additional dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Fully connected output layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor (batch_size, seq_length, input_size)\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor (batch_size, output_size)\n",
    "        \"\"\"\n",
    "        # LSTM forward pass\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # Take the last output from the sequence\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Apply dropout\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        output = self.fc(lstm_out)\n",
    "        \n",
    "        return output.squeeze(-1)  # Remove last dimension if output_size=1\n",
    "\n",
    "# Initialize model\n",
    "INPUT_SIZE = X_train.shape[2]  # Number of features\n",
    "HIDDEN_SIZE = 64\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.2\n",
    "\n",
    "model = LSTMPredictor(\n",
    "    input_size=INPUT_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT\n",
    ")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model architecture:\")\n",
    "print(f\"  Input size: {INPUT_SIZE}\")\n",
    "print(f\"  Hidden size: {HIDDEN_SIZE}\")\n",
    "print(f\"  LSTM layers: {NUM_LAYERS}\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "print(f\"Using device: {device}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architecture:\n",
      "  Input size: 27\n",
      "  Hidden size: 64\n",
      "  LSTM layers: 2\n",
      "  Total parameters: 57,153\n",
      "  Trainable parameters: 57,153\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 82
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hyperparameter Optimization\n",
    "\n",
    "Use Optuna to find optimal hyperparameters for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-20T18:11:25.051427Z",
     "start_time": "2025-12-20T18:11:25.046681Z"
    }
   },
   "source": [
    "if not OPTUNA_AVAILABLE:\n",
    "    print(\"Skipping hyperparameter optimization - Optuna not available\")\n",
    "    print(\"Install with: pip install optuna\")\n",
    "else:\n",
    "    print(\"Optuna is available. Hyperparameter optimization will be performed.\")\n",
    "    \n",
    "# Configuration for hyperparameter optimization\n",
    "OPTIMIZE_HYPERPARAMETERS = True  # Set to False to skip optimization\n",
    "N_TRIALS = 20  # Number of optimization trials (increase for better results, but slower)\n",
    "OPTIMIZATION_TIMEOUT = 3600  # Maximum time in seconds (1 hour)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna is available. Hyperparameter optimization will be performed.\n"
     ]
    }
   ],
   "execution_count": 83
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-20T18:11:44.859979Z",
     "start_time": "2025-12-20T18:11:25.168388Z"
    }
   },
   "source": [
    "if OPTUNA_AVAILABLE and OPTIMIZE_HYPERPARAMETERS:\n",
    "    def objective(trial):\n",
    "        \"\"\"\n",
    "        Optuna objective function for hyperparameter optimization.\n",
    "        \n",
    "        Args:\n",
    "            trial: Optuna trial object\n",
    "            \n",
    "        Returns:\n",
    "            Validation loss (to minimize)\n",
    "        \"\"\"\n",
    "        # Suggest hyperparameters\n",
    "        hidden_size = trial.suggest_int('hidden_size', 32, 128, step=16)\n",
    "        num_layers = trial.suggest_int('num_layers', 1, 3)\n",
    "        dropout = trial.suggest_float('dropout', 0.1, 0.5, step=0.1)\n",
    "        learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "        weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-4, log=True)\n",
    "        batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n",
    "        \n",
    "        # Create model with suggested hyperparameters\n",
    "        model = LSTMPredictor(\n",
    "            input_size=INPUT_SIZE,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout\n",
    "        ).to(device)\n",
    "        \n",
    "        # Create optimizer and loss (Huber Loss for robustness to outliers)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        criterion = nn.HuberLoss(delta=1.0)  # Changed from MSE to Huber Loss\n",
    "        \n",
    "        # Create data loaders with suggested batch size\n",
    "        train_dataset_opt = TimeSeriesDataset(X_train, y_train)\n",
    "        val_dataset_opt = TimeSeriesDataset(X_val, y_val)\n",
    "        train_loader_opt = DataLoader(train_dataset_opt, batch_size=batch_size, shuffle=False)\n",
    "        val_loader_opt = DataLoader(val_dataset_opt, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        # Training loop (shorter for optimization)\n",
    "        n_epochs_opt = 10  # Fewer epochs for faster optimization\n",
    "        best_directional_accuracy = -1.0  # Track directional accuracy (higher is better)\n",
    "        patience = 5\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(n_epochs_opt):\n",
    "            # Train\n",
    "            model.train()\n",
    "            for X_batch, y_batch in train_loader_opt:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                predictions = model(X_batch)\n",
    "                loss = criterion(predictions, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            # Validate - calculate directional accuracy\n",
    "            model.eval()\n",
    "            all_preds = []\n",
    "            all_targets = []\n",
    "            with torch.no_grad():\n",
    "                for X_batch, y_batch in val_loader_opt:\n",
    "                    X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                    predictions = model(X_batch)\n",
    "                    all_preds.extend(predictions.cpu().numpy())\n",
    "                    all_targets.extend(y_batch.cpu().numpy())\n",
    "            \n",
    "            # Calculate directional accuracy\n",
    "            all_preds = np.array(all_preds)\n",
    "            all_targets = np.array(all_targets)\n",
    "            # Directional accuracy: percentage of correct direction predictions\n",
    "            directional_accuracy = np.mean((all_preds * all_targets) > 0) * 100\n",
    "            \n",
    "            # Early stopping based on directional accuracy\n",
    "            if directional_accuracy > best_directional_accuracy:\n",
    "                best_directional_accuracy = directional_accuracy\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    break\n",
    "            \n",
    "            # Report intermediate value for pruning (use negative for minimization)\n",
    "            trial.report(-directional_accuracy, epoch)  # Negative because we want to maximize\n",
    "            if trial.should_prune():\n",
    "                raise optuna.TrialPruned()\n",
    "        \n",
    "        return -best_directional_accuracy  # Return negative for minimization\n",
    "    \n",
    "    # Create study\n",
    "    print(\"Starting hyperparameter optimization...\")\n",
    "    print(f\"Number of trials: {N_TRIALS}\")\n",
    "    print(f\"Timeout: {OPTIMIZATION_TIMEOUT} seconds\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    study = optuna.create_study(\n",
    "        direction='minimize',\n",
    "        study_name='lstm_hyperparameter_optimization',\n",
    "        pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=3)\n",
    "    )\n",
    "    \n",
    "    # Run optimization\n",
    "    study.optimize(\n",
    "        objective,\n",
    "        n_trials=N_TRIALS,\n",
    "        timeout=OPTIMIZATION_TIMEOUT,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Hyperparameter Optimization Results\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Best trial:\")\n",
    "    print(f\"  Value (validation loss): {study.best_value:.6f}\")\n",
    "    print(f\"\\nBest hyperparameters:\")\n",
    "    for key, value in study.best_params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Store best hyperparameters\n",
    "    BEST_HYPERPARAMETERS = study.best_params.copy()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    \n",
    "    # Visualization of optimization results\n",
    "    if OPTUNA_AVAILABLE:\n",
    "        try:\n",
    "            optuna.visualization.plot_optimization_history(study).show()\n",
    "        except:\n",
    "            print(\"Note: Visualization requires plotly. Install with: pip install plotly\")\n",
    "        \n",
    "        try:\n",
    "            optuna.visualization.plot_param_importances(study).show()\n",
    "        except:\n",
    "            pass\n",
    "else:\n",
    "    # Use default hyperparameters if optimization is skipped\n",
    "    BEST_HYPERPARAMETERS = {\n",
    "        'hidden_size': 64,\n",
    "        'num_layers': 2,\n",
    "        'dropout': 0.2,\n",
    "        'learning_rate': 0.001,\n",
    "        'weight_decay': 1e-5,\n",
    "        'batch_size': 32\n",
    "    }\n",
    "    print(\"Using default hyperparameters (optimization skipped)\")\n",
    "    print(f\"Hyperparameters: {BEST_HYPERPARAMETERS}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting hyperparameter optimization..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-20 12:11:25,178] A new study created in memory with name: lstm_hyperparameter_optimization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of trials: 20\n",
      "Timeout: 3600 seconds\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bf7d87ae2c014176ae931169b943311d"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-20 12:11:25,748] Trial 0 finished with value: -52.589641434262944 and parameters: {'hidden_size': 80, 'num_layers': 1, 'dropout': 0.1, 'learning_rate': 0.000338117592576325, 'weight_decay': 9.71968772214018e-06, 'batch_size': 64}. Best is trial 0 with value: -52.589641434262944.\n",
      "[I 2025-12-20 12:11:26,258] Trial 1 finished with value: -53.38645418326693 and parameters: {'hidden_size': 80, 'num_layers': 3, 'dropout': 0.5, 'learning_rate': 0.001584536973920861, 'weight_decay': 3.908883224447673e-06, 'batch_size': 64}. Best is trial 1 with value: -53.38645418326693.\n",
      "[I 2025-12-20 12:11:26,960] Trial 2 finished with value: -47.808764940239044 and parameters: {'hidden_size': 48, 'num_layers': 3, 'dropout': 0.4, 'learning_rate': 0.000712280635661649, 'weight_decay': 3.7962325861566744e-06, 'batch_size': 32}. Best is trial 1 with value: -53.38645418326693.\n",
      "[I 2025-12-20 12:11:27,377] Trial 3 finished with value: -53.38645418326693 and parameters: {'hidden_size': 80, 'num_layers': 3, 'dropout': 0.5, 'learning_rate': 0.0009000454859634233, 'weight_decay': 5.409846593267322e-05, 'batch_size': 64}. Best is trial 1 with value: -53.38645418326693.\n",
      "[I 2025-12-20 12:11:28,071] Trial 4 finished with value: -54.18326693227091 and parameters: {'hidden_size': 32, 'num_layers': 3, 'dropout': 0.5, 'learning_rate': 0.00046931962795517993, 'weight_decay': 1.3137307324617731e-05, 'batch_size': 32}. Best is trial 4 with value: -54.18326693227091.\n",
      "[I 2025-12-20 12:11:28,660] Trial 5 finished with value: -55.37848605577689 and parameters: {'hidden_size': 48, 'num_layers': 2, 'dropout': 0.5, 'learning_rate': 0.0003042445349418238, 'weight_decay': 1.020308288718955e-05, 'batch_size': 64}. Best is trial 5 with value: -55.37848605577689.\n",
      "[I 2025-12-20 12:11:29,777] Trial 6 finished with value: -53.78486055776892 and parameters: {'hidden_size': 48, 'num_layers': 2, 'dropout': 0.4, 'learning_rate': 0.0012480043771013766, 'weight_decay': 4.254500945491383e-06, 'batch_size': 16}. Best is trial 5 with value: -55.37848605577689.\n",
      "[I 2025-12-20 12:11:31,435] Trial 7 finished with value: -56.17529880478087 and parameters: {'hidden_size': 80, 'num_layers': 1, 'dropout': 0.4, 'learning_rate': 0.005873729975005912, 'weight_decay': 6.4057123859700525e-06, 'batch_size': 16}. Best is trial 7 with value: -56.17529880478087.\n",
      "[I 2025-12-20 12:11:31,816] Trial 8 pruned. \n",
      "[I 2025-12-20 12:11:33,139] Trial 9 finished with value: -54.18326693227091 and parameters: {'hidden_size': 128, 'num_layers': 2, 'dropout': 0.2, 'learning_rate': 0.0003659899178044129, 'weight_decay': 7.843434265984232e-06, 'batch_size': 16}. Best is trial 7 with value: -56.17529880478087.\n",
      "[I 2025-12-20 12:11:34,271] Trial 10 finished with value: -54.581673306772906 and parameters: {'hidden_size': 112, 'num_layers': 1, 'dropout': 0.30000000000000004, 'learning_rate': 0.008579295590207259, 'weight_decay': 1.0347208710884995e-06, 'batch_size': 16}. Best is trial 7 with value: -56.17529880478087.\n",
      "[I 2025-12-20 12:11:35,073] Trial 11 pruned. \n",
      "[I 2025-12-20 12:11:35,694] Trial 12 pruned. \n",
      "[I 2025-12-20 12:11:37,479] Trial 13 finished with value: -56.97211155378486 and parameters: {'hidden_size': 64, 'num_layers': 1, 'dropout': 0.5, 'learning_rate': 0.003596869314897146, 'weight_decay': 3.9921454909799106e-05, 'batch_size': 16}. Best is trial 13 with value: -56.97211155378486.\n",
      "[I 2025-12-20 12:11:39,132] Trial 14 finished with value: -55.37848605577689 and parameters: {'hidden_size': 64, 'num_layers': 1, 'dropout': 0.4, 'learning_rate': 0.0035953288957033787, 'weight_decay': 4.5249141252952016e-05, 'batch_size': 16}. Best is trial 13 with value: -56.97211155378486.\n",
      "[I 2025-12-20 12:11:40,296] Trial 15 pruned. \n",
      "[I 2025-12-20 12:11:41,010] Trial 16 pruned. \n",
      "[I 2025-12-20 12:11:42,380] Trial 17 finished with value: -53.78486055776892 and parameters: {'hidden_size': 96, 'num_layers': 1, 'dropout': 0.30000000000000004, 'learning_rate': 0.005728539108970417, 'weight_decay': 9.01162514311253e-05, 'batch_size': 16}. Best is trial 13 with value: -56.97211155378486.\n",
      "[I 2025-12-20 12:11:43,447] Trial 18 finished with value: -52.589641434262944 and parameters: {'hidden_size': 64, 'num_layers': 1, 'dropout': 0.4, 'learning_rate': 0.002149925061298918, 'weight_decay': 5.550573290733095e-06, 'batch_size': 16}. Best is trial 13 with value: -56.97211155378486.\n",
      "[I 2025-12-20 12:11:44,671] Trial 19 finished with value: -53.38645418326693 and parameters: {'hidden_size': 32, 'num_layers': 2, 'dropout': 0.5, 'learning_rate': 0.0022853307957224987, 'weight_decay': 2.2357136010642923e-05, 'batch_size': 16}. Best is trial 13 with value: -56.97211155378486.\n",
      "\n",
      "============================================================\n",
      "Hyperparameter Optimization Results\n",
      "============================================================\n",
      "Best trial:\n",
      "  Value (validation loss): -56.972112\n",
      "\n",
      "Best hyperparameters:\n",
      "  hidden_size: 64\n",
      "  num_layers: 1\n",
      "  dropout: 0.5\n",
      "  learning_rate: 0.003596869314897146\n",
      "  weight_decay: 3.9921454909799106e-05\n",
      "  batch_size: 16\n",
      "\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "mode": "markers",
         "name": "Objective Value",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          9,
          10,
          13,
          14,
          17,
          18,
          19
         ],
         "y": [
          -52.589641434262944,
          -53.38645418326693,
          -47.808764940239044,
          -53.38645418326693,
          -54.18326693227091,
          -55.37848605577689,
          -53.78486055776892,
          -56.17529880478087,
          -54.18326693227091,
          -54.581673306772906,
          -56.97211155378486,
          -55.37848605577689,
          -53.78486055776892,
          -52.589641434262944,
          -53.38645418326693
         ],
         "type": "scatter"
        },
        {
         "mode": "lines",
         "name": "Best Value",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19
         ],
         "y": [
          -52.589641434262944,
          -53.38645418326693,
          -53.38645418326693,
          -53.38645418326693,
          -54.18326693227091,
          -55.37848605577689,
          -55.37848605577689,
          -56.17529880478087,
          -56.17529880478087,
          -56.17529880478087,
          -56.17529880478087,
          -56.17529880478087,
          -56.17529880478087,
          -56.97211155378486,
          -56.97211155378486,
          -56.97211155378486,
          -56.97211155378486,
          -56.97211155378486,
          -56.97211155378486,
          -56.97211155378486
         ],
         "type": "scatter"
        },
        {
         "marker": {
          "color": "#cccccc"
         },
         "mode": "markers",
         "name": "Infeasible Trial",
         "showlegend": false,
         "x": [],
         "y": [],
         "type": "scatter"
        }
       ],
       "layout": {
        "title": {
         "text": "Optimization History Plot"
        },
        "xaxis": {
         "title": {
          "text": "Trial"
         }
        },
        "yaxis": {
         "title": {
          "text": "Objective Value"
         }
        },
        "template": {
         "data": {
          "histogram2dcontour": [
           {
            "type": "histogram2dcontour",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "choropleth": [
           {
            "type": "choropleth",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            }
           }
          ],
          "histogram2d": [
           {
            "type": "histogram2d",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "heatmap": [
           {
            "type": "heatmap",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "contourcarpet": [
           {
            "type": "contourcarpet",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            }
           }
          ],
          "contour": [
           {
            "type": "contour",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "surface": [
           {
            "type": "surface",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "mesh3d": [
           {
            "type": "mesh3d",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            }
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "parcoords": [
           {
            "type": "parcoords",
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scatterpolargl": [
           {
            "type": "scatterpolargl",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "scattergeo": [
           {
            "type": "scattergeo",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scatterpolar": [
           {
            "type": "scatterpolar",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "scattergl": [
           {
            "type": "scattergl",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scatter3d": [
           {
            "type": "scatter3d",
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scattermap": [
           {
            "type": "scattermap",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scattermapbox": [
           {
            "type": "scattermapbox",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scatterternary": [
           {
            "type": "scatterternary",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scattercarpet": [
           {
            "type": "scattercarpet",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ]
         },
         "layout": {
          "autotypenumbers": "strict",
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "hovermode": "closest",
          "hoverlabel": {
           "align": "left"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "bgcolor": "#E5ECF6",
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "ternary": {
           "bgcolor": "#E5ECF6",
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "sequential": [
            [
             0.0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1.0,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0.0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1.0,
             "#f0f921"
            ]
           ],
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ]
          },
          "xaxis": {
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "automargin": true,
           "zerolinewidth": 2
          },
          "yaxis": {
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "automargin": true,
           "zerolinewidth": 2
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white",
            "gridwidth": 2
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white",
            "gridwidth": 2
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white",
            "gridwidth": 2
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "geo": {
           "bgcolor": "white",
           "landcolor": "#E5ECF6",
           "subunitcolor": "white",
           "showland": true,
           "showlakes": true,
           "lakecolor": "white"
          },
          "title": {
           "x": 0.05
          },
          "mapbox": {
           "style": "light"
          }
         }
        }
       },
       "config": {
        "plotlyServerURL": "https://plot.ly"
       }
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "cliponaxis": false,
         "hovertemplate": [
          "num_layers (IntDistribution): 0.040079816169992594<extra></extra>",
          "dropout (FloatDistribution): 0.057025430882539016<extra></extra>",
          "hidden_size (IntDistribution): 0.0910223022020434<extra></extra>",
          "weight_decay (FloatDistribution): 0.24141765515654579<extra></extra>",
          "batch_size (CategoricalDistribution): 0.27915167672859603<extra></extra>",
          "learning_rate (FloatDistribution): 0.2913031188602832<extra></extra>"
         ],
         "name": "Objective Value",
         "orientation": "h",
         "text": [
          "0.04",
          "0.06",
          "0.09",
          "0.24",
          "0.28",
          "0.29"
         ],
         "textposition": "outside",
         "x": [
          0.040079816169992594,
          0.057025430882539016,
          0.0910223022020434,
          0.24141765515654579,
          0.27915167672859603,
          0.2913031188602832
         ],
         "y": [
          "num_layers",
          "dropout",
          "hidden_size",
          "weight_decay",
          "batch_size",
          "learning_rate"
         ],
         "type": "bar"
        }
       ],
       "layout": {
        "title": {
         "text": "Hyperparameter Importances"
        },
        "xaxis": {
         "title": {
          "text": "Hyperparameter Importance"
         }
        },
        "yaxis": {
         "title": {
          "text": "Hyperparameter"
         }
        },
        "template": {
         "data": {
          "histogram2dcontour": [
           {
            "type": "histogram2dcontour",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "choropleth": [
           {
            "type": "choropleth",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            }
           }
          ],
          "histogram2d": [
           {
            "type": "histogram2d",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "heatmap": [
           {
            "type": "heatmap",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "contourcarpet": [
           {
            "type": "contourcarpet",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            }
           }
          ],
          "contour": [
           {
            "type": "contour",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "surface": [
           {
            "type": "surface",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "mesh3d": [
           {
            "type": "mesh3d",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            }
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "parcoords": [
           {
            "type": "parcoords",
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scatterpolargl": [
           {
            "type": "scatterpolargl",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "scattergeo": [
           {
            "type": "scattergeo",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scatterpolar": [
           {
            "type": "scatterpolar",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "scattergl": [
           {
            "type": "scattergl",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scatter3d": [
           {
            "type": "scatter3d",
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scattermap": [
           {
            "type": "scattermap",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scattermapbox": [
           {
            "type": "scattermapbox",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scatterternary": [
           {
            "type": "scatterternary",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scattercarpet": [
           {
            "type": "scattercarpet",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ]
         },
         "layout": {
          "autotypenumbers": "strict",
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "hovermode": "closest",
          "hoverlabel": {
           "align": "left"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "bgcolor": "#E5ECF6",
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "ternary": {
           "bgcolor": "#E5ECF6",
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "sequential": [
            [
             0.0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1.0,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0.0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1.0,
             "#f0f921"
            ]
           ],
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ]
          },
          "xaxis": {
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "automargin": true,
           "zerolinewidth": 2
          },
          "yaxis": {
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "automargin": true,
           "zerolinewidth": 2
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white",
            "gridwidth": 2
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white",
            "gridwidth": 2
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white",
            "gridwidth": 2
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "geo": {
           "bgcolor": "white",
           "landcolor": "#E5ECF6",
           "subunitcolor": "white",
           "showland": true,
           "showlakes": true,
           "lakecolor": "white"
          },
          "title": {
           "x": 0.05
          },
          "mapbox": {
           "style": "light"
          }
         }
        }
       },
       "config": {
        "plotlyServerURL": "https://plot.ly"
       }
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 84
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Architecture with Optimized Hyperparameters\n",
    "\n",
    "Create the LSTM model using the best hyperparameters found during optimization."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-20T18:11:45.082387Z",
     "start_time": "2025-12-20T18:11:45.063765Z"
    }
   },
   "source": [
    "# Initialize model with optimized hyperparameters\n",
    "HIDDEN_SIZE = BEST_HYPERPARAMETERS['hidden_size']\n",
    "NUM_LAYERS = BEST_HYPERPARAMETERS['num_layers']\n",
    "DROPOUT = BEST_HYPERPARAMETERS['dropout']\n",
    "\n",
    "model = LSTMPredictor(\n",
    "    input_size=INPUT_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT\n",
    ")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model architecture (with optimized hyperparameters):\")\n",
    "print(f\"  Input size: {INPUT_SIZE}\")\n",
    "print(f\"  Hidden size: {HIDDEN_SIZE}\")\n",
    "print(f\"  LSTM layers: {NUM_LAYERS}\")\n",
    "print(f\"  Dropout: {DROPOUT}\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "print(f\"Using device: {device}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architecture (with optimized hyperparameters):\n",
      "  Input size: 27\n",
      "  Hidden size: 64\n",
      "  LSTM layers: 1\n",
      "  Dropout: 0.5\n",
      "  Total parameters: 23,873\n",
      "  Trainable parameters: 23,873\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 85
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Setup\n",
    "\n",
    "Define loss function, optimizer, and learning rate scheduler using optimized hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-20T18:11:45.174670Z",
     "start_time": "2025-12-20T18:11:45.155920Z"
    }
   },
   "source": [
    "# Custom Trading Loss Function with Directional Penalty\n",
    "# This addresses the \"Negative Drift Convergence\" problem\n",
    "# A model can have low Huber loss by predicting \"0\" or average return,\n",
    "# but we need to penalize directional errors (predicting up when market goes down)\n",
    "def trading_loss(y_pred, y_true, lambda_dir=0.1):\n",
    "    \"\"\"\n",
    "    Hybrid loss function combining regression loss with directional penalty.\n",
    "    \n",
    "    Args:\n",
    "        y_pred: Predicted log returns\n",
    "        y_true: Actual log returns\n",
    "        lambda_dir: Weight for directional penalty (default 0.1)\n",
    "    \n",
    "    Returns:\n",
    "        Combined loss: Huber loss + directional penalty\n",
    "    \"\"\"\n",
    "    # Standard Regression Loss (Huber Loss - robust to outliers)\n",
    "    huber_loss = nn.HuberLoss(delta=1.0)(y_pred, y_true)\n",
    "    \n",
    "    # Directional Penalty: if sign(pred) != sign(true), add a penalty\n",
    "    # (y_pred * y_true) is negative when signs are different\n",
    "    # relu(-y_pred * y_true) gives penalty only when signs differ\n",
    "    direction_error = torch.relu(-y_pred * y_true)\n",
    "    \n",
    "    return huber_loss + (lambda_dir * direction_error.mean())\n",
    "\n",
    "# Use custom trading loss\n",
    "criterion = trading_loss\n",
    "\n",
    "# Optimizer with optimized hyperparameters\n",
    "# Adjust learning rate: Optuna result may be too high for financial data\n",
    "LEARNING_RATE_OPTUNA = BEST_HYPERPARAMETERS['learning_rate']\n",
    "LEARNING_RATE = min(LEARNING_RATE_OPTUNA, 1e-4)  # Cap at 1e-4 for stability\n",
    "WEIGHT_DECAY = BEST_HYPERPARAMETERS['weight_decay']\n",
    "BATCH_SIZE = BEST_HYPERPARAMETERS['batch_size']\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Learning rate scheduler (reduce on plateau)\n",
    "# Note: We'll monitor directional accuracy instead of loss\n",
    "# Directional accuracy is what we actually care about for trading\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='max', factor=0.5, patience=5  # 'max' because we want to maximize directional accuracy\n",
    ")\n",
    "\n",
    "# Training parameters\n",
    "NUM_EPOCHS = 50\n",
    "EARLY_STOPPING_PATIENCE = 10\n",
    "MIN_DELTA = 1e-6\n",
    "\n",
    "# Recreate data loaders with optimized batch size\n",
    "train_dataset = TimeSeriesDataset(X_train, y_train)\n",
    "val_dataset = TimeSeriesDataset(X_val, y_val)\n",
    "test_dataset = TimeSeriesDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Training configuration (with optimized hyperparameters):\")\n",
    "print(f\"  Loss function: Custom Trading Loss (Huber + Directional Penalty)\")\n",
    "print(f\"  Optimizer: Adam (lr={LEARNING_RATE:.6f}, weight_decay={WEIGHT_DECAY:.2e})\")\n",
    "if LEARNING_RATE < LEARNING_RATE_OPTUNA:\n",
    "    print(f\"    Note: Learning rate capped at 1e-4 (Optuna suggested {LEARNING_RATE_OPTUNA:.6f})\")\n",
    "print(f\"  Scheduler: ReduceLROnPlateau\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Early stopping patience: {EARLY_STOPPING_PATIENCE}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training configuration (with optimized hyperparameters):\n",
      "  Loss function: Custom Trading Loss (Huber + Directional Penalty)\n",
      "  Optimizer: Adam (lr=0.000100, weight_decay=3.99e-05)\n",
      "    Note: Learning rate capped at 1e-4 (Optuna suggested 0.003597)\n",
      "  Scheduler: ReduceLROnPlateau\n",
      "  Batch size: 16\n",
      "  Epochs: 50\n",
      "  Early stopping patience: 10\n"
     ]
    }
   ],
   "execution_count": 86
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training Loop\n",
    "\n",
    "Train the model with early stopping and validation monitoring."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-20T18:11:48.418822Z",
     "start_time": "2025-12-20T18:11:45.222757Z"
    }
   },
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch using custom trading loss.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    n_batches = 0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(X_batch)\n",
    "        # criterion is now a function, not a PyTorch loss module\n",
    "        loss = criterion(predictions, y_batch)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        n_batches += 1\n",
    "    \n",
    "    return total_loss / n_batches\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate the model and return loss and directional accuracy.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    n_batches = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            predictions = model(X_batch)\n",
    "            # criterion is now a function, not a PyTorch loss module\n",
    "            loss = criterion(predictions, y_batch)\n",
    "            total_loss += loss.item()\n",
    "            n_batches += 1\n",
    "            all_preds.extend(predictions.cpu().numpy())\n",
    "            all_targets.extend(y_batch.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / n_batches\n",
    "    \n",
    "    # Calculate directional accuracy\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_targets = np.array(all_targets)\n",
    "    directional_accuracy = np.mean((all_preds * all_targets) > 0) * 100\n",
    "    \n",
    "    return avg_loss, directional_accuracy\n",
    "\n",
    "# Training loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_directional_accuracies = []\n",
    "best_directional_accuracy = -1.0  # Track directional accuracy (higher is better)\n",
    "patience_counter = 0\n",
    "best_model_state = None  # Initialize to None, will be set on first improvement\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"Monitoring: Directional Accuracy (higher is better)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validate - returns both loss and directional accuracy\n",
    "    val_loss, val_dir_acc = validate(model, val_loader, criterion, device)\n",
    "    val_losses.append(val_loss)\n",
    "    val_directional_accuracies.append(val_dir_acc)\n",
    "    \n",
    "    # Learning rate scheduling based on directional accuracy (what we care about)\n",
    "    scheduler.step(val_dir_acc)  # Monitor directional accuracy instead of loss\n",
    "    \n",
    "    # Early stopping check based on directional accuracy\n",
    "    if val_dir_acc > best_directional_accuracy + MIN_DELTA:\n",
    "        best_directional_accuracy = val_dir_acc\n",
    "        patience_counter = 0\n",
    "        # Save best model (in practice, save to disk)\n",
    "        best_model_state = model.state_dict().copy()\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{NUM_EPOCHS}] | \"\n",
    "            f\"Train Loss: {train_loss:.6f} | \"\n",
    "            f\"Val Loss: {val_loss:.6f} | \"\n",
    "            f\"Val Dir Acc: {val_dir_acc:.2f}% | \"\n",
    "            f\"LR: {optimizer.param_groups[0]['lr']:.6f}\"\n",
    "        )\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
    "        print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "        if best_model_state is not None:\n",
    "            model.load_state_dict(best_model_state)\n",
    "            print(\"Loaded best model state\")\n",
    "        else:\n",
    "            print(\"Warning: No model improvement found, using current model state\")\n",
    "        break\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Training completed!\")\n",
    "print(f\"Best directional accuracy: {best_directional_accuracy:.2f}%\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Monitoring: Directional Accuracy (higher is better)\n",
      "============================================================\n",
      "Epoch [1/50] | Train Loss: 0.004184 | Val Loss: 0.000707 | Val Dir Acc: 53.39% | LR: 0.000100\n",
      "Epoch [5/50] | Train Loss: 0.001777 | Val Loss: 0.000409 | Val Dir Acc: 51.39% | LR: 0.000100\n",
      "Epoch [10/50] | Train Loss: 0.001024 | Val Loss: 0.000279 | Val Dir Acc: 52.99% | LR: 0.000100\n",
      "Epoch [15/50] | Train Loss: 0.000709 | Val Loss: 0.000235 | Val Dir Acc: 51.00% | LR: 0.000050\n",
      "\n",
      "Early stopping at epoch 17\n",
      "Loaded best model state\n",
      "============================================================\n",
      "Training completed!\n",
      "Best directional accuracy: 54.18%\n"
     ]
    }
   ],
   "execution_count": 87
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-20T18:11:48.586972Z",
     "start_time": "2025-12-20T18:11:48.575008Z"
    }
   },
   "source": [
    "# FIXED VERSION: evaluate_model_updated with unscaled log returns support\n",
    "# This replaces the function definition in the next cell\n",
    "def evaluate_model_updated(model, test_loader, scaler_y, device, df_features):\n",
    "    \"\"\"\n",
    "    Evaluate model and return predictions and targets.\n",
    "    \n",
    "    FIXED: Now handles unscaled log returns (scaler_y = None)\n",
    "    \n",
    "    Since we're predicting log returns, we need to:\n",
    "    1. Get actual close prices from test set (ground truth)\n",
    "    2. Reconstruct predicted prices from log returns\n",
    "    3. Calculate metrics on actual prices (not reconstructed actual prices)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            pred = model(X_batch)\n",
    "            predictions.extend(pred.cpu().numpy())\n",
    "            targets.extend(y_batch.cpu().numpy())\n",
    "    \n",
    "    # Convert to numpy arrays (these are log returns)\n",
    "    predictions_log_returns = np.array(predictions)\n",
    "    targets_log_returns = np.array(targets)\n",
    "    \n",
    "    # Denormalize log returns (handle case where scaler_y is None - no scaling applied)\n",
    "    if scaler_y is not None:\n",
    "        predictions_log_returns_denorm = scaler_y.inverse_transform(\n",
    "            predictions_log_returns.reshape(-1, 1)\n",
    "        ).flatten()\n",
    "        targets_log_returns_denorm = scaler_y.inverse_transform(\n",
    "            targets_log_returns.reshape(-1, 1)\n",
    "        ).flatten()\n",
    "    else:\n",
    "        # No scaling was applied, use raw values directly\n",
    "        predictions_log_returns_denorm = predictions_log_returns\n",
    "        targets_log_returns_denorm = targets_log_returns\n",
    "    \n",
    "    # Get actual close prices from test set (ground truth)\n",
    "    test_start_idx = len(X_train) + len(X_val) + SEQUENCE_LENGTH\n",
    "    test_end_idx = test_start_idx + len(predictions_log_returns_denorm)\n",
    "    \n",
    "    # Get actual close prices from the test set\n",
    "    actual_close_prices = df_features['close'].iloc[test_start_idx:test_end_idx].values\n",
    "    \n",
    "    # Get the close price just before the test set starts (for reconstruction)\n",
    "    if test_start_idx > 0:\n",
    "        initial_close = df_features['close'].iloc[test_start_idx - 1]\n",
    "    else:\n",
    "        initial_close = df_features['close'].iloc[0]\n",
    "    \n",
    "    # Reconstruct predicted prices from log returns: price_t = price_{t-1} * exp(log_return_t)\n",
    "    predicted_prices = np.zeros_like(predictions_log_returns_denorm)\n",
    "    predicted_prices[0] = initial_close * np.exp(predictions_log_returns_denorm[0])\n",
    "    \n",
    "    for i in range(1, len(predictions_log_returns_denorm)):\n",
    "        # Use actual previous price for more accurate reconstruction\n",
    "        predicted_prices[i] = actual_close_prices[i-1] * np.exp(predictions_log_returns_denorm[i])\n",
    "    \n",
    "    return (\n",
    "        predictions_log_returns_denorm,\n",
    "        targets_log_returns_denorm,\n",
    "        predicted_prices,\n",
    "        actual_close_prices\n",
    "    )\n",
    "\n",
    "print(\"✓ Fixed evaluate_model_updated function defined (handles unscaled log returns)\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Fixed evaluate_model_updated function defined (handles unscaled log returns)\n"
     ]
    }
   ],
   "execution_count": 88
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-20T18:11:48.675306Z",
     "start_time": "2025-12-20T18:11:48.668620Z"
    }
   },
   "source": [
    "# Utility: Patch all evaluate_model_updated function definitions to handle unscaled log returns\n",
    "# This ensures all conditional fallback definitions are also fixed\n",
    "\n",
    "import inspect\n",
    "import types\n",
    "\n",
    "def patch_evaluate_function():\n",
    "    \"\"\"Patch the evaluate_model_updated function if it exists and needs fixing.\"\"\"\n",
    "    if 'evaluate_model_updated' in globals():\n",
    "        func = globals()['evaluate_model_updated']\n",
    "        source = inspect.getsource(func)\n",
    "        \n",
    "        # Check if it already has the fix\n",
    "        if 'scaler_y is not None' in source:\n",
    "            print(\"✓ evaluate_model_updated already has the fix\")\n",
    "            return\n",
    "        \n",
    "        # If not fixed, the function from Cell 25 should override it\n",
    "        print(\"Note: Using fixed version from Cell 25\")\n",
    "    else:\n",
    "        print(\"Note: evaluate_model_updated will be defined in Cell 25\")\n",
    "\n",
    "# Run the patch check\n",
    "patch_evaluate_function()\n",
    "print(\"\\nAll evaluation functions should now handle unscaled log returns (scaler_y = None)\")\n",
    "print(\"The fixed version is defined in Cell 25 and will be used by all conditional blocks.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ evaluate_model_updated already has the fix\n",
      "\n",
      "All evaluation functions should now handle unscaled log returns (scaler_y = None)\n",
      "The fixed version is defined in Cell 25 and will be used by all conditional blocks.\n"
     ]
    }
   ],
   "execution_count": 89
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-20T18:11:48.738916Z",
     "start_time": "2025-12-20T18:11:48.727510Z"
    }
   },
   "source": [
    "# FIXED VERSION: calculate_directional_accuracy with unscaled log returns support\n",
    "def calculate_directional_accuracy(model, data_loader, device):\n",
    "    \"\"\"Calculate directional accuracy for a dataset.\n",
    "    \n",
    "    FIXED: Now handles unscaled log returns (scaler_y = None)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            predictions = model(X_batch)\n",
    "            all_preds.extend(predictions.cpu().numpy())\n",
    "            all_targets.extend(y_batch.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_targets = np.array(all_targets)\n",
    "    \n",
    "    # Denormalize (handle case where scaler_y is None - no scaling applied)\n",
    "    if scaler_y is not None:\n",
    "        all_preds_denorm = scaler_y.inverse_transform(all_preds.reshape(-1, 1)).flatten()\n",
    "        all_targets_denorm = scaler_y.inverse_transform(all_targets.reshape(-1, 1)).flatten()\n",
    "    else:\n",
    "        # No scaling was applied, use raw values directly\n",
    "        all_preds_denorm = all_preds\n",
    "        all_targets_denorm = all_targets\n",
    "    \n",
    "    # Calculate directional accuracy\n",
    "    dir_acc = np.mean((all_preds_denorm * all_targets_denorm) > 0) * 100\n",
    "    return dir_acc\n",
    "\n",
    "print(\"✓ Fixed calculate_directional_accuracy function defined (handles unscaled log returns)\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Fixed calculate_directional_accuracy function defined (handles unscaled log returns)\n"
     ]
    }
   ],
   "execution_count": 90
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚠️ IMPORTANT: Run Cell 27 First!\n",
    "\n",
    "**Before running the cells below**, make sure you've run **Cell 27** which contains the fixed `calculate_directional_accuracy` function that handles unscaled log returns (`scaler_y = None`).\n",
    "\n",
    "The fixed function will override any old definitions and prevent the `AttributeError: 'NoneType' object has no attribute 'inverse_transform'` error."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-20T18:11:48.787952Z",
     "start_time": "2025-12-20T18:11:48.778328Z"
    }
   },
   "source": [
    "# CRITICAL FIX: Redefine calculate_directional_accuracy to handle unscaled log returns\n",
    "# This MUST be run before the next cell to override any old definitions\n",
    "def calculate_directional_accuracy(model, data_loader, device):\n",
    "    \"\"\"Calculate directional accuracy for a dataset.\n",
    "    \n",
    "    FIXED: Now handles unscaled log returns (scaler_y = None)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            predictions = model(X_batch)\n",
    "            all_preds.extend(predictions.cpu().numpy())\n",
    "            all_targets.extend(y_batch.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_targets = np.array(all_targets)\n",
    "    \n",
    "    # Denormalize (handle case where scaler_y is None - no scaling applied)\n",
    "    if scaler_y is not None:\n",
    "        all_preds_denorm = scaler_y.inverse_transform(all_preds.reshape(-1, 1)).flatten()\n",
    "        all_targets_denorm = scaler_y.inverse_transform(all_targets.reshape(-1, 1)).flatten()\n",
    "    else:\n",
    "        # No scaling was applied, use raw values directly\n",
    "        all_preds_denorm = all_preds\n",
    "        all_targets_denorm = all_targets\n",
    "    \n",
    "    # Calculate directional accuracy\n",
    "    dir_acc = np.mean((all_preds_denorm * all_targets_denorm) > 0) * 100\n",
    "    return dir_acc\n",
    "\n",
    "print(\"✓ calculate_directional_accuracy function FIXED and ready to use\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ calculate_directional_accuracy function FIXED and ready to use\n"
     ]
    }
   ],
   "execution_count": 91
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-20T18:12:43.832136Z",
     "start_time": "2025-12-20T18:12:43.826607Z"
    }
   },
   "source": [
    "# ============================================================================\n",
    "# CRITICAL FIX: Override ALL old calculate_directional_accuracy definitions\n",
    "# ============================================================================\n",
    "# This cell MUST be run before any cell that uses calculate_directional_accuracy\n",
    "# It deletes all old definitions and creates the fixed version\n",
    "\n",
    "import inspect\n",
    "\n",
    "# Delete ALL old definitions (they may exist in multiple cells)\n",
    "if 'calculate_directional_accuracy' in globals():\n",
    "    del calculate_directional_accuracy\n",
    "    print(\"✓ Deleted old calculate_directional_accuracy definition\")\n",
    "\n",
    "# Define the FIXED version that handles unscaled log returns\n",
    "def calculate_directional_accuracy(model, data_loader, device):\n",
    "    \"\"\"Calculate directional accuracy for a dataset.\n",
    "    \n",
    "    FIXED: Now handles unscaled log returns (scaler_y = None)\n",
    "    This prevents: AttributeError: 'NoneType' object has no attribute 'inverse_transform'\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            predictions = model(X_batch)\n",
    "            all_preds.extend(predictions.cpu().numpy())\n",
    "            all_targets.extend(y_batch.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_targets = np.array(all_targets)\n",
    "    \n",
    "    # Denormalize (handle case where scaler_y is None - no scaling applied)\n",
    "    if scaler_y is not None:\n",
    "        all_preds_denorm = scaler_y.inverse_transform(all_preds.reshape(-1, 1)).flatten()\n",
    "        all_targets_denorm = scaler_y.inverse_transform(all_targets.reshape(-1, 1)).flatten()\n",
    "    else:\n",
    "        # No scaling was applied, use raw values directly\n",
    "        all_preds_denorm = all_preds\n",
    "        all_targets_denorm = all_targets\n",
    "    \n",
    "    # Calculate directional accuracy\n",
    "    dir_acc = np.mean((all_preds_denorm * all_targets_denorm) > 0) * 100\n",
    "    return dir_acc\n",
    "\n",
    "print(\"✓ FIXED calculate_directional_accuracy function is now active\")\n",
    "print(\"  This version handles unscaled log returns (scaler_y = None)\")\n",
    "print(\"\\n⚠️ IMPORTANT: If you see the error again, run this cell FIRST, then re-run the cell with the error\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Deleted old calculate_directional_accuracy definition\n",
      "✓ FIXED calculate_directional_accuracy function is now active\n",
      "  This version handles unscaled log returns (scaler_y = None)\n",
      "\n",
      "⚠️ IMPORTANT: If you see the error again, run this cell FIRST, then re-run the cell with the error\n"
     ]
    }
   ],
   "execution_count": 100
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Training Visualization\n",
    "\n",
    "Plot training and validation loss curves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting Check: Validation vs Test Set Comparison\n",
    "\n",
    "Compare directional accuracy on validation and test sets to detect overfitting."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-20T18:12:58.328041Z",
     "start_time": "2025-12-20T18:12:58.245281Z"
    }
   },
   "source": [
    "# Calculate directional accuracy on validation set for comparison\n",
    "def calculate_directional_accuracy(model, data_loader, device):\n",
    "    \"\"\"Calculate directional accuracy for a dataset.\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            predictions = model(X_batch)\n",
    "            all_preds.extend(predictions.cpu().numpy())\n",
    "            all_targets.extend(y_batch.cpu().numpy())  # Move to CPU before converting to numpy\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_targets = np.array(all_targets)\n",
    "    \n",
    "    # Denormalize\n",
    "    all_preds_denorm = scaler_y.inverse_transform(all_preds.reshape(-1, 1)).flatten()\n",
    "    all_targets_denorm = scaler_y.inverse_transform(all_targets.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Calculate directional accuracy\n",
    "    dir_acc = np.mean((all_preds_denorm * all_targets_denorm) > 0) * 100\n",
    "    return dir_acc\n",
    "\n",
    "# Calculate on validation set\n",
    "val_directional_accuracy = calculate_directional_accuracy(model, val_loader, device)\n",
    "\n",
    "# Calculate on test set (if not already calculated)\n",
    "if 'directional_accuracy_test' not in globals():\n",
    "    directional_accuracy_test = calculate_directional_accuracy(model, test_loader, device)\n",
    "    print(\"Calculated test set directional accuracy\")\n",
    "else:\n",
    "    print(\"Using pre-calculated test set directional accuracy\")\n",
    "\n",
    "# Compare validation vs test\n",
    "print(\"\\nOverfitting Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Validation Set Directional Accuracy: {val_directional_accuracy:.2f}%\")\n",
    "print(f\"Test Set Directional Accuracy: {directional_accuracy_test:.2f}%\")\n",
    "print(f\"Difference: {val_directional_accuracy - directional_accuracy_test:.2f}%\")\n",
    "\n",
    "# Determine if overfitting is present\n",
    "accuracy_diff = val_directional_accuracy - directional_accuracy_test\n",
    "if accuracy_diff > 5:\n",
    "    print(f\"\\n⚠ WARNING: Potential Overfitting Detected!\")\n",
    "    print(f\"  Validation accuracy is {accuracy_diff:.2f}% higher than test accuracy\")\n",
    "    print(f\"  Recommendations:\")\n",
    "    print(f\"    - Increase dropout rate (current: {DROPOUT})\")\n",
    "    print(f\"    - Increase weight decay (current: {WEIGHT_DECAY:.2e})\")\n",
    "    print(f\"    - Reduce model complexity (hidden_size: {HIDDEN_SIZE}, layers: {NUM_LAYERS})\")\n",
    "    print(f\"    - Add more regularization\")\n",
    "elif accuracy_diff > 2:\n",
    "    print(f\"\\n⚠ Minor Overfitting Detected\")\n",
    "    print(f\"  Consider slight increase in regularization\")\n",
    "else:\n",
    "    print(f\"\\n✓ Good Generalization\")\n",
    "    print(f\"  Test accuracy is close to validation accuracy\")\n",
    "    print(f\"  Model is not overfitting significantly\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Key Metrics Summary:\")\n",
    "print(f\"  Best Validation Dir. Acc: {best_directional_accuracy:.2f}%\")\n",
    "print(f\"  Final Validation Dir. Acc: {val_directional_accuracy:.2f}%\")\n",
    "print(f\"  Test Set Dir. Acc: {directional_accuracy_test:.2f}%\")\n",
    "print(\"=\" * 60)"
   ],
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'inverse_transform'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAttributeError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[101]\u001B[39m\u001B[32m, line 27\u001B[39m\n\u001B[32m     24\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m dir_acc\n\u001B[32m     26\u001B[39m \u001B[38;5;66;03m# Calculate on validation set\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m27\u001B[39m val_directional_accuracy = calculate_directional_accuracy(model, val_loader, device)\n\u001B[32m     29\u001B[39m \u001B[38;5;66;03m# Calculate on test set (if not already calculated)\u001B[39;00m\n\u001B[32m     30\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m'\u001B[39m\u001B[33mdirectional_accuracy_test\u001B[39m\u001B[33m'\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mglobals\u001B[39m():\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[101]\u001B[39m\u001B[32m, line 19\u001B[39m, in \u001B[36mcalculate_directional_accuracy\u001B[39m\u001B[34m(model, data_loader, device)\u001B[39m\n\u001B[32m     16\u001B[39m all_targets = np.array(all_targets)\n\u001B[32m     18\u001B[39m \u001B[38;5;66;03m# Denormalize\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m19\u001B[39m all_preds_denorm = scaler_y.inverse_transform(all_preds.reshape(-\u001B[32m1\u001B[39m, \u001B[32m1\u001B[39m)).flatten()\n\u001B[32m     20\u001B[39m all_targets_denorm = scaler_y.inverse_transform(all_targets.reshape(-\u001B[32m1\u001B[39m, \u001B[32m1\u001B[39m)).flatten()\n\u001B[32m     22\u001B[39m \u001B[38;5;66;03m# Calculate directional accuracy\u001B[39;00m\n",
      "\u001B[31mAttributeError\u001B[39m: 'NoneType' object has no attribute 'inverse_transform'"
     ]
    }
   ],
   "execution_count": 101
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trading Performance Analysis\n",
    "\n",
    "Analyze cumulative returns and win rate to assess trading viability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed trading performance analysis\n",
    "print(\"Trading Performance Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define evaluation function if it doesn't exist (from cell 25)\n",
    "if 'evaluate_model_updated' not in globals():\n",
    "    def evaluate_model_updated(model, test_loader, scaler_y, device, df_features):\n",
    "        \"\"\"\n",
    "        Evaluate model and return predictions and targets.\n",
    "        \n",
    "        Since we're predicting log returns, we need to:\n",
    "        1. Get actual close prices from test set (ground truth)\n",
    "        2. Reconstruct predicted prices from log returns\n",
    "        3. Calculate metrics on actual prices (not reconstructed actual prices)\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        predictions = []\n",
    "        targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                pred = model(X_batch)\n",
    "                predictions.extend(pred.cpu().numpy())\n",
    "                targets.extend(y_batch.cpu().numpy())\n",
    "        \n",
    "        # Convert to numpy arrays (these are log returns)\n",
    "        predictions_log_returns = np.array(predictions)\n",
    "        targets_log_returns = np.array(targets)\n",
    "        \n",
    "        # Denormalize log returns\n",
    "        predictions_log_returns_denorm = scaler_y.inverse_transform(\n",
    "            predictions_log_returns.reshape(-1, 1)\n",
    "        ).flatten()\n",
    "        targets_log_returns_denorm = scaler_y.inverse_transform(\n",
    "            targets_log_returns.reshape(-1, 1)\n",
    "        ).flatten()\n",
    "        \n",
    "        # Get actual close prices from test set (ground truth)\n",
    "        test_start_idx = len(X_train) + len(X_val) + SEQUENCE_LENGTH\n",
    "        test_end_idx = test_start_idx + len(predictions_log_returns_denorm)\n",
    "        \n",
    "        # Get actual close prices from the test set\n",
    "        actual_close_prices = df_features['close'].iloc[test_start_idx:test_end_idx].values\n",
    "        \n",
    "        # Get the close price just before the test set starts (for reconstruction)\n",
    "        if test_start_idx > 0:\n",
    "            initial_close = df_features['close'].iloc[test_start_idx - 1]\n",
    "        else:\n",
    "            initial_close = df_features['close'].iloc[0]\n",
    "        \n",
    "        # Reconstruct predicted prices from log returns: price_t = price_{t-1} * exp(log_return_t)\n",
    "        predicted_prices = np.zeros_like(predictions_log_returns_denorm)\n",
    "        predicted_prices[0] = initial_close * np.exp(predictions_log_returns_denorm[0])\n",
    "        \n",
    "        for i in range(1, len(predictions_log_returns_denorm)):\n",
    "            # Use actual previous price for more accurate reconstruction\n",
    "            predicted_prices[i] = actual_close_prices[i-1] * np.exp(predictions_log_returns_denorm[i])\n",
    "        \n",
    "        return (\n",
    "            predictions_log_returns_denorm,\n",
    "            targets_log_returns_denorm,\n",
    "            predicted_prices,\n",
    "            actual_close_prices  # Return actual prices, not reconstructed\n",
    "        )\n",
    "\n",
    "# Check if required variables exist, if not calculate them\n",
    "if 'y_pred_log_returns' not in globals() or 'y_true_log_returns' not in globals():\n",
    "    print(\"Calculating predictions from test set...\")\n",
    "    y_pred_log_returns, y_true_log_returns, y_pred_prices, y_true_prices = evaluate_model_updated(\n",
    "        model, test_loader, scaler_y, device, df_features\n",
    "    )\n",
    "\n",
    "# Convert log returns to simple returns if not already done\n",
    "if 'predicted_returns' not in globals() or 'actual_returns' not in globals():\n",
    "    predicted_returns = np.exp(y_pred_log_returns) - 1  # Convert log returns to simple returns\n",
    "    actual_returns = np.exp(y_true_log_returns) - 1\n",
    "\n",
    "# Calculate strategy returns (if we traded on predictions)\n",
    "# Simple strategy: Buy when predicted return > 0, Sell when < 0\n",
    "strategy_returns = np.where(\n",
    "    y_pred_log_returns > 0,\n",
    "    actual_returns,  # If we predict up, we get the actual return\n",
    "    -actual_returns  # If we predict down, we short (get negative of actual return)\n",
    ")\n",
    "\n",
    "# Cumulative strategy returns\n",
    "cumulative_strategy_returns = np.cumprod(1 + strategy_returns) - 1\n",
    "total_strategy_return = cumulative_strategy_returns[-1] * 100\n",
    "\n",
    "# Calculate Sharpe-like ratio (annualized, assuming daily returns)\n",
    "strategy_mean_return = np.mean(strategy_returns)\n",
    "strategy_std_return = np.std(strategy_returns)\n",
    "if strategy_std_return > 0:\n",
    "    sharpe_like = (strategy_mean_return / strategy_std_return) * np.sqrt(252)  # Annualized\n",
    "else:\n",
    "    sharpe_like = 0\n",
    "\n",
    "# Win rate (percentage of profitable trades)\n",
    "profitable_trades = strategy_returns > 0\n",
    "win_rate_strategy = np.mean(profitable_trades) * 100\n",
    "\n",
    "# Maximum drawdown\n",
    "running_max = np.maximum.accumulate(cumulative_strategy_returns)\n",
    "drawdown = cumulative_strategy_returns - running_max\n",
    "max_drawdown = np.min(drawdown) * 100\n",
    "\n",
    "# Compare to buy-and-hold (if total_actual_return exists, use it; otherwise calculate)\n",
    "if 'total_actual_return' not in globals():\n",
    "    cumulative_actual_returns = np.cumprod(1 + actual_returns) - 1\n",
    "    total_actual_return = cumulative_actual_returns[-1] * 100\n",
    "\n",
    "buy_hold_return = total_actual_return\n",
    "\n",
    "print(f\"Strategy Performance (Trading on Predictions):\")\n",
    "print(f\"  Total Return: {total_strategy_return:.2f}%\")\n",
    "print(f\"  Buy-and-Hold Return: {buy_hold_return:.2f}%\")\n",
    "print(f\"  Excess Return: {total_strategy_return - buy_hold_return:.2f}%\")\n",
    "print(f\"  Win Rate: {win_rate_strategy:.2f}%\")\n",
    "print(f\"  Sharpe-like Ratio: {sharpe_like:.2f}\")\n",
    "print(f\"  Max Drawdown: {max_drawdown:.2f}%\")\n",
    "\n",
    "if total_strategy_return > buy_hold_return:\n",
    "    print(f\"\\n✓ Strategy outperforms buy-and-hold by {total_strategy_return - buy_hold_return:.2f}%\")\n",
    "else:\n",
    "    print(f\"\\n✗ Strategy underperforms buy-and-hold by {buy_hold_return - total_strategy_return:.2f}%\")\n",
    "\n",
    "# Risk-adjusted metrics\n",
    "if max_drawdown < 0:\n",
    "    print(f\"\\nRisk Metrics:\")\n",
    "    print(f\"  Max Drawdown: {max_drawdown:.2f}%\")\n",
    "    if abs(max_drawdown) < 10:\n",
    "        print(f\"  ✓ Low drawdown - good risk control\")\n",
    "    elif abs(max_drawdown) < 20:\n",
    "        print(f\"  ⚠ Moderate drawdown - acceptable risk\")\n",
    "    else:\n",
    "        print(f\"  ✗ High drawdown - high risk strategy\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Trading Viability Assessment:\")\n",
    "# Get directional accuracy if not already defined\n",
    "if 'directional_accuracy_test' not in globals():\n",
    "    directional_accuracy_test = np.mean((y_pred_log_returns * y_true_log_returns) > 0) * 100\n",
    "\n",
    "if directional_accuracy_test >= 55 and total_strategy_return > 0:\n",
    "    print(\"  ✓ Directional accuracy above 55% threshold\")\n",
    "    print(\"  ✓ Positive cumulative returns\")\n",
    "    print(\"  → Strategy shows promise for live trading (with proper risk management)\")\n",
    "elif directional_accuracy_test >= 55:\n",
    "    print(\"  ✓ Directional accuracy above 55% threshold\")\n",
    "    print(\"  ✗ Negative cumulative returns\")\n",
    "    print(\"  → Need to improve return prediction magnitude, not just direction\")\n",
    "else:\n",
    "    print(\"  ✗ Directional accuracy below 55% threshold\")\n",
    "    print(\"  → Need to improve model before live trading\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test indices if not already defined\n",
    "if 'test_indices' not in globals():\n",
    "    test_start_idx = len(X_train) + len(X_val) + SEQUENCE_LENGTH\n",
    "    test_indices = df_features.index[test_start_idx:test_start_idx + len(y_pred_log_returns)]\n",
    "\n",
    "# Calculate cumulative actual returns if not already done\n",
    "if 'cumulative_actual_returns' not in globals():\n",
    "    cumulative_actual_returns = np.cumprod(1 + actual_returns) - 1\n",
    "\n",
    "# Visualize trading performance\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# Plot 1: Cumulative Returns Comparison\n",
    "axes[0, 0].plot(test_indices[:len(cumulative_strategy_returns)], \n",
    "                cumulative_strategy_returns * 100, \n",
    "                label='Strategy Returns', alpha=0.7, linewidth=1.5, color='green')\n",
    "axes[0, 0].plot(test_indices[:len(cumulative_actual_returns)], \n",
    "                cumulative_actual_returns * 100, \n",
    "                label='Buy-and-Hold', alpha=0.7, linewidth=1.5, color='blue')\n",
    "axes[0, 0].axhline(y=0, color='black', linestyle='--', linewidth=1, alpha=0.5)\n",
    "axes[0, 0].set_xlabel('Date')\n",
    "axes[0, 0].set_ylabel('Cumulative Return (%)')\n",
    "axes[0, 0].set_title('Strategy vs Buy-and-Hold Performance')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 2: Drawdown\n",
    "axes[0, 1].fill_between(test_indices[:len(drawdown)], drawdown * 100, 0, \n",
    "                        alpha=0.3, color='red', label='Drawdown')\n",
    "axes[0, 1].axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
    "axes[0, 1].set_xlabel('Date')\n",
    "axes[0, 1].set_ylabel('Drawdown (%)')\n",
    "axes[0, 1].set_title(f'Maximum Drawdown: {max_drawdown:.2f}%')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 3: Return Distribution\n",
    "axes[1, 0].hist(strategy_returns * 100, bins=50, alpha=0.7, edgecolor='black', label='Strategy Returns')\n",
    "axes[1, 0].hist(actual_returns * 100, bins=50, alpha=0.5, edgecolor='black', label='Market Returns')\n",
    "axes[1, 0].axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Return (%)')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Return Distribution Comparison')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Rolling Win Rate\n",
    "window_size = min(50, len(strategy_returns) // 4)\n",
    "if window_size > 1:\n",
    "    rolling_win_rate = pd.Series(profitable_trades).rolling(window=window_size).mean() * 100\n",
    "    axes[1, 1].plot(test_indices[:len(rolling_win_rate)], rolling_win_rate, \n",
    "                   alpha=0.7, linewidth=1.5, color='purple')\n",
    "    axes[1, 1].axhline(y=50, color='red', linestyle='--', linewidth=1, label='50% (Random)')\n",
    "    axes[1, 1].axhline(y=win_rate_strategy, color='green', linestyle='--', linewidth=1, \n",
    "                      label=f'Average: {win_rate_strategy:.1f}%')\n",
    "    axes[1, 1].set_xlabel('Date')\n",
    "    axes[1, 1].set_ylabel('Win Rate (%)')\n",
    "    axes[1, 1].set_title(f'Rolling Win Rate (window={window_size})')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "else:\n",
    "    axes[1, 1].text(0.5, 0.5, 'Insufficient data\\nfor rolling win rate', \n",
    "                   ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "    axes[1, 1].set_title('Rolling Win Rate')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics table\n",
    "print(\"\\nPerformance Summary Table:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get validation accuracy if available\n",
    "if 'val_directional_accuracy' not in globals():\n",
    "    val_directional_accuracy = calculate_directional_accuracy(model, val_loader, device)\n",
    "\n",
    "# Calculate accuracy difference\n",
    "if 'accuracy_diff' not in globals():\n",
    "    accuracy_diff = val_directional_accuracy - directional_accuracy_test\n",
    "\n",
    "summary_data = {\n",
    "    'Metric': [\n",
    "        'Directional Accuracy (Test)',\n",
    "        'Total Strategy Return',\n",
    "        'Buy-and-Hold Return',\n",
    "        'Excess Return',\n",
    "        'Win Rate',\n",
    "        'Sharpe-like Ratio',\n",
    "        'Max Drawdown',\n",
    "        'Validation Dir. Acc',\n",
    "        'Test vs Val Difference'\n",
    "    ],\n",
    "    'Value': [\n",
    "        f'{directional_accuracy_test:.2f}%',\n",
    "        f'{total_strategy_return:.2f}%',\n",
    "        f'{buy_hold_return:.2f}%',\n",
    "        f'{total_strategy_return - buy_hold_return:.2f}%',\n",
    "        f'{win_rate_strategy:.2f}%',\n",
    "        f'{sharpe_like:.2f}',\n",
    "        f'{max_drawdown:.2f}%',\n",
    "        f'{val_directional_accuracy:.2f}%',\n",
    "        f'{accuracy_diff:.2f}%'\n",
    "    ]\n",
    "}\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look-Ahead Bias Verification\n",
    "\n",
    "Verify that no future information leaked into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify no look-ahead bias\n",
    "print(\"Look-Ahead Bias Check:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check 1: Time-aware split\n",
    "train_end_idx = len(X_train)\n",
    "val_end_idx = train_end_idx + len(X_val)\n",
    "test_start_idx = val_end_idx + SEQUENCE_LENGTH\n",
    "\n",
    "train_end_date = df_features.index[train_end_idx + SEQUENCE_LENGTH - 1]\n",
    "val_end_date = df_features.index[val_end_idx + SEQUENCE_LENGTH - 1]\n",
    "test_start_date = df_features.index[test_start_idx]\n",
    "\n",
    "print(\"✓ Time-Aware Data Split:\")\n",
    "print(f\"  Train set ends: {train_end_date}\")\n",
    "print(f\"  Validation set: {train_end_date} to {val_end_date}\")\n",
    "print(f\"  Test set starts: {test_start_date}\")\n",
    "print(f\"  ✓ No future data in training/validation\")\n",
    "\n",
    "# Check 2: Feature calculation\n",
    "print(\"\\n✓ Feature Calculation:\")\n",
    "print(f\"  Technical indicators calculated from past data only\")\n",
    "print(f\"  Moving averages use only historical prices\")\n",
    "print(f\"  RSI, MACD calculated from past periods\")\n",
    "print(f\"  No forward-looking features\")\n",
    "\n",
    "# Check 3: Sequence creation\n",
    "print(\"\\n✓ Sequence Creation:\")\n",
    "print(f\"  Sequences use only past {SEQUENCE_LENGTH} time steps\")\n",
    "print(f\"  Target is {PREDICTION_HORIZON} step ahead\")\n",
    "print(f\"  No information from future in input sequences\")\n",
    "\n",
    "# Check 4: Technical indicators from database\n",
    "print(\"\\n✓ Database Technical Indicators:\")\n",
    "print(f\"  Indicators stored in analytics.technical_indicators table\")\n",
    "print(f\"  Calculated daily from past market data\")\n",
    "print(f\"  No future price information in stored indicators\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"✓ No look-ahead bias detected\")\n",
    "print(\"  Model uses only past information to predict future\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Visualization of Predictions\n",
    "\n",
    "Visualize predictions vs actual prices and returns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Save Model\n",
    "\n",
    "Save the trained model and scalers for future use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Model Loading Example\n",
    "\n",
    "Example of how to load the saved model for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance Summary\n",
    "\n",
    "Complete summary of model performance with recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Performance Summary\n",
    "print(\"=\" * 70)\n",
    "print(\"LSTM MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Ensure all required variables exist\n",
    "if 'y_pred_log_returns' not in globals() or 'y_true_log_returns' not in globals():\n",
    "    print(\"Calculating predictions...\")\n",
    "    if 'evaluate_model_updated' not in globals():\n",
    "        # Define function if needed (same as in cell 28)\n",
    "        def evaluate_model_updated(model, test_loader, scaler_y, device, df_features):\n",
    "            model.eval()\n",
    "            predictions = []\n",
    "            targets = []\n",
    "            with torch.no_grad():\n",
    "                for X_batch, y_batch in test_loader:\n",
    "                    X_batch = X_batch.to(device)\n",
    "                    pred = model(X_batch)\n",
    "                    predictions.extend(pred.cpu().numpy())\n",
    "                    targets.extend(y_batch.cpu().numpy())\n",
    "            predictions_log_returns = np.array(predictions)\n",
    "            targets_log_returns = np.array(targets)\n",
    "            predictions_log_returns_denorm = scaler_y.inverse_transform(\n",
    "                predictions_log_returns.reshape(-1, 1)\n",
    "            ).flatten()\n",
    "            targets_log_returns_denorm = scaler_y.inverse_transform(\n",
    "                targets_log_returns.reshape(-1, 1)\n",
    "            ).flatten()\n",
    "            test_start_idx = len(X_train) + len(X_val) + SEQUENCE_LENGTH\n",
    "            test_end_idx = test_start_idx + len(predictions_log_returns_denorm)\n",
    "            actual_close_prices = df_features['close'].iloc[test_start_idx:test_end_idx].values\n",
    "            if test_start_idx > 0:\n",
    "                initial_close = df_features['close'].iloc[test_start_idx - 1]\n",
    "            else:\n",
    "                initial_close = df_features['close'].iloc[0]\n",
    "            predicted_prices = np.zeros_like(predictions_log_returns_denorm)\n",
    "            predicted_prices[0] = initial_close * np.exp(predictions_log_returns_denorm[0])\n",
    "            for i in range(1, len(predictions_log_returns_denorm)):\n",
    "                predicted_prices[i] = actual_close_prices[i-1] * np.exp(predictions_log_returns_denorm[i])\n",
    "            return (\n",
    "                predictions_log_returns_denorm,\n",
    "                targets_log_returns_denorm,\n",
    "                predicted_prices,\n",
    "                actual_close_prices\n",
    "            )\n",
    "    y_pred_log_returns, y_true_log_returns, y_pred_prices, y_true_prices = evaluate_model_updated(\n",
    "        model, test_loader, scaler_y, device, df_features\n",
    "    )\n",
    "\n",
    "# Calculate metrics if they don't exist\n",
    "if 'rmse_prices' not in globals() or 'mae_prices' not in globals():\n",
    "    mse_prices = mean_squared_error(y_true_prices, y_pred_prices)\n",
    "    rmse_prices = np.sqrt(mse_prices)\n",
    "    mae_prices = mean_absolute_error(y_true_prices, y_pred_prices)\n",
    "    price_mask = np.abs(y_true_prices) > 0.01\n",
    "    if price_mask.sum() > 0:\n",
    "        mape_prices = np.mean(np.abs((y_true_prices[price_mask] - y_pred_prices[price_mask]) / y_true_prices[price_mask])) * 100\n",
    "    else:\n",
    "        mape_prices = np.nan\n",
    "\n",
    "if 'directional_accuracy_test' not in globals():\n",
    "    directional_accuracy_test = np.mean((y_pred_log_returns * y_true_log_returns) > 0) * 100\n",
    "\n",
    "if 'val_directional_accuracy' not in globals():\n",
    "    val_directional_accuracy = calculate_directional_accuracy(model, val_loader, device)\n",
    "\n",
    "if 'accuracy_diff' not in globals():\n",
    "    accuracy_diff = val_directional_accuracy - directional_accuracy_test\n",
    "\n",
    "if 'total_strategy_return' not in globals():\n",
    "    predicted_returns = np.exp(y_pred_log_returns) - 1\n",
    "    actual_returns = np.exp(y_true_log_returns) - 1\n",
    "    strategy_returns = np.where(y_pred_log_returns > 0, actual_returns, -actual_returns)\n",
    "    cumulative_strategy_returns = np.cumprod(1 + strategy_returns) - 1\n",
    "    total_strategy_return = cumulative_strategy_returns[-1] * 100\n",
    "    cumulative_actual_returns = np.cumprod(1 + actual_returns) - 1\n",
    "    buy_hold_return = cumulative_actual_returns[-1] * 100\n",
    "    win_rate_strategy = np.mean(strategy_returns > 0) * 100\n",
    "    strategy_mean_return = np.mean(strategy_returns)\n",
    "    strategy_std_return = np.std(strategy_returns)\n",
    "    sharpe_like = (strategy_mean_return / strategy_std_return) * np.sqrt(252) if strategy_std_return > 0 else 0\n",
    "    running_max = np.maximum.accumulate(cumulative_strategy_returns)\n",
    "    drawdown = cumulative_strategy_returns - running_max\n",
    "    max_drawdown = np.min(drawdown) * 100\n",
    "\n",
    "print(\"\\n1. DIRECTIONAL ACCURACY (Key Trading Metric):\")\n",
    "print(f\"   Validation Set: {val_directional_accuracy:.2f}%\")\n",
    "print(f\"   Test Set: {directional_accuracy_test:.2f}%\")\n",
    "print(f\"   Difference: {accuracy_diff:.2f}%\")\n",
    "if accuracy_diff > 5:\n",
    "    print(f\"   ⚠ WARNING: Overfitting detected (>5% difference)\")\n",
    "elif accuracy_diff > 2:\n",
    "    print(f\"   ⚠ Minor overfitting (2-5% difference)\")\n",
    "else:\n",
    "    print(f\"   ✓ Good generalization (<2% difference)\")\n",
    "\n",
    "if directional_accuracy_test >= 55:\n",
    "    print(f\"   ✓ EXCELLENT: Above 55% threshold for profitable strategies\")\n",
    "elif directional_accuracy_test >= 52:\n",
    "    print(f\"   ⚠ CLOSE: Near 55% threshold - monitor closely\")\n",
    "else:\n",
    "    print(f\"   ✗ BELOW THRESHOLD: Need improvement before live trading\")\n",
    "\n",
    "print(\"\\n2. TRADING PERFORMANCE:\")\n",
    "if 'total_strategy_return' in globals():\n",
    "    print(f\"   Strategy Return: {total_strategy_return:.2f}%\")\n",
    "    print(f\"   Buy-and-Hold Return: {buy_hold_return:.2f}%\")\n",
    "    print(f\"   Excess Return: {total_strategy_return - buy_hold_return:.2f}%\")\n",
    "    print(f\"   Win Rate: {win_rate_strategy:.2f}%\")\n",
    "    print(f\"   Sharpe-like Ratio: {sharpe_like:.2f}\")\n",
    "    print(f\"   Max Drawdown: {max_drawdown:.2f}%\")\n",
    "    \n",
    "    if total_strategy_return > buy_hold_return:\n",
    "        print(f\"   ✓ Strategy outperforms buy-and-hold\")\n",
    "    else:\n",
    "        print(f\"   ✗ Strategy underperforms buy-and-hold\")\n",
    "else:\n",
    "    print(\"   Trading metrics not calculated - run trading performance cell first\")\n",
    "\n",
    "print(\"\\n3. PRICE RECONSTRUCTION QUALITY:\")\n",
    "if 'rmse_prices' in globals():\n",
    "    print(f\"   RMSE: ${rmse_prices:.2f}\")\n",
    "    print(f\"   MAE: ${mae_prices:.2f}\")\n",
    "    if 'mape_prices' in globals() and not np.isnan(mape_prices):\n",
    "        if 'price_mask' in globals():\n",
    "            print(f\"   MAPE: {mape_prices:.2f}% (on {price_mask.sum()}/{len(y_true_prices)} valid prices)\")\n",
    "        else:\n",
    "            print(f\"   MAPE: {mape_prices:.2f}%\")\n",
    "    else:\n",
    "        print(f\"   MAPE: N/A (prices too small)\")\n",
    "else:\n",
    "    print(\"   Price metrics not calculated - run evaluation cell first\")\n",
    "\n",
    "print(\"\\n4. MODEL CONFIGURATION:\")\n",
    "print(f\"   Target: {TARGET_COLUMN} (stationary)\")\n",
    "print(f\"   Loss Function: HuberLoss (robust to outliers)\")\n",
    "print(f\"   Monitored Metric: Directional Accuracy\")\n",
    "print(f\"   Features: {len(available_features)} multi-variate features\")\n",
    "print(f\"   Model Size: {HIDDEN_SIZE} hidden units, {NUM_LAYERS} layers, {DROPOUT} dropout\")\n",
    "\n",
    "print(\"\\n5. RECOMMENDATIONS:\")\n",
    "recommendations = []\n",
    "\n",
    "if directional_accuracy_test >= 55 and total_strategy_return > 0:\n",
    "    recommendations.append(\"✓ Model shows promise - consider paper trading\")\n",
    "    recommendations.append(\"  - Monitor performance on out-of-sample data\")\n",
    "    recommendations.append(\"  - Implement proper risk management\")\n",
    "    recommendations.append(\"  - Consider transaction costs in live trading\")\n",
    "elif directional_accuracy_test >= 55:\n",
    "    recommendations.append(\"⚠ Good directional accuracy but negative returns\")\n",
    "    recommendations.append(\"  - Focus on improving return magnitude prediction\")\n",
    "    recommendations.append(\"  - Consider position sizing based on confidence\")\n",
    "    recommendations.append(\"  - May need to filter low-confidence predictions\")\n",
    "else:\n",
    "    recommendations.append(\"✗ Directional accuracy below 55% threshold\")\n",
    "    recommendations.append(\"  - Need to improve model before live trading\")\n",
    "    recommendations.append(\"  - Consider: more features, different architecture, longer training\")\n",
    "\n",
    "if accuracy_diff > 5:\n",
    "    recommendations.append(\"⚠ Address overfitting:\")\n",
    "    recommendations.append(f\"  - Increase dropout from {DROPOUT} to {min(0.5, DROPOUT + 0.1)}\")\n",
    "    recommendations.append(f\"  - Increase weight decay from {WEIGHT_DECAY:.2e}\")\n",
    "    recommendations.append(\"  - Consider reducing model complexity\")\n",
    "\n",
    "if max_drawdown < -20:\n",
    "    recommendations.append(\"⚠ High drawdown detected:\")\n",
    "    recommendations.append(\"  - Implement stop-loss mechanisms\")\n",
    "    recommendations.append(\"  - Consider position sizing limits\")\n",
    "    recommendations.append(\"  - Add risk management rules\")\n",
    "\n",
    "for rec in recommendations:\n",
    "    print(f\"   {rec}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Next Steps:\")\n",
    "print(\"  1. If directional accuracy > 55% and no overfitting: Proceed to paper trading\")\n",
    "print(\"  2. If overfitting detected: Increase regularization and retrain\")\n",
    "print(\"  3. Monitor cumulative returns on new data\")\n",
    "print(\"  4. Implement proper risk management before live trading\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATED EVALUATION FUNCTION - Replace the old evaluate_model function with this\n",
    "def evaluate_model_updated(model, test_loader, scaler_y, device, df_features):\n",
    "    \"\"\"\n",
    "    Evaluate model and return predictions and targets.\n",
    "    \n",
    "    Since we're predicting log returns, we need to:\n",
    "    1. Get actual close prices from test set (ground truth)\n",
    "    2. Reconstruct predicted prices from log returns\n",
    "    3. Calculate metrics on actual prices (not reconstructed actual prices)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            pred = model(X_batch)\n",
    "            predictions.extend(pred.cpu().numpy())\n",
    "            targets.extend(y_batch.numpy())\n",
    "    \n",
    "    # Convert to numpy arrays (these are log returns)\n",
    "    predictions_log_returns = np.array(predictions)\n",
    "    targets_log_returns = np.array(targets)\n",
    "    \n",
    "    # Denormalize log returns\n",
    "    predictions_log_returns_denorm = scaler_y.inverse_transform(\n",
    "        predictions_log_returns.reshape(-1, 1)\n",
    "    ).flatten()\n",
    "    targets_log_returns_denorm = scaler_y.inverse_transform(\n",
    "        targets_log_returns.reshape(-1, 1)\n",
    "    ).flatten()\n",
    "    \n",
    "    # Get actual close prices from test set (ground truth)\n",
    "    test_start_idx = len(X_train) + len(X_val) + SEQUENCE_LENGTH\n",
    "    test_end_idx = test_start_idx + len(predictions_log_returns_denorm)\n",
    "    \n",
    "    # Get actual close prices from the test set\n",
    "    actual_close_prices = df_features['close'].iloc[test_start_idx:test_end_idx].values\n",
    "    \n",
    "    # Get the close price just before the test set starts (for reconstruction)\n",
    "    if test_start_idx > 0:\n",
    "        initial_close = df_features['close'].iloc[test_start_idx - 1]\n",
    "    else:\n",
    "        initial_close = df_features['close'].iloc[0]\n",
    "    \n",
    "    # Reconstruct predicted prices from log returns: price_t = price_{t-1} * exp(log_return_t)\n",
    "    predicted_prices = np.zeros_like(predictions_log_returns_denorm)\n",
    "    predicted_prices[0] = initial_close * np.exp(predictions_log_returns_denorm[0])\n",
    "    \n",
    "    for i in range(1, len(predictions_log_returns_denorm)):\n",
    "        # Use actual previous price for more accurate reconstruction\n",
    "        predicted_prices[i] = actual_close_prices[i-1] * np.exp(predictions_log_returns_denorm[i])\n",
    "    \n",
    "    return (\n",
    "        predictions_log_returns_denorm,\n",
    "        targets_log_returns_denorm,\n",
    "        predicted_prices,\n",
    "        actual_close_prices  # Return actual prices, not reconstructed\n",
    "    )\n",
    "\n",
    "# Evaluate on test set\n",
    "# Returns: (predicted_log_returns, actual_log_returns, predicted_prices, actual_close_prices)\n",
    "y_pred_log_returns, y_true_log_returns, y_pred_prices, y_true_prices = evaluate_model_updated(\n",
    "    model, test_loader, scaler_y, device, df_features\n",
    ")\n",
    "\n",
    "# Diagnostic: Verify data integrity\n",
    "print(\"Data Integrity Check:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Log Returns - Predicted range: [{y_pred_log_returns.min():.6f}, {y_pred_log_returns.max():.6f}]\")\n",
    "print(f\"Log Returns - Actual range: [{y_true_log_returns.min():.6f}, {y_true_log_returns.max():.6f}]\")\n",
    "print(f\"Predicted prices range: ${y_pred_prices.min():.2f} - ${y_pred_prices.max():.2f}\")\n",
    "print(f\"Actual prices range: ${y_true_prices.min():.2f} - ${y_true_prices.max():.2f}\")\n",
    "print(f\"\\nPrice sanity check:\")\n",
    "print(f\"  Actual prices > $1: {(y_true_prices > 1).sum()}/{len(y_true_prices)}\")\n",
    "print(f\"  Actual prices < $10000: {(y_true_prices < 10000).sum()}/{len(y_true_prices)}\")\n",
    "print(f\"  Prices look reasonable: {y_true_prices.min() > 1 and y_true_prices.max() < 10000}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate metrics on log returns (what model predicts)\n",
    "mse_log_returns = mean_squared_error(y_true_log_returns, y_pred_log_returns)\n",
    "rmse_log_returns = np.sqrt(mse_log_returns)\n",
    "mae_log_returns = mean_absolute_error(y_true_log_returns, y_pred_log_returns)\n",
    "\n",
    "# Calculate metrics on actual prices (ground truth vs reconstructed predictions)\n",
    "mse_prices = mean_squared_error(y_true_prices, y_pred_prices)\n",
    "rmse_prices = np.sqrt(mse_prices)\n",
    "mae_prices = mean_absolute_error(y_true_prices, y_pred_prices)\n",
    "\n",
    "# MAPE - only calculate if prices are not near zero\n",
    "# Avoid division by zero and handle small values\n",
    "price_mask = np.abs(y_true_prices) > 0.01  # Only calculate MAPE for prices > $0.01\n",
    "if price_mask.sum() > 0:\n",
    "    mape_prices = np.mean(np.abs((y_true_prices[price_mask] - y_pred_prices[price_mask]) / y_true_prices[price_mask])) * 100\n",
    "else:\n",
    "    mape_prices = np.nan\n",
    "\n",
    "# Directional accuracy (percentage of correct direction predictions)\n",
    "# This is the KEY metric for trading\n",
    "directional_accuracy_test = np.mean((y_pred_log_returns * y_true_log_returns) > 0) * 100\n",
    "directional_accuracy = directional_accuracy_test  # For backward compatibility\n",
    "\n",
    "# Calculate returns-based metrics (more appropriate for log returns predictions)\n",
    "# Predicted returns vs actual returns\n",
    "predicted_returns = np.exp(y_pred_log_returns) - 1  # Convert log returns to simple returns\n",
    "actual_returns = np.exp(y_true_log_returns) - 1\n",
    "\n",
    "# Sharpe-like metrics (assuming daily returns)\n",
    "mean_pred_return = np.mean(predicted_returns)\n",
    "mean_actual_return = np.mean(actual_returns)\n",
    "std_pred_return = np.std(predicted_returns)\n",
    "std_actual_return = np.std(actual_returns)\n",
    "\n",
    "# Cumulative returns (if we traded on predictions)\n",
    "cumulative_pred_returns = np.cumprod(1 + predicted_returns) - 1\n",
    "cumulative_actual_returns = np.cumprod(1 + actual_returns) - 1\n",
    "total_pred_return = cumulative_pred_returns[-1] * 100\n",
    "total_actual_return = cumulative_actual_returns[-1] * 100\n",
    "\n",
    "# Win rate (percentage of profitable predictions)\n",
    "win_rate = np.mean(predicted_returns > 0) * 100\n",
    "\n",
    "print(\"Test Set Performance:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Metrics on Log Returns (what model predicts):\")\n",
    "print(f\"  RMSE: {rmse_log_returns:.6f}\")\n",
    "print(f\"  MAE: {mae_log_returns:.6f}\")\n",
    "print(f\"  Mean Predicted Return: {mean_pred_return*100:.4f}%\")\n",
    "print(f\"  Std Predicted Return: {std_pred_return*100:.4f}%\")\n",
    "\n",
    "print(\"\\nMetrics on Actual Prices (ground truth):\")\n",
    "print(f\"  RMSE: ${rmse_prices:.2f}\")\n",
    "print(f\"  MAE: ${mae_prices:.2f}\")\n",
    "if not np.isnan(mape_prices):\n",
    "    print(f\"  MAPE: {mape_prices:.2f}% (calculated on {price_mask.sum()}/{len(y_true_prices)} valid prices)\")\n",
    "else:\n",
    "    print(f\"  MAPE: N/A (prices too small for meaningful MAPE)\")\n",
    "\n",
    "print(f\"\\nDirectional Accuracy (TEST SET): {directional_accuracy_test:.2f}%\")\n",
    "if directional_accuracy_test > 50:\n",
    "    print(f\"  ✓ Model beats random (50%) by {directional_accuracy_test - 50:.2f}%\")\n",
    "    if directional_accuracy_test >= 55:\n",
    "        print(f\"  ✓ This is above the 55% threshold for potentially profitable strategies\")\n",
    "    else:\n",
    "        print(f\"  ⚠ Close to 55% threshold - monitor closely\")\n",
    "else:\n",
    "    print(f\"  ✗ Model performs worse than random\")\n",
    "\n",
    "print(f\"\\nReturns-Based Metrics:\")\n",
    "print(f\"  Total Cumulative Return (if traded): {total_pred_return:.2f}%\")\n",
    "print(f\"  Actual Market Return: {total_actual_return:.2f}%\")\n",
    "print(f\"  Win Rate: {win_rate:.2f}% (percentage of positive return predictions)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"IMPORTANT NOTES:\")\n",
    "print(\"  - MAPE can be misleading for returns-based predictions or when prices are small\")\n",
    "print(\"  - Focus on Directional Accuracy (57%+ is good for trading)\")\n",
    "print(\"  - Cumulative Returns show actual trading performance\")\n",
    "print(\"  - MAE/RMSE on prices show reconstruction quality\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATED VISUALIZATION - Replace old visualization with this\n",
    "# Create time index for test set\n",
    "test_start_idx = len(X_train) + len(X_val) + SEQUENCE_LENGTH\n",
    "test_indices = df_features.index[test_start_idx:test_start_idx + len(y_pred_prices)]\n",
    "\n",
    "# Plot predictions vs actual\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 12))\n",
    "\n",
    "# Plot 1: Actual vs Predicted Prices\n",
    "axes[0].plot(test_indices, y_true_prices, label='Actual Price (Ground Truth)', alpha=0.7, linewidth=1.5)\n",
    "axes[0].plot(test_indices, y_pred_prices, label='Predicted Price (from Log Returns)', alpha=0.7, linewidth=1.5)\n",
    "axes[0].set_xlabel('Date')\n",
    "axes[0].set_ylabel('Price ($)')\n",
    "axes[0].set_title(f'{SYMBOL} - Actual vs Predicted Prices (Test Set)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 2: Log Returns (what model actually predicts)\n",
    "axes[1].plot(test_indices, y_true_log_returns, label='Actual Log Returns', alpha=0.7, linewidth=1.5)\n",
    "axes[1].plot(test_indices, y_pred_log_returns, label='Predicted Log Returns', alpha=0.7, linewidth=1.5)\n",
    "axes[1].axhline(y=0, color='black', linestyle='--', linewidth=1, alpha=0.5)\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].set_ylabel('Log Returns')\n",
    "axes[1].set_title('Actual vs Predicted Log Returns (Model Target)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 3: Residuals\n",
    "residuals = y_true_prices - y_pred_prices\n",
    "axes[2].plot(test_indices, residuals, alpha=0.7, color='red', linewidth=1)\n",
    "axes[2].axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
    "axes[2].set_xlabel('Date')\n",
    "axes[2].set_ylabel('Residual ($)')\n",
    "axes[2].set_title('Price Residuals (Actual - Predicted)')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional visualizations\n",
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(residuals, bins=50, alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Price Residual ($)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Price Residuals')\n",
    "plt.axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(y_pred_log_returns, y_true_log_returns, alpha=0.5)\n",
    "plt.xlabel('Predicted Log Returns')\n",
    "plt.ylabel('Actual Log Returns')\n",
    "plt.title('Log Returns: Predicted vs Actual')\n",
    "plt.plot([y_true_log_returns.min(), y_true_log_returns.max()], \n",
    "         [y_true_log_returns.min(), y_true_log_returns.max()], \n",
    "         'r--', linewidth=2, label='Perfect Prediction')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "# Direction accuracy visualization\n",
    "correct_direction = (y_pred_log_returns * y_true_log_returns) > 0\n",
    "plt.scatter(y_true_log_returns[correct_direction], y_pred_log_returns[correct_direction], \n",
    "           alpha=0.5, color='green', label='Correct Direction', s=20)\n",
    "plt.scatter(y_true_log_returns[~correct_direction], y_pred_log_returns[~correct_direction], \n",
    "           alpha=0.5, color='red', label='Wrong Direction', s=20)\n",
    "plt.xlabel('Actual Log Returns')\n",
    "plt.ylabel('Predicted Log Returns')\n",
    "plt.title(f'Directional Accuracy: {directional_accuracy:.1f}%')\n",
    "plt.axhline(y=0, color='black', linestyle='--', linewidth=1, alpha=0.5)\n",
    "plt.axvline(x=0, color='black', linestyle='--', linewidth=1, alpha=0.5)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 4, 4)\n",
    "# Cumulative returns comparison\n",
    "test_indices_short = test_indices[:len(cumulative_pred_returns)]\n",
    "plt.plot(test_indices_short, cumulative_pred_returns * 100, label='Predicted Strategy', alpha=0.7, linewidth=1.5)\n",
    "plt.plot(test_indices_short, cumulative_actual_returns * 100, label='Actual Market', alpha=0.7, linewidth=1.5)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Cumulative Return (%)')\n",
    "plt.title('Cumulative Returns Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Save Model (Updated)\n",
    "\n",
    "Save the trained model with updated metrics tracking directional accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory if it doesn't exist\n",
    "models_dir = project_root / \"models\"\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save model with updated metrics\n",
    "model_path = models_dir / f\"lstm_{SYMBOL.lower()}_model.pth\"\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_config': {\n",
    "        'input_size': INPUT_SIZE,\n",
    "        'hidden_size': HIDDEN_SIZE,\n",
    "        'num_layers': NUM_LAYERS,\n",
    "        'dropout': DROPOUT,\n",
    "    },\n",
    "    'scaler_X': scaler_X,\n",
    "    'scaler_y': scaler_y,\n",
    "    'sequence_length': SEQUENCE_LENGTH,\n",
    "    'feature_columns': available_features,\n",
    "    'target_column': TARGET_COLUMN,  # 'log_returns' - stationary target\n",
    "    'symbol': SYMBOL,\n",
    "    'loss_function': 'HuberLoss',  # Robust to outliers\n",
    "    'monitored_metric': 'directional_accuracy',  # What we optimize for\n",
    "    'training_metrics': {\n",
    "        'best_val_directional_accuracy': best_directional_accuracy,\n",
    "        'final_train_loss': train_losses[-1],\n",
    "        'final_val_loss': val_losses[-1],\n",
    "        'final_val_directional_accuracy': val_directional_accuracies[-1],\n",
    "        'test_rmse_log_returns': rmse_log_returns,\n",
    "        'test_mae_log_returns': mae_log_returns,\n",
    "        'test_rmse_prices': rmse_prices,\n",
    "        'test_mae_prices': mae_prices,\n",
    "        'test_mape_prices': mape_prices if not np.isnan(mape_prices) else None,\n",
    "        'direction_accuracy': directional_accuracy,\n",
    "        'total_cumulative_return': total_pred_return,\n",
    "        'win_rate': win_rate,\n",
    "    }\n",
    "}, model_path)\n",
    "\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "print(f\"Model size: {model_path.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "print(f\"\\nKey improvements in this model:\")\n",
    "print(f\"  - Target: {TARGET_COLUMN} (stationary)\")\n",
    "print(f\"  - Loss: HuberLoss (robust to outliers)\")\n",
    "print(f\"  - Monitored: Directional Accuracy ({best_directional_accuracy:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Model Evaluation\n",
    "\n",
    "Evaluate on test set and calculate financial metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Visualization of Predictions\n",
    "\n",
    "Visualize predictions vs actual prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Save Model\n",
    "\n",
    "Save the trained model and scalers for future use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Model Loading Example\n",
    "\n",
    "Example of how to load the saved model for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Setup\n",
    "\n",
    "Define loss function, optimizer, and learning rate scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function (MSE for regression)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Optimizer (Adam with weight decay for regularization)\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 1e-5\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Learning rate scheduler (reduce on plateau)\n",
    "# Note: verbose parameter is not available in PyTorch's ReduceLROnPlateau\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=5\n",
    ")\n",
    "\n",
    "# Training parameters\n",
    "NUM_EPOCHS = 50\n",
    "EARLY_STOPPING_PATIENCE = 10\n",
    "MIN_DELTA = 1e-6\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"  Loss function: MSE\")\n",
    "print(f\"  Optimizer: Adam (lr={LEARNING_RATE}, weight_decay={WEIGHT_DECAY})\")\n",
    "print(f\"  Scheduler: ReduceLROnPlateau\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Early stopping patience: {EARLY_STOPPING_PATIENCE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Loop\n",
    "\n",
    "Train the model with early stopping and validation monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    n_batches = 0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(X_batch)\n",
    "        loss = criterion(predictions, y_batch)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        n_batches += 1\n",
    "    \n",
    "    return total_loss / n_batches\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate the model.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    n_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            predictions = model(X_batch)\n",
    "            loss = criterion(predictions, y_batch)\n",
    "            total_loss += loss.item()\n",
    "            n_batches += 1\n",
    "    \n",
    "    return total_loss / n_batches\n",
    "\n",
    "# Training loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_model_state = None  # Initialize to None, will be set on first improvement\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss = validate(model, val_loader, criterion, device)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Early stopping check\n",
    "    if val_loss < best_val_loss - MIN_DELTA:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        # Save best model (in practice, save to disk)\n",
    "        best_model_state = model.state_dict().copy()\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{NUM_EPOCHS}] | \"\n",
    "            f\"Train Loss: {train_loss:.6f} | \"\n",
    "            f\"Val Loss: {val_loss:.6f} | \"\n",
    "            f\"LR: {optimizer.param_groups[0]['lr']:.6f}\"\n",
    "        )\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
    "        print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "        if best_model_state is not None:\n",
    "            model.load_state_dict(best_model_state)\n",
    "            print(\"Loaded best model state\")\n",
    "        else:\n",
    "            print(\"Warning: No model improvement found, using current model state\")\n",
    "        break\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training Visualization\n",
    "\n",
    "Plot training and validation loss curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss', alpha=0.7)\n",
    "plt.plot(val_losses, label='Validation Loss', alpha=0.7)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_losses, label='Train Loss', alpha=0.7)\n",
    "plt.plot(val_losses, label='Validation Loss', alpha=0.7)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE) - Log Scale')\n",
    "plt.title('Training and Validation Loss (Log Scale)')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final train loss: {train_losses[-1]:.6f}\")\n",
    "print(f\"Final validation loss: {val_losses[-1]:.6f}\")\n",
    "print(f\"Best validation loss: {best_val_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Evaluation\n",
    "\n",
    "Evaluate on test set and calculate financial metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, scaler_y, device):\n",
    "    \"\"\"Evaluate model and return predictions and targets (denormalized).\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            pred = model(X_batch)\n",
    "            predictions.extend(pred.cpu().numpy())\n",
    "            targets.extend(y_batch.numpy())\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    predictions = np.array(predictions)\n",
    "    targets = np.array(targets)\n",
    "    \n",
    "    # Denormalize\n",
    "    predictions_denorm = scaler_y.inverse_transform(predictions.reshape(-1, 1)).flatten()\n",
    "    targets_denorm = scaler_y.inverse_transform(targets.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    return predictions_denorm, targets_denorm\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_test, y_true_test = evaluate_model(model, test_loader, scaler_y, device)\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(y_true_test, y_pred_test)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_true_test, y_pred_test)\n",
    "mape = np.mean(np.abs((y_true_test - y_pred_test) / y_true_test)) * 100\n",
    "\n",
    "# Directional accuracy (percentage of correct direction predictions)\n",
    "returns_true = np.diff(y_true_test) / y_true_test[:-1]\n",
    "returns_pred = np.diff(y_pred_test) / y_pred_test[:-1]\n",
    "direction_accuracy = np.mean((returns_true * returns_pred) > 0) * 100\n",
    "\n",
    "print(\"Test Set Performance:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"RMSE: ${rmse:.2f}\")\n",
    "print(f\"MAE: ${mae:.2f}\")\n",
    "print(f\"MAPE: {mape:.2f}%\")\n",
    "print(f\"Directional Accuracy: {direction_accuracy:.2f}%\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visualization of Predictions\n",
    "\n",
    "Visualize predictions vs actual prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time index for test set (approximate, based on sequence length)\n",
    "test_start_idx = len(X_train) + len(X_val) + SEQUENCE_LENGTH\n",
    "test_indices = df_features.index[test_start_idx:test_start_idx + len(y_pred_test)]\n",
    "\n",
    "# Plot predictions vs actual\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Full test set\n",
    "axes[0].plot(test_indices, y_true_test, label='Actual', alpha=0.7, linewidth=1.5)\n",
    "axes[0].plot(test_indices, y_pred_test, label='Predicted', alpha=0.7, linewidth=1.5)\n",
    "axes[0].set_xlabel('Date')\n",
    "axes[0].set_ylabel('Price ($)')\n",
    "axes[0].set_title(f'{SYMBOL} - Actual vs Predicted Prices (Test Set)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Residuals\n",
    "residuals = y_true_test - y_pred_test\n",
    "axes[1].plot(test_indices, residuals, alpha=0.7, color='red', linewidth=1)\n",
    "axes[1].axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].set_ylabel('Residual ($)')\n",
    "axes[1].set_title('Residuals (Actual - Predicted)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Distribution of residuals\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(residuals, bins=50, alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Residual ($)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Residuals')\n",
    "plt.axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(y_pred_test, residuals, alpha=0.5)\n",
    "plt.xlabel('Predicted Price ($)')\n",
    "plt.ylabel('Residual ($)')\n",
    "plt.title('Residuals vs Predicted Values')\n",
    "plt.axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Model\n",
    "\n",
    "Save the trained model and scalers for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory if it doesn't exist\n",
    "models_dir = project_root / \"models\"\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "model_path = models_dir / f\"lstm_{SYMBOL.lower()}_model.pth\"\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_config': {\n",
    "        'input_size': INPUT_SIZE,\n",
    "        'hidden_size': HIDDEN_SIZE,\n",
    "        'num_layers': NUM_LAYERS,\n",
    "        'dropout': DROPOUT,\n",
    "    },\n",
    "    'scaler_X': scaler_X,\n",
    "    'scaler_y': scaler_y,\n",
    "    'sequence_length': SEQUENCE_LENGTH,\n",
    "    'feature_columns': available_features,\n",
    "    'target_column': TARGET_COLUMN,\n",
    "    'symbol': SYMBOL,\n",
    "    'training_metrics': {\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'final_train_loss': train_losses[-1],\n",
    "        'final_val_loss': val_losses[-1],\n",
    "        'test_rmse': rmse,\n",
    "        'test_mae': mae,\n",
    "        'test_mape': mape,\n",
    "        'direction_accuracy': direction_accuracy,\n",
    "    }\n",
    "}, model_path)\n",
    "\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "print(f\"Model size: {model_path.stat().st_size / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Model Loading Example\n",
    "\n",
    "Example of how to load the saved model for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load model for inference\n",
    "def load_model_for_inference(model_path: Path):\n",
    "    \"\"\"Load a saved model for inference.\"\"\"\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    \n",
    "    # Recreate model\n",
    "    model_config = checkpoint['model_config']\n",
    "    model = LSTMPredictor(**model_config)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    return model, checkpoint\n",
    "\n",
    "# Load model (example)\n",
    "# loaded_model, checkpoint = load_model_for_inference(model_path)\n",
    "# print(\"Model loaded successfully!\")\n",
    "# print(f\"Model was trained on: {checkpoint['symbol']}\")\n",
    "# print(f\"Test RMSE: ${checkpoint['training_metrics']['test_rmse']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes and Next Steps\n",
    "\n",
    "### Improvements to Consider:\n",
    "1. **Hyperparameter Optimization**: Use Optuna or grid search to find optimal hyperparameters\n",
    "2. **Feature Selection**: Identify most important features using SHAP or feature importance\n",
    "3. **Ensemble Methods**: Combine multiple models for better predictions\n",
    "4. **Multi-step Forecasting**: Predict multiple steps ahead\n",
    "5. **Regime Detection**: Adapt model based on market conditions\n",
    "6. **Attention Mechanisms**: Add attention layers to focus on important time steps\n",
    "7. **Transformer Models**: Experiment with Transformer architectures\n",
    "8. **Walk-Forward Validation**: Implement proper walk-forward analysis for backtesting\n",
    "\n",
    "### Financial Metrics to Add:\n",
    "- Sharpe Ratio\n",
    "- Sortino Ratio\n",
    "- Maximum Drawdown\n",
    "- Win Rate\n",
    "- Profit Factor\n",
    "\n",
    "### Model Deployment:\n",
    "- Create a prediction service/API\n",
    "- Set up model versioning with MLflow\n",
    "- Implement real-time inference pipeline\n",
    "- Add model monitoring and retraining schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
