{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LSTM Model for Stock Price Prediction\n",
        "\n",
        "This notebook trains an LSTM (Long Short-Term Memory) neural network to predict stock prices using historical OHLCV data from the trading system database.\n",
        "\n",
        "## Objectives\n",
        "- Load historical market data from PostgreSQL\n",
        "- Engineer features (technical indicators, returns, volatility)\n",
        "- Create time-aware train/validation/test splits\n",
        "- Train an LSTM model using PyTorch\n",
        "- Evaluate model performance with financial metrics\n",
        "- Visualize predictions and residuals\n",
        "\n",
        "## Requirements\n",
        "- PyTorch (install with: `pip install torch`)\n",
        "- All other dependencies from `requirements.txt`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Setup and Imports\n",
        "\"\"\"\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timedelta, timezone\n",
        "from typing import Tuple, Optional\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add project root to path\n",
        "project_root = Path().resolve().parent\n",
        "if str(project_root) not in sys.path:\n",
        "    sys.path.insert(0, str(project_root))\n",
        "\n",
        "# Core libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Database\n",
        "from sqlalchemy import select, desc\n",
        "from src.shared.database.base import db_readonly_session\n",
        "from src.shared.database.models.market_data import MarketData\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (14, 6)\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(RANDOM_SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Loading\n",
        "\n",
        "Load historical market data from the database for a specific symbol."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_market_data(\n",
        "    symbol: str,\n",
        "    start_date: Optional[datetime] = None,\n",
        "    end_date: Optional[datetime] = None,\n",
        "    data_source: str = \"yahoo\",\n",
        "    min_records: int = 1000\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Load market data from database for a specific symbol.\n",
        "    \n",
        "    Args:\n",
        "        symbol: Stock symbol (e.g., 'AAPL')\n",
        "        start_date: Start date (default: 1 year ago)\n",
        "        end_date: End date (default: today)\n",
        "        data_source: Data source filter ('yahoo', 'polygon', 'alpaca')\n",
        "        min_records: Minimum number of records required\n",
        "        \n",
        "    Returns:\n",
        "        DataFrame with OHLCV data indexed by timestamp (UTC)\n",
        "    \"\"\"\n",
        "    if start_date is None:\n",
        "        start_date = datetime.now(timezone.utc) - timedelta(days=365)\n",
        "    if end_date is None:\n",
        "        end_date = datetime.now(timezone.utc)\n",
        "    \n",
        "    with db_readonly_session() as session:\n",
        "        query = (\n",
        "            select(MarketData)\n",
        "            .where(MarketData.symbol == symbol.upper())\n",
        "            .where(MarketData.data_source == data_source.lower())\n",
        "            .where(MarketData.timestamp >= start_date)\n",
        "            .where(MarketData.timestamp <= end_date)\n",
        "            .order_by(MarketData.timestamp)\n",
        "        )\n",
        "        \n",
        "        result = session.execute(query)\n",
        "        records = result.scalars().all()\n",
        "    \n",
        "    if len(records) < min_records:\n",
        "        raise ValueError(\n",
        "            f\"Insufficient data: {len(records)} records found, \"\n",
        "            f\"minimum {min_records} required\"\n",
        "        )\n",
        "    \n",
        "    # Convert to DataFrame\n",
        "    data = []\n",
        "    for record in records:\n",
        "        if record.is_complete:  # Only include complete OHLCV records\n",
        "            data.append({\n",
        "                'timestamp': record.timestamp,\n",
        "                'open': float(record.open),\n",
        "                'high': float(record.high),\n",
        "                'low': float(record.low),\n",
        "                'close': float(record.close),\n",
        "                'volume': int(record.volume) if record.volume else 0,\n",
        "            })\n",
        "    \n",
        "    df = pd.DataFrame(data)\n",
        "    df.set_index('timestamp', inplace=True)\n",
        "    df.sort_index(inplace=True)\n",
        "    \n",
        "    # Remove duplicates (keep last)\n",
        "    df = df[~df.index.duplicated(keep='last')]\n",
        "    \n",
        "    print(f\"Loaded {len(df)} records for {symbol}\")\n",
        "    print(f\"Date range: {df.index.min()} to {df.index.max()}\")\n",
        "    print(f\"Data completeness: {df.isnull().sum().sum()} missing values\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Load data for a symbol (change as needed)\n",
        "SYMBOL = \"AAPL\"\n",
        "df_raw = load_market_data(SYMBOL, data_source=\"yahoo\")\n",
        "df_raw.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Feature Engineering\n",
        "\n",
        "Create technical indicators and derived features for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def engineer_features(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Engineer features from OHLCV data.\n",
        "    \n",
        "    Features include:\n",
        "    - Returns (log and simple)\n",
        "    - Volatility (rolling standard deviation)\n",
        "    - Technical indicators (RSI, MACD, moving averages)\n",
        "    - Price ratios (high/low, close/open)\n",
        "    - Volume features\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame with OHLCV data\n",
        "        \n",
        "    Returns:\n",
        "        DataFrame with additional features\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    \n",
        "    # Returns\n",
        "    df['returns'] = df['close'].pct_change()\n",
        "    df['log_returns'] = np.log(df['close'] / df['close'].shift(1))\n",
        "    \n",
        "    # Volatility (rolling 20-period)\n",
        "    df['volatility'] = df['returns'].rolling(window=20).std()\n",
        "    \n",
        "    # Price ratios\n",
        "    df['high_low_ratio'] = df['high'] / df['low']\n",
        "    df['close_open_ratio'] = df['close'] / df['open']\n",
        "    \n",
        "    # Moving averages\n",
        "    df['sma_5'] = df['close'].rolling(window=5).mean()\n",
        "    df['sma_20'] = df['close'].rolling(window=20).mean()\n",
        "    df['sma_50'] = df['close'].rolling(window=50).mean()\n",
        "    \n",
        "    # RSI (Relative Strength Index)\n",
        "    delta = df['close'].diff()\n",
        "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
        "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
        "    rs = gain / loss\n",
        "    df['rsi'] = 100 - (100 / (1 + rs))\n",
        "    \n",
        "    # MACD\n",
        "    exp1 = df['close'].ewm(span=12, adjust=False).mean()\n",
        "    exp2 = df['close'].ewm(span=26, adjust=False).mean()\n",
        "    df['macd'] = exp1 - exp2\n",
        "    df['macd_signal'] = df['macd'].ewm(span=9, adjust=False).mean()\n",
        "    df['macd_hist'] = df['macd'] - df['macd_signal']\n",
        "    \n",
        "    # Volume features\n",
        "    df['volume_ma'] = df['volume'].rolling(window=20).mean()\n",
        "    df['volume_ratio'] = df['volume'] / df['volume_ma']\n",
        "    \n",
        "    # Price position (where close is relative to high-low range)\n",
        "    df['price_position'] = (df['close'] - df['low']) / (df['high'] - df['low'])\n",
        "    \n",
        "    # Drop rows with NaN (from rolling calculations)\n",
        "    df = df.dropna()\n",
        "    \n",
        "    print(f\"Feature engineering complete. Shape: {df.shape}\")\n",
        "    print(f\"Features: {list(df.columns)}\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "df_features = engineer_features(df_raw)\n",
        "df_features.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Preprocessing\n",
        "\n",
        "Prepare data for LSTM: select features, normalize, and create sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "SEQUENCE_LENGTH = 60  # Number of time steps to look back\n",
        "PREDICTION_HORIZON = 1  # Predict next 1 period\n",
        "TARGET_COLUMN = 'close'  # What we're predicting\n",
        "\n",
        "# Select features (exclude target and timestamp-related)\n",
        "feature_columns = [\n",
        "    'open', 'high', 'low', 'close', 'volume',\n",
        "    'returns', 'log_returns', 'volatility',\n",
        "    'high_low_ratio', 'close_open_ratio',\n",
        "    'sma_5', 'sma_20', 'sma_50',\n",
        "    'rsi', 'macd', 'macd_signal', 'macd_hist',\n",
        "    'volume_ma', 'volume_ratio', 'price_position'\n",
        "]\n",
        "\n",
        "# Ensure all columns exist\n",
        "available_features = [col for col in feature_columns if col in df_features.columns]\n",
        "print(f\"Using {len(available_features)} features: {available_features}\")\n",
        "\n",
        "# Extract feature matrix and target\n",
        "X = df_features[available_features].values\n",
        "y = df_features[TARGET_COLUMN].values\n",
        "\n",
        "print(f\"Feature matrix shape: {X.shape}\")\n",
        "print(f\"Target shape: {y.shape}\")\n",
        "\n",
        "# Normalize features (fit on training data only - will be done in split)\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = MinMaxScaler()\n",
        "\n",
        "# For now, fit on all data (will refit on train only later)\n",
        "X_scaled = scaler_X.fit_transform(X)\n",
        "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n",
        "\n",
        "print(f\"Normalized feature range: [{X_scaled.min():.2f}, {X_scaled.max():.2f}]\")\n",
        "print(f\"Normalized target range: [{y_scaled.min():.2f}, {y_scaled.max():.2f}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_sequences(\n",
        "    X: np.ndarray,\n",
        "    y: np.ndarray,\n",
        "    seq_length: int,\n",
        "    prediction_horizon: int = 1\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Create sequences for LSTM training.\n",
        "    \n",
        "    Args:\n",
        "        X: Feature matrix (n_samples, n_features)\n",
        "        y: Target vector (n_samples,)\n",
        "        seq_length: Length of input sequences\n",
        "        prediction_horizon: Steps ahead to predict\n",
        "        \n",
        "    Returns:\n",
        "        X_seq: Sequences (n_samples - seq_length, seq_length, n_features)\n",
        "        y_seq: Targets (n_samples - seq_length,)\n",
        "    \"\"\"\n",
        "    X_seq, y_seq = [], []\n",
        "    \n",
        "    for i in range(len(X) - seq_length - prediction_horizon + 1):\n",
        "        X_seq.append(X[i:i + seq_length])\n",
        "        y_seq.append(y[i + seq_length + prediction_horizon - 1])\n",
        "    \n",
        "    return np.array(X_seq), np.array(y_seq)\n",
        "\n",
        "# Create sequences\n",
        "X_seq, y_seq = create_sequences(X_scaled, y_scaled, SEQUENCE_LENGTH, PREDICTION_HORIZON)\n",
        "\n",
        "print(f\"Sequence shape: {X_seq.shape}\")\n",
        "print(f\"Target shape: {y_seq.shape}\")\n",
        "print(f\"Total sequences: {len(X_seq)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Time-Aware Data Splitting\n",
        "\n",
        "Split data chronologically to avoid look-ahead bias (critical for financial data)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def time_aware_split(\n",
        "    X: np.ndarray,\n",
        "    y: np.ndarray,\n",
        "    train_ratio: float = 0.7,\n",
        "    val_ratio: float = 0.15\n",
        ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Split data chronologically (no shuffling for time series).\n",
        "    \n",
        "    Args:\n",
        "        X: Feature sequences\n",
        "        y: Target values\n",
        "        train_ratio: Proportion for training\n",
        "        val_ratio: Proportion for validation\n",
        "        \n",
        "    Returns:\n",
        "        X_train, X_val, X_test, y_train, y_val, y_test\n",
        "    \"\"\"\n",
        "    n_samples = len(X)\n",
        "    train_end = int(n_samples * train_ratio)\n",
        "    val_end = int(n_samples * (train_ratio + val_ratio))\n",
        "    \n",
        "    X_train = X[:train_end]\n",
        "    X_val = X[train_end:val_end]\n",
        "    X_test = X[val_end:]\n",
        "    \n",
        "    y_train = y[:train_end]\n",
        "    y_val = y[train_end:val_end]\n",
        "    y_test = y[val_end:]\n",
        "    \n",
        "    print(f\"Train: {len(X_train)} samples ({len(X_train)/n_samples*100:.1f}%)\")\n",
        "    print(f\"Validation: {len(X_val)} samples ({len(X_val)/n_samples*100:.1f}%)\")\n",
        "    print(f\"Test: {len(X_test)} samples ({len(X_test)/n_samples*100:.1f}%)\")\n",
        "    \n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
        "\n",
        "# Split data\n",
        "X_train, X_val, X_test, y_train, y_val, y_test = time_aware_split(\n",
        "    X_seq, y_seq, train_ratio=0.7, val_ratio=0.15\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. PyTorch Dataset and DataLoader\n",
        "\n",
        "Create PyTorch datasets for efficient batching."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TimeSeriesDataset(Dataset):\n",
        "    \"\"\"PyTorch Dataset for time series sequences.\"\"\"\n",
        "    \n",
        "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            X: Feature sequences (n_samples, seq_length, n_features)\n",
        "            y: Target values (n_samples,)\n",
        "        \"\"\"\n",
        "        self.X = torch.FloatTensor(X)\n",
        "        self.y = torch.FloatTensor(y)\n",
        "    \n",
        "    def __len__(self) -> int:\n",
        "        return len(self.X)\n",
        "    \n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = TimeSeriesDataset(X_train, y_train)\n",
        "val_dataset = TimeSeriesDataset(X_val, y_val)\n",
        "test_dataset = TimeSeriesDataset(X_test, y_test)\n",
        "\n",
        "# Create data loaders\n",
        "BATCH_SIZE = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(f\"Train batches: {len(train_loader)}\")\n",
        "print(f\"Validation batches: {len(val_loader)}\")\n",
        "print(f\"Test batches: {len(test_loader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. LSTM Model Architecture\n",
        "\n",
        "Define the LSTM model with dropout for regularization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LSTMPredictor(nn.Module):\n",
        "    \"\"\"\n",
        "    LSTM model for stock price prediction.\n",
        "    \n",
        "    Architecture:\n",
        "    - LSTM layers for sequence learning\n",
        "    - Dropout for regularization\n",
        "    - Fully connected layers for output\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size: int,\n",
        "        hidden_size: int = 64,\n",
        "        num_layers: int = 2,\n",
        "        dropout: float = 0.2,\n",
        "        output_size: int = 1\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_size: Number of features per time step\n",
        "            hidden_size: Number of LSTM hidden units\n",
        "            num_layers: Number of LSTM layers\n",
        "            dropout: Dropout probability\n",
        "            output_size: Size of output (typically 1 for price prediction)\n",
        "        \"\"\"\n",
        "        super(LSTMPredictor, self).__init__()\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        # LSTM layers\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            batch_first=True\n",
        "        )\n",
        "        \n",
        "        # Additional dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        # Fully connected output layer\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "        \n",
        "        Args:\n",
        "            x: Input tensor (batch_size, seq_length, input_size)\n",
        "            \n",
        "        Returns:\n",
        "            Output tensor (batch_size, output_size)\n",
        "        \"\"\"\n",
        "        # LSTM forward pass\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        \n",
        "        # Take the last output from the sequence\n",
        "        lstm_out = lstm_out[:, -1, :]\n",
        "        \n",
        "        # Apply dropout\n",
        "        lstm_out = self.dropout(lstm_out)\n",
        "        \n",
        "        # Fully connected layer\n",
        "        output = self.fc(lstm_out)\n",
        "        \n",
        "        return output.squeeze(-1)  # Remove last dimension if output_size=1\n",
        "\n",
        "# Initialize model\n",
        "INPUT_SIZE = X_train.shape[2]  # Number of features\n",
        "HIDDEN_SIZE = 64\n",
        "NUM_LAYERS = 2\n",
        "DROPOUT = 0.2\n",
        "\n",
        "model = LSTMPredictor(\n",
        "    input_size=INPUT_SIZE,\n",
        "    hidden_size=HIDDEN_SIZE,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    dropout=DROPOUT\n",
        ")\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Model architecture:\")\n",
        "print(f\"  Input size: {INPUT_SIZE}\")\n",
        "print(f\"  Hidden size: {HIDDEN_SIZE}\")\n",
        "print(f\"  LSTM layers: {NUM_LAYERS}\")\n",
        "print(f\"  Total parameters: {total_params:,}\")\n",
        "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "# Move to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Training Setup\n",
        "\n",
        "Define loss function, optimizer, and learning rate scheduler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loss function (MSE for regression)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Optimizer (Adam with weight decay for regularization)\n",
        "LEARNING_RATE = 0.001\n",
        "WEIGHT_DECAY = 1e-5\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "# Learning rate scheduler (reduce on plateau)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
        ")\n",
        "\n",
        "# Training parameters\n",
        "NUM_EPOCHS = 50\n",
        "EARLY_STOPPING_PATIENCE = 10\n",
        "MIN_DELTA = 1e-6\n",
        "\n",
        "print(f\"Training configuration:\")\n",
        "print(f\"  Loss function: MSE\")\n",
        "print(f\"  Optimizer: Adam (lr={LEARNING_RATE}, weight_decay={WEIGHT_DECAY})\")\n",
        "print(f\"  Scheduler: ReduceLROnPlateau\")\n",
        "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
        "print(f\"  Early stopping patience: {EARLY_STOPPING_PATIENCE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Training Loop\n",
        "\n",
        "Train the model with early stopping and validation monitoring."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    \"\"\"Train for one epoch.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    n_batches = 0\n",
        "    \n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(X_batch)\n",
        "        loss = criterion(predictions, y_batch)\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        n_batches += 1\n",
        "    \n",
        "    return total_loss / n_batches\n",
        "\n",
        "def validate(model, val_loader, criterion, device):\n",
        "    \"\"\"Validate the model.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    n_batches = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            predictions = model(X_batch)\n",
        "            loss = criterion(predictions, y_batch)\n",
        "            total_loss += loss.item()\n",
        "            n_batches += 1\n",
        "    \n",
        "    return total_loss / n_batches\n",
        "\n",
        "# Training loop\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "\n",
        "print(\"Starting training...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    # Train\n",
        "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    train_losses.append(train_loss)\n",
        "    \n",
        "    # Validate\n",
        "    val_loss = validate(model, val_loader, criterion, device)\n",
        "    val_losses.append(val_loss)\n",
        "    \n",
        "    # Learning rate scheduling\n",
        "    scheduler.step(val_loss)\n",
        "    \n",
        "    # Early stopping check\n",
        "    if val_loss < best_val_loss - MIN_DELTA:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "        # Save best model (in practice, save to disk)\n",
        "        best_model_state = model.state_dict().copy()\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "    \n",
        "    # Print progress\n",
        "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
        "        print(\n",
        "            f\"Epoch [{epoch+1}/{NUM_EPOCHS}] | \"\n",
        "            f\"Train Loss: {train_loss:.6f} | \"\n",
        "            f\"Val Loss: {val_loss:.6f} | \"\n",
        "            f\"LR: {optimizer.param_groups[0]['lr']:.6f}\"\n",
        "        )\n",
        "    \n",
        "    # Early stopping\n",
        "    if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
        "        print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
        "        model.load_state_dict(best_model_state)\n",
        "        break\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Training completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Training Visualization\n",
        "\n",
        "Plot training and validation loss curves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training curves\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Train Loss', alpha=0.7)\n",
        "plt.plot(val_losses, label='Validation Loss', alpha=0.7)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss (MSE)')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_losses, label='Train Loss', alpha=0.7)\n",
        "plt.plot(val_losses, label='Validation Loss', alpha=0.7)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss (MSE) - Log Scale')\n",
        "plt.title('Training and Validation Loss (Log Scale)')\n",
        "plt.yscale('log')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Final train loss: {train_losses[-1]:.6f}\")\n",
        "print(f\"Final validation loss: {val_losses[-1]:.6f}\")\n",
        "print(f\"Best validation loss: {best_val_loss:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Model Evaluation\n",
        "\n",
        "Evaluate on test set and calculate financial metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(model, test_loader, scaler_y, device):\n",
        "    \"\"\"Evaluate model and return predictions and targets (denormalized).\"\"\"\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    targets = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in test_loader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            pred = model(X_batch)\n",
        "            predictions.extend(pred.cpu().numpy())\n",
        "            targets.extend(y_batch.numpy())\n",
        "    \n",
        "    # Convert to numpy arrays\n",
        "    predictions = np.array(predictions)\n",
        "    targets = np.array(targets)\n",
        "    \n",
        "    # Denormalize\n",
        "    predictions_denorm = scaler_y.inverse_transform(predictions.reshape(-1, 1)).flatten()\n",
        "    targets_denorm = scaler_y.inverse_transform(targets.reshape(-1, 1)).flatten()\n",
        "    \n",
        "    return predictions_denorm, targets_denorm\n",
        "\n",
        "# Evaluate on test set\n",
        "y_pred_test, y_true_test = evaluate_model(model, test_loader, scaler_y, device)\n",
        "\n",
        "# Calculate metrics\n",
        "mse = mean_squared_error(y_true_test, y_pred_test)\n",
        "rmse = np.sqrt(mse)\n",
        "mae = mean_absolute_error(y_true_test, y_pred_test)\n",
        "mape = np.mean(np.abs((y_true_test - y_pred_test) / y_true_test)) * 100\n",
        "\n",
        "# Directional accuracy (percentage of correct direction predictions)\n",
        "returns_true = np.diff(y_true_test) / y_true_test[:-1]\n",
        "returns_pred = np.diff(y_pred_test) / y_pred_test[:-1]\n",
        "direction_accuracy = np.mean((returns_true * returns_pred) > 0) * 100\n",
        "\n",
        "print(\"Test Set Performance:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"RMSE: ${rmse:.2f}\")\n",
        "print(f\"MAE: ${mae:.2f}\")\n",
        "print(f\"MAPE: {mape:.2f}%\")\n",
        "print(f\"Directional Accuracy: {direction_accuracy:.2f}%\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Visualization of Predictions\n",
        "\n",
        "Visualize predictions vs actual prices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create time index for test set (approximate, based on sequence length)\n",
        "test_start_idx = len(X_train) + len(X_val) + SEQUENCE_LENGTH\n",
        "test_indices = df_features.index[test_start_idx:test_start_idx + len(y_pred_test)]\n",
        "\n",
        "# Plot predictions vs actual\n",
        "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
        "\n",
        "# Full test set\n",
        "axes[0].plot(test_indices, y_true_test, label='Actual', alpha=0.7, linewidth=1.5)\n",
        "axes[0].plot(test_indices, y_pred_test, label='Predicted', alpha=0.7, linewidth=1.5)\n",
        "axes[0].set_xlabel('Date')\n",
        "axes[0].set_ylabel('Price ($)')\n",
        "axes[0].set_title(f'{SYMBOL} - Actual vs Predicted Prices (Test Set)')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "axes[0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Residuals\n",
        "residuals = y_true_test - y_pred_test\n",
        "axes[1].plot(test_indices, residuals, alpha=0.7, color='red', linewidth=1)\n",
        "axes[1].axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
        "axes[1].set_xlabel('Date')\n",
        "axes[1].set_ylabel('Residual ($)')\n",
        "axes[1].set_title('Residuals (Actual - Predicted)')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "axes[1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Distribution of residuals\n",
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(residuals, bins=50, alpha=0.7, edgecolor='black')\n",
        "plt.xlabel('Residual ($)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Residuals')\n",
        "plt.axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(y_pred_test, residuals, alpha=0.5)\n",
        "plt.xlabel('Predicted Price ($)')\n",
        "plt.ylabel('Residual ($)')\n",
        "plt.title('Residuals vs Predicted Values')\n",
        "plt.axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Save Model\n",
        "\n",
        "Save the trained model and scalers for future use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create models directory if it doesn't exist\n",
        "models_dir = project_root / \"models\"\n",
        "models_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Save model\n",
        "model_path = models_dir / f\"lstm_{SYMBOL.lower()}_model.pth\"\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'model_config': {\n",
        "        'input_size': INPUT_SIZE,\n",
        "        'hidden_size': HIDDEN_SIZE,\n",
        "        'num_layers': NUM_LAYERS,\n",
        "        'dropout': DROPOUT,\n",
        "    },\n",
        "    'scaler_X': scaler_X,\n",
        "    'scaler_y': scaler_y,\n",
        "    'sequence_length': SEQUENCE_LENGTH,\n",
        "    'feature_columns': available_features,\n",
        "    'target_column': TARGET_COLUMN,\n",
        "    'symbol': SYMBOL,\n",
        "    'training_metrics': {\n",
        "        'best_val_loss': best_val_loss,\n",
        "        'final_train_loss': train_losses[-1],\n",
        "        'final_val_loss': val_losses[-1],\n",
        "        'test_rmse': rmse,\n",
        "        'test_mae': mae,\n",
        "        'test_mape': mape,\n",
        "        'direction_accuracy': direction_accuracy,\n",
        "    }\n",
        "}, model_path)\n",
        "\n",
        "print(f\"Model saved to: {model_path}\")\n",
        "print(f\"Model size: {model_path.stat().st_size / 1024 / 1024:.2f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Model Loading Example\n",
        "\n",
        "Example of how to load the saved model for inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Load model for inference\n",
        "def load_model_for_inference(model_path: Path):\n",
        "    \"\"\"Load a saved model for inference.\"\"\"\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    \n",
        "    # Recreate model\n",
        "    model_config = checkpoint['model_config']\n",
        "    model = LSTMPredictor(**model_config)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    \n",
        "    return model, checkpoint\n",
        "\n",
        "# Load model (example)\n",
        "# loaded_model, checkpoint = load_model_for_inference(model_path)\n",
        "# print(\"Model loaded successfully!\")\n",
        "# print(f\"Model was trained on: {checkpoint['symbol']}\")\n",
        "# print(f\"Test RMSE: ${checkpoint['training_metrics']['test_rmse']:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notes and Next Steps\n",
        "\n",
        "### Improvements to Consider:\n",
        "1. **Hyperparameter Optimization**: Use Optuna or grid search to find optimal hyperparameters\n",
        "2. **Feature Selection**: Identify most important features using SHAP or feature importance\n",
        "3. **Ensemble Methods**: Combine multiple models for better predictions\n",
        "4. **Multi-step Forecasting**: Predict multiple steps ahead\n",
        "5. **Regime Detection**: Adapt model based on market conditions\n",
        "6. **Attention Mechanisms**: Add attention layers to focus on important time steps\n",
        "7. **Transformer Models**: Experiment with Transformer architectures\n",
        "8. **Walk-Forward Validation**: Implement proper walk-forward analysis for backtesting\n",
        "\n",
        "### Financial Metrics to Add:\n",
        "- Sharpe Ratio\n",
        "- Sortino Ratio\n",
        "- Maximum Drawdown\n",
        "- Win Rate\n",
        "- Profit Factor\n",
        "\n",
        "### Model Deployment:\n",
        "- Create a prediction service/API\n",
        "- Set up model versioning with MLflow\n",
        "- Implement real-time inference pipeline\n",
        "- Add model monitoring and retraining schedule"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
