{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bd1bec3e-b191-4e9f-9f10-2cd04e673c41",
      "metadata": {},
      "source": [
        "# LSTM Price Prediction (v3)\n",
        "\n",
        "Import packages and load market data; technical indicators are computed in this notebook (not from DB)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "0aeb9cad-39a9-4e22-9bc6-fdd62a6b61ee",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-01-31T17:45:08.707211400Z",
          "start_time": "2026-01-31T17:44:13.046217600Z"
        },
        "jupyter": {
          "is_executing": true
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded environment variables from: D:\\PythonProjects\\Trading-System\\.env\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Setup and Imports\n",
        "\"\"\"\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timedelta, timezone\n",
        "from typing import Tuple, Optional\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add project root to path\n",
        "project_root = Path().resolve().parent\n",
        "if str(project_root) not in sys.path:\n",
        "    sys.path.insert(0, str(project_root))\n",
        "\n",
        "# Load environment variables from .env file\n",
        "from dotenv import load_dotenv\n",
        "env_path = project_root / \".env\"\n",
        "if env_path.exists():\n",
        "    load_dotenv(env_path)\n",
        "    print(f\"Loaded environment variables from: {env_path}\")\n",
        "else:\n",
        "    print(f\"Warning: .env file not found at {env_path}\")\n",
        "    print(\"Please ensure your database credentials are set in environment variables or .env file\")\n",
        "\n",
        "# Core libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# PyTorch Forecasting (encoder-only scaling, no look-ahead bias)\n",
        "from pytorch_forecasting import TimeSeriesDataSet\n",
        "from pytorch_forecasting.data.encoders import EncoderNormalizer\n",
        "\n",
        "# optuna\n",
        "import optuna\n",
        "\n",
        "# Database\n",
        "from sqlalchemy import select, desc\n",
        "from src.shared.database.base import db_readonly_session\n",
        "from src.shared.database.models.market_data import MarketData\n",
        "\n",
        "# Yahoo Finance (for direct OHLCV fetch in notebook)\n",
        "import yfinance as yf\n",
        "\n",
        "# statsmodel\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.stats.diagnostic import acorr_ljungbox"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "05fe2178-c05f-4bdd-908f-ad6336fe4fe7",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-02-06T17:20:25.267117100Z",
          "start_time": "2026-02-06T17:20:24.589401200Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.7.0+cu128\n",
            "CUDA available: True\n",
            "CUDA device: NVIDIA GeForce RTX 3050 Laptop GPU\n"
          ]
        }
      ],
      "source": [
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (14, 6)\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(RANDOM_SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2be54f2-5b89-4563-995e-a03bf78fca82",
      "metadata": {},
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f12dcfa7-f44f-4430-b525-88e90b44c6b7",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-01-31T17:45:08.894827700Z",
          "start_time": "2026-01-31T17:45:08.874618100Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inverse transformation functions defined:\n",
            "  - inverse_transform_pct_return(): Convert % returns → prices\n",
            "  - inverse_transform_log_return(): Convert log returns → prices\n"
          ]
        }
      ],
      "source": [
        "# Helper functions for inverse transformation (convert predictions back to prices)\n",
        "\n",
        "def inverse_transform_pct_return(predictions: np.ndarray, base_prices: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Convert percentage return predictions back to price predictions.\n",
        "    \n",
        "    Args:\n",
        "        predictions: Predicted percentage returns\n",
        "        base_prices: Base prices (typically previous period's close prices)\n",
        "        \n",
        "    Returns:\n",
        "        Predicted prices\n",
        "    \"\"\"\n",
        "    return base_prices * (1 + predictions)\n",
        "\n",
        "\n",
        "def inverse_transform_log_return(predictions: np.ndarray, base_prices: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Convert log return predictions back to price predictions.\n",
        "    \n",
        "    Args:\n",
        "        predictions: Predicted log returns\n",
        "        base_prices: Base prices (typically previous period's close prices)\n",
        "        \n",
        "    Returns:\n",
        "        Predicted prices\n",
        "    \"\"\"\n",
        "    return base_prices * np.exp(predictions)\n",
        "\n",
        "print(\"Inverse transformation functions defined:\")\n",
        "print(\"  - inverse_transform_pct_return(): Convert % returns → prices\")\n",
        "print(\"  - inverse_transform_log_return(): Convert log returns → prices\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18a1c82e-3462-4e6c-9b3a-aef4a363eec4",
      "metadata": {},
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6c4b26a-1d53-482b-b16d-22d5fb2bde65",
      "metadata": {},
      "source": [
        "## Load OHLC Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "7ea1649f-6d2f-424a-9a8c-cb0d05f15142",
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_market_data(\n",
        "    symbol: str,\n",
        "    start_date: Optional[datetime] = None,\n",
        "    end_date: Optional[datetime] = None,\n",
        "    data_source: str = \"yahoo_adjusted\",\n",
        "    min_records: int = 1000\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Load OHLCV market data from database for a specific symbol.\n",
        "    Default data_source='yahoo_adjusted' (splits/dividends adjusted). Use 'yahoo' for raw.\n",
        "    Technical indicators are computed in this notebook, not loaded from DB.\n",
        "    \n",
        "    Args:\n",
        "        symbol: Stock symbol (e.g., 'AAPL')\n",
        "        start_date: Start date (default: 1 year ago)\n",
        "        end_date: End date (default: today)\n",
        "        data_source: Data source ('yahoo_adjusted', 'yahoo', 'polygon', 'alpaca')\n",
        "        min_records: Minimum number of records required\n",
        "        \n",
        "    Returns:\n",
        "        DataFrame with OHLCV columns (timestamp, open, high, low, close, volume)\n",
        "    \"\"\"\n",
        "    if start_date is None:\n",
        "        start_date = datetime.now(timezone.utc) - timedelta(days=500)\n",
        "    if end_date is None:\n",
        "        end_date = datetime.now(timezone.utc)\n",
        "    \n",
        "    symbol_upper = symbol.upper()\n",
        "    \n",
        "    # Load market data\n",
        "    with db_readonly_session() as session:\n",
        "        # Query market data\n",
        "        market_query = (\n",
        "            select(MarketData)\n",
        "            .where(MarketData.symbol == symbol_upper)\n",
        "            .where(MarketData.data_source == data_source.lower())\n",
        "            .where(MarketData.timestamp >= start_date)\n",
        "            .where(MarketData.timestamp <= end_date)\n",
        "            .order_by(MarketData.timestamp)\n",
        "        )\n",
        "        \n",
        "        market_result = session.execute(market_query)\n",
        "        market_records = market_result.scalars().all()\n",
        "    \n",
        "    if len(market_records) < min_records:\n",
        "        raise ValueError(\n",
        "            f\"Insufficient data: {len(market_records)} records found, \"\n",
        "            f\"minimum {min_records} required\"\n",
        "        )\n",
        "    \n",
        "    # Convert market data to DataFrame\n",
        "    market_data = []\n",
        "    for record in market_records:\n",
        "        if record.is_complete:  # Only include complete OHLCV records\n",
        "            market_data.append({\n",
        "                'timestamp': record.timestamp,\n",
        "                'open': float(record.open),\n",
        "                'high': float(record.high),\n",
        "                'low': float(record.low),\n",
        "                'close': float(record.close),\n",
        "                'volume': int(record.volume) if record.volume else 0,\n",
        "            })\n",
        "    \n",
        "    df_market = pd.DataFrame(market_data)\n",
        "    return df_market"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cc0b441",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: verify where market_data lives in the DB you're connected to.\n",
        "# Run this if DB load fails with \"relation does not exist\". Expect: schema=data_ingestion, table_name=market_data.\n",
        "from sqlalchemy import text\n",
        "try:\n",
        "    with db_readonly_session() as session:\n",
        "        r = session.execute(text(\n",
        "            \"SELECT table_schema, table_name FROM information_schema.tables WHERE table_name = 'market_data'\"\n",
        "        )).fetchall()\n",
        "    if r:\n",
        "        print(\"market_data found:\", [dict(zip([\"schema\", \"table\"], row)) for row in r])\n",
        "    else:\n",
        "        print(\"No table named 'market_data' in this database. Create it with scripts/01_create_databases.sql and 02_create_core_tables.sql.\")\n",
        "except Exception as e:\n",
        "    print(\"Could not check DB:\", e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81d174b5",
      "metadata": {},
      "outputs": [],
      "source": [
        "def fetch_market_data_yahoo(\n",
        "    symbol: str,\n",
        "    start_date: Optional[datetime] = None,\n",
        "    end_date: Optional[datetime] = None,\n",
        "    interval: str = \"1h\",\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Fetch OHLCV from Yahoo Finance (yfinance). No DB required.\n",
        "    Uses auto_adjust=True so prices are adjusted for splits/dividends.\n",
        "    Yahoo limits 1h data to the last 730 days; keep start_date within that range.\n",
        "    Returns DataFrame with columns: timestamp, open, high, low, close, volume (UTC).\n",
        "    \"\"\"\n",
        "    if start_date is None:\n",
        "        start_date = datetime.now(timezone.utc) - timedelta(days=365)\n",
        "    if end_date is None:\n",
        "        end_date = datetime.now(timezone.utc)\n",
        "    start = start_date.date() if hasattr(start_date, \"date\") else start_date\n",
        "    end = end_date.date() if hasattr(end_date, \"date\") else end_date\n",
        "    ticker = yf.Ticker(symbol.upper())\n",
        "    hist = ticker.history(start=start, end=end, interval=interval, auto_adjust=True)\n",
        "    if hist.empty:\n",
        "        raise ValueError(f\"No data from Yahoo for {symbol} between {start} and {end}\")\n",
        "    hist = hist.rename(columns={\"Open\": \"open\", \"High\": \"high\", \"Low\": \"low\", \"Close\": \"close\", \"Volume\": \"volume\"})\n",
        "    hist = hist[[\"open\", \"high\", \"low\", \"close\", \"volume\"]]\n",
        "    hist.index = pd.to_datetime(hist.index)\n",
        "    if hist.index.tz is None:\n",
        "        hist.index = hist.index.tz_localize(\"UTC\", ambiguous=\"infer\")\n",
        "    else:\n",
        "        hist.index = hist.index.tz_convert(\"UTC\")\n",
        "    hist[\"timestamp\"] = hist.index\n",
        "    df_market = hist.reset_index(drop=True)[[\"timestamp\", \"open\", \"high\", \"low\", \"close\", \"volume\"]]\n",
        "    df_market[\"volume\"] = df_market[\"volume\"].fillna(0).astype(int)\n",
        "    return df_market"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01846acf-2a2b-4810-9dba-afbba657c3c0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m2026-02-07 07:45:35.194\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36msrc.shared.database.base\u001b[0m:\u001b[36mdb_readonly_session\u001b[0m:\u001b[36m159\u001b[0m - \u001b[31m\u001b[1mError in read-only session: (psycopg2.errors.UndefinedTable) relation \"data_ingestion.market_data\" does not exist\n",
            "LINE 2: FROM data_ingestion.market_data \n",
            "             ^\n",
            "\n",
            "[SQL: SELECT data_ingestion.market_data.id, data_ingestion.market_data.symbol, data_ingestion.market_data.timestamp, data_ingestion.market_data.data_source, data_ingestion.market_data.open, data_ingestion.market_data.high, data_ingestion.market_data.low, data_ingestion.market_data.close, data_ingestion.market_data.volume, data_ingestion.market_data.created_at \n",
            "FROM data_ingestion.market_data \n",
            "WHERE data_ingestion.market_data.symbol = %(symbol_1)s AND data_ingestion.market_data.data_source = %(data_source_1)s AND data_ingestion.market_data.timestamp >= %(timestamp_1)s AND data_ingestion.market_data.timestamp <= %(timestamp_2)s ORDER BY data_ingestion.market_data.timestamp]\n",
            "[parameters: {'symbol_1': 'MU', 'data_source_1': 'yahoo_adjusted', 'timestamp_1': datetime.datetime(2024, 2, 18, 13, 45, 35, 23709, tzinfo=datetime.timezone.utc), 'timestamp_2': datetime.datetime(2026, 2, 7, 13, 45, 35, 24943, tzinfo=datetime.timezone.utc)}]\n",
            "(Background on this error at: https://sqlalche.me/e/20/f405)\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DB unavailable or insufficient data ((psycopg2.errors.UndefinedTable) relation \"data_ingestion.market_data\" does not exist\n",
            "LINE 2: FROM data_ingestion.market_data \n",
            "             ^\n",
            "\n",
            "[SQL: SELECT data_ingestion.market_data.id, data_ingestion.market_data.symbol, data_ingestion.market_data.timestamp, data_ingestion.market_data.data_source, data_ingestion.market_data.open, data_ingestion.market_data.high, data_ingestion.market_data.low, data_ingestion.market_data.close, data_ingestion.market_data.volume, data_ingestion.market_data.created_at \n",
            "FROM data_ingestion.market_data \n",
            "WHERE data_ingestion.market_data.symbol = %(symbol_1)s AND data_ingestion.market_data.data_source = %(data_source_1)s AND data_ingestion.market_data.timestamp >= %(timestamp_1)s AND data_ingestion.market_data.timestamp <= %(timestamp_2)s ORDER BY data_ingestion.market_data.timestamp]\n",
            "[parameters: {'symbol_1': 'MU', 'data_source_1': 'yahoo_adjusted', 'timestamp_1': datetime.datetime(2024, 2, 18, 13, 45, 35, 23709, tzinfo=datetime.timezone.utc), 'timestamp_2': datetime.datetime(2026, 2, 7, 13, 45, 35, 24943, tzinfo=datetime.timezone.utc)}]\n",
            "(Background on this error at: https://sqlalche.me/e/20/f405)). Loading from Yahoo...\n",
            "Loaded from Yahoo (yfinance)\n"
          ]
        }
      ],
      "source": [
        "# Load OHLCV: try DB (yahoo_adjusted) first, else fetch from Yahoo directly (1h; 730-day limit).\n",
        "# DB table must be data_ingestion.market_data in the DB from .env (TRADING_DB_NAME). Run the cell above to verify.\n",
        "SYMBOL = \"MU\"\n",
        "START_DATE = datetime.now(timezone.utc) - timedelta(days=720)\n",
        "try:\n",
        "    df_market = load_market_data(SYMBOL, start_date=START_DATE, data_source=\"yahoo_adjusted\", min_records=100)\n",
        "    print(\"Loaded from DB (yahoo_adjusted)\")\n",
        "except Exception:\n",
        "    print(\"DB unavailable or insufficient data (e.g. table missing). Loading from Yahoo...\")\n",
        "    df_market = fetch_market_data_yahoo(SYMBOL, start_date=START_DATE)\n",
        "    print(\"Loaded from Yahoo (yfinance)\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9f5f269-95d8-426d-bd30-c069e315fb0c",
      "metadata": {},
      "source": [
        "## Outlier Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89ca3c38-27f9-48f5-8e58-9fc7a3e33736",
      "metadata": {},
      "source": [
        "**Handling \"Wick\" Outliers** (The \"Flash Crash\" Problem) Even with auto_adjust=True, Yahoo Finance can occasionally have \"bad prints\"—data points where the High or Low is unnaturally far from the Open/Close. To protect your PyTorch model from these without deleting data, use a Clipping strategy before passing it to the normalizer. This prevents a single bad data point from skewing the local \\(\\mu \\) and \\(\\sigma \\) of your window. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a6dcdb5-3d2c-4cd5-ab9b-398acdfa6218",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Wick-outlier clipping (bad prints / flash-crash): cap OHLC at 0.5% and 99.5% quantiles.\n",
        "# Preserves trend, prevents a single bad point from skewing scale; applied before indicators/normalizer.\n",
        "for col in [\"open\", \"high\", \"low\", \"close\"]:\n",
        "    upper = df_market[col].quantile(0.995)\n",
        "    lower = df_market[col].quantile(0.005)\n",
        "    df_market[col] = df_market[col].clip(lower, upper)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fd23ad4-0d58-4f7a-9db6-41457f65bd5b",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Date range:\", df_market[\"timestamp\"].min(), \"to\", df_market[\"timestamp\"].max())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f9498b4-345e-4491-a0be-a2e4c1bb3c63",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_market.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09e555e2-d299-4264-808e-287c893d038e",
      "metadata": {},
      "source": [
        "***Recommended Workflow for OHLC Data***\n",
        "\n",
        "- **Load OHLC Data**: we are using adjusted prices from yahoo here\n",
        "\n",
        "- **Calculate Technical Indicators**: Generate your TIs (e.g., using libraries like pandas_ta or ta-lib) while the data is still in its raw price/volume form.\n",
        "- **Clean & Prepare**: Handle any NaN values created by lagging indicators (e.g., a 20-period MA will have 19 NaN rows at the start).\n",
        "- **Apply Normalization**: Use your TimeSeriesDataSet with the EncoderNormalizer. This will scale each feature—both the raw OHLC and the new TIs—dynamically for every window to prevent look-ahead bias. \n",
        "\n",
        "***Why this order matters***\n",
        "\n",
        "- **Mathematical Integrity**: Most TIs are functions of price or volume. If you feed \"z-score scaled\" values into an RSI formula, the resulting indicator will be mathematically meaningless.\n",
        "- **Feature Consistency**: Neural networks like LSTMs or Transformers are highly sensitive to input scale. Once you have your TIs, they may have wildly different ranges (e.g., Volume in millions vs. RSI between 0-100). Normalizing all features together after calculation ensures the model weights aren't \"swamped\" by large-scale features.\n",
        "- **Leakage Prevention**: By using the EncoderNormalizer after TI calculation, you ensure that even the indicator values are scaled only based on their own local history within the encoder window, strictly avoiding future data leaks. \n",
        "\n",
        "***Special Note on Different Indicators***\n",
        "- **Bounded Indicators** (e.g., RSI, Stochastic): These are already naturally scaled between 0 and 100. Some practitioners choose not to scale these further, but running them through a normalizer is generally safer for deep learning convergence.\n",
        "- **Unbounded Indicators** (e.g., MACD, Moving Averages): These must be normalized as they follow the price scale and will drift over time. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e877dd4c",
      "metadata": {},
      "source": [
        "## Compute Technical Indicators\n",
        "\n",
        "Technical indicators are calculated from OHLCV in this notebook (no DB). Same set as project: SMA, EMA, RSI, MACD, Bollinger Bands, volatility, price changes, volume."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "859bcaf4",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_technical_indicators(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Compute technical indicators from OHLCV DataFrame (in-notebook, no DB).\n",
        "    Expects columns: open, high, low, close, volume. Uses same definitions as project.\n",
        "    \"\"\"\n",
        "    out = df.copy()\n",
        "    c = out[\"close\"]\n",
        "    v = out[\"volume\"]\n",
        "\n",
        "    # Moving averages\n",
        "    out[\"sma_20\"] = c.rolling(20).mean()\n",
        "    out[\"sma_50\"] = c.rolling(50).mean()\n",
        "    out[\"sma_200\"] = c.rolling(200).mean()\n",
        "    out[\"ema_12\"] = c.ewm(span=12, adjust=False).mean()\n",
        "    out[\"ema_26\"] = c.ewm(span=26, adjust=False).mean()\n",
        "    out[\"ema_50\"] = c.ewm(span=50, adjust=False).mean()\n",
        "\n",
        "    # RSI (14) - Wilder smoothing: avg_gain/avg_loss via EMA of delta\n",
        "    delta = c.diff()\n",
        "    gain = delta.where(delta > 0, 0.0)\n",
        "    loss = (-delta).where(delta < 0, 0.0)\n",
        "    ag = gain.ewm(alpha=1/14, adjust=False).mean()\n",
        "    al = loss.ewm(alpha=1/14, adjust=False).mean()\n",
        "    rs = ag / al.replace(0, np.nan)\n",
        "    out[\"rsi\"] = (100 - (100 / (1 + rs))).fillna(100)  # no loss -> RSI 100\n",
        "    out[\"rsi_14\"] = out[\"rsi\"]\n",
        "\n",
        "    # MACD (12, 26, 9)\n",
        "    ema12 = c.ewm(span=12, adjust=False).mean()\n",
        "    ema26 = c.ewm(span=26, adjust=False).mean()\n",
        "    out[\"macd_line\"] = ema12 - ema26\n",
        "    out[\"macd_signal\"] = out[\"macd_line\"].ewm(span=9, adjust=False).mean()\n",
        "    out[\"macd_histogram\"] = out[\"macd_line\"] - out[\"macd_signal\"]\n",
        "\n",
        "    # Bollinger Bands (20, 2)\n",
        "    out[\"bb_middle\"] = c.rolling(20).mean()\n",
        "    bb_std = c.rolling(20).std()\n",
        "    out[\"bb_upper\"] = out[\"bb_middle\"] + 2 * bb_std\n",
        "    out[\"bb_lower\"] = out[\"bb_middle\"] - 2 * bb_std\n",
        "    spread = out[\"bb_upper\"] - out[\"bb_lower\"]\n",
        "    out[\"bb_position\"] = (c - out[\"bb_lower\"]) / spread.replace(0, np.nan)\n",
        "    out[\"bb_width\"] = (spread / out[\"bb_middle\"].replace(0, np.nan)) * 100\n",
        "\n",
        "    # Bounded indicators (fixed range; optional to scale, we leave them unscaled in scalers)\n",
        "    # Stochastic %K (14): 0–100\n",
        "    low_14 = out[\"low\"].rolling(14).min()\n",
        "    high_14 = out[\"high\"].rolling(14).max()\n",
        "    stoch_range = (high_14 - low_14).replace(0, np.nan)\n",
        "    out[\"stoch_k\"] = ((c - low_14) / stoch_range * 100).clip(0, 100)\n",
        "    # Williams %R (14): -100 to 0 (oversold near -100, overbought near 0)\n",
        "    out[\"williams_r\"] = ((high_14 - c) / stoch_range * -100).clip(-100, 0)\n",
        "\n",
        "    # Volatility (annualized %) and price changes\n",
        "    returns = c.pct_change()\n",
        "    out[\"volatility_20\"] = returns.rolling(20).std() * np.sqrt(252) * 100\n",
        "    out[\"price_change_1d\"] = c.pct_change(1) * 100\n",
        "    out[\"price_change_5d\"] = c.pct_change(5) * 100\n",
        "    out[\"price_change_30d\"] = c.pct_change(30) * 100\n",
        "\n",
        "    # Volume\n",
        "    out[\"avg_volume_20\"] = v.rolling(20).mean()\n",
        "    out[\"current_volume\"] = v\n",
        "\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c5832eb-b7a1-4733-b7a1-0b910c0e169f",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_features = compute_technical_indicators(df_market)\n",
        "# Remove rows with any null (e.g. warm-up for SMA_200, RSI, etc.)\n",
        "df_features = df_features.dropna()\n",
        "df_features.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f215983f",
      "metadata": {},
      "source": [
        "## Normalization\n",
        "\n",
        "*encoder-only, no look-ahead bias*\n",
        "\n",
        "Use **TimeSeriesDataSet** with **EncoderNormalizer**: scaling is fit on each encoder sequence only, so no future information leaks into the past (no look-ahead bias).\n",
        "\n",
        "**Robust scaling (median + IQR):** We use **method=\"robust\"** instead of mean/std. Outliers (e.g. Black Swan events, flash crashes) are **not deleted**—they stay in the data so the model can learn from them—but robust scaling prevents extreme spikes from squishing the rest of the values into a tiny range. Applied after technical indicators, before the model.\n",
        "\n",
        "**Indicator scaling rationale:**\n",
        "- **Bounded indicators** (RSI, bb_position, stoch_k, williams_r): Fixed scale → no scaling (identity).\n",
        "- **Unbounded indicators** (MACD, MAs, close, volume, etc.): **EncoderNormalizer(method=\"robust\")** (encoder-only, median/IQR)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e61a2ee8",
      "metadata": {},
      "source": [
        "### Data pipeline (how everything connects)\n",
        "\n",
        "All later steps (e.g. **Temporal Fusion Transformer**) use the same data chain:\n",
        "\n",
        "| Step | Variable | Description |\n",
        "|------|----------|-------------|\n",
        "| 1 | `df_market` | Raw OHLCV from DB (one symbol) |\n",
        "| 2 | `df_features` | OHLCV + technical indicators, nulls dropped |\n",
        "| 3 | `df_ts` | Same as `df_features` + `time_idx` + `ticker` (required by TimeSeriesDataSet) |\n",
        "| 4 | `dataset` | TimeSeriesDataSet built from `df_ts` (encoder-only scaling, no look-ahead) |\n",
        "| 5 | `dataloader` | `dataset.to_dataloader(...)` → batches for training |\n",
        "\n",
        "**TFT and any other model in this notebook will use `dataset` and `dataloader` from above.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfe95d90-891f-4471-94d7-8f6e9204777d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare dataframe for TimeSeriesDataSet: need time_idx and group_ids\n",
        "df_ts = df_features.copy()\n",
        "df_ts[\"time_idx\"] = np.arange(len(df_ts))\n",
        "df_ts[\"ticker\"] = SYMBOL  # single series → one group\n",
        "\n",
        "# Time-varying features (unknown in the future). Bounded ones stay in list but get identity scaling.\n",
        "# Close-focused: we predict close; indicators are close-based. open/high/low not fed as inputs to avoid\n",
        "# redundancy and multicollinearity. high/low still used inside compute_technical_indicators for stoch_k, williams_r.\n",
        "time_varying_unknown_reals = [\n",
        "    \"close\", \"volume\",\n",
        "    \"sma_20\", \"sma_50\", \"sma_200\", \"ema_12\", \"ema_26\", \"ema_50\",\n",
        "    \"rsi\", \"rsi_14\", \"macd_line\", \"macd_signal\", \"macd_histogram\",\n",
        "    \"bb_upper\", \"bb_middle\", \"bb_lower\", \"bb_position\", \"bb_width\",\n",
        "    \"stoch_k\", \"williams_r\",\n",
        "    \"volatility_20\", \"price_change_1d\", \"price_change_5d\", \"price_change_30d\",\n",
        "    \"avg_volume_20\", \"current_volume\",\n",
        "]\n",
        "time_varying_unknown_reals = [c for c in time_varying_unknown_reals if c in df_ts.columns]\n",
        "\n",
        "# Bounded: fixed scale (0–100, 0–1, -100–0) → no scaling (identity). Unbounded → EncoderNormalizer.\n",
        "BOUNDED_INDICATORS = [\"rsi\", \"rsi_14\", \"bb_position\", \"stoch_k\", \"williams_r\"]\n",
        "MIN_ENCODER_LENGTH = 60\n",
        "MAX_ENCODER_LENGTH = 60\n",
        "MIN_PREDICTION_LENGTH = 5\n",
        "MAX_PREDICTION_LENGTH = 5\n",
        "\n",
        "# Target has its own normalizer; do not put target in scalers (library requirement)\n",
        "scaler_cols = [c for c in time_varying_unknown_reals if c != \"close\"]\n",
        "# Robust scaling (median/IQR): keeps outliers in data, prevents extremes from squishing the rest\n",
        "scalers = {\n",
        "    col: (None if col in BOUNDED_INDICATORS else EncoderNormalizer(method=\"robust\", center=True))\n",
        "    for col in scaler_cols\n",
        "}\n",
        "\n",
        "# Show which attributes get normalization and which do not (target \"close\" uses target_normalizer)\n",
        "normalized_attrs = [c for c in scaler_cols if scalers[c] is not None]\n",
        "identity_attrs = [c for c in scaler_cols if scalers[c] is None]\n",
        "norm_summary = pd.DataFrame({\n",
        "    \"attribute\": normalized_attrs + identity_attrs + [\"close\"],\n",
        "    \"normalization\": [\"EncoderNormalizer (robust, encoder-only)\"] * len(normalized_attrs)\n",
        "    + [\"None (identity)\"] * len(identity_attrs)\n",
        "    + [\"EncoderNormalizer (robust, target_normalizer)\"],\n",
        "    \"reason\": [\"Unbounded; robust scale (median/IQR); outliers retained\"] * len(normalized_attrs)\n",
        "    + [\"Bounded (fixed range)\"] * len(identity_attrs)\n",
        "    + [\"Target; robust encoder-only scale\"],\n",
        "})\n",
        "display(norm_summary)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6257d75d-b90c-4394-ba6d-49da8511bb5f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# EncoderNormalizer: fit scale on each encoder sequence only → no look-ahead bias\n",
        "dataset = TimeSeriesDataSet(\n",
        "    df_ts,\n",
        "    time_idx=\"time_idx\",\n",
        "    target=\"close\",\n",
        "    group_ids=[\"ticker\"],\n",
        "    min_encoder_length=MIN_ENCODER_LENGTH,\n",
        "    max_encoder_length=MAX_ENCODER_LENGTH,\n",
        "    min_prediction_length=MIN_PREDICTION_LENGTH,\n",
        "    max_prediction_length=MAX_PREDICTION_LENGTH,\n",
        "    time_varying_unknown_reals=time_varying_unknown_reals,\n",
        "    target_normalizer=EncoderNormalizer(method=\"robust\", center=True),\n",
        "    scalers=scalers,\n",
        ")\n",
        "\n",
        "dataloader = dataset.to_dataloader(train=True, batch_size=32, num_workers=0)\n",
        "print(f\"TimeSeriesDataSet: {len(dataset)} samples, encoder={MAX_ENCODER_LENGTH}, prediction={MAX_PREDICTION_LENGTH}\")\n",
        "batch = next(iter(dataloader))\n",
        "print(f\"Batch keys: {list(batch[0].keys())}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40d585cd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify link: prev data → TFT-ready dataset (run this to confirm the pipeline)\n",
        "assert \"time_idx\" in df_ts.columns and \"ticker\" in df_ts.columns\n",
        "assert len(df_ts) == len(df_features), \"df_ts = df_features + time_idx + ticker\"\n",
        "assert set(df_features.columns).issubset(df_ts.columns), \"df_ts contains all df_features columns\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15364c89-6098-4529-a79d-2e832ea9b33b",
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline = {\n",
        "    \"df_market\": (len(df_market), list(df_market.columns)),\n",
        "    \"df_features\": (len(df_features), list(df_features.columns)),\n",
        "    \"df_ts\": (len(df_ts), list(df_ts.columns)),\n",
        "    \"dataset (TimeSeriesDataSet)\": (len(dataset), \"encoder/decoder batches\"),\n",
        "    \"dataloader\": (len(dataloader), \"batch_size=32\"),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "980c6324-a23c-4b40-a575-3aa33d492221",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Data pipeline (prev → new dataset):\")\n",
        "\n",
        "for name, (size, detail) in pipeline.items():\n",
        "    print(f\"{name}: n={size}  ({detail})\")\n",
        "    print(f\"\\n\")\n",
        "print(\"\\n→ Use 'dataset' and 'dataloader' for TFT (or any PyTorch Forecasting model).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "568fffb3-8024-4572-80ae-13c9b72bea95",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "001cfa91-c19a-4e8e-be34-e6e122c8dfdc",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "torch_gpu",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
